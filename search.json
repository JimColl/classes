[
  {
    "objectID": "working.html",
    "href": "working.html",
    "title": "A system for productivity",
    "section": "",
    "text": "I drink and I know things.\n- Tyrion Lannister\nIn outlining the pathways towards critical spatial thinking, I distinguished between “tool making” and “tool using”. These differences are hierarchical in nature when framing them in the context of learning to become a GIScientist (i.e. a tool maker is demonstrating higher thinking capabilities than that of the tool user), but that relationship is reversed when we look at the actual amount of output produced (i.e. the tool user will make more in the same time span, because he is not concerned with the creation or maintenance of said tool). It’s also worth saying that these pros and cons are moot when you enter the armory and start working in a team, as a well paired team should always outproduce an individual. However, to be the most efficient producer I can, we should underlay both of those efforts with a strong foundation in the direction and method of travel we’re aiming at. Knowing what direction to go and how to get there is what I’ll define as “knowledge work” (a term I first stumbled on in Andys working notes). It’s not quite quite tech-evangelizing, which I think of moreso as applying different methods of travel to someone elses desired direction, but at this point I have to wonder how many abstract metaphors are too many metaphors?\nSo, to bring this back to something more tangible, regardless of your job title or direction of travel, whether you are an academic, a consultant, a practitioner, a knowledge worker, or a media star, it’s all about the content. To reduce the friction between head work and hand work, maintain reproducibility, and help limit the amount of time I spend retreading old paths and reinventing wheels, I’ll walk you through some of the key stops along my workflow.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems"
    ]
  },
  {
    "objectID": "working.html#my-workflow",
    "href": "working.html#my-workflow",
    "title": "A system for productivity",
    "section": "My Workflow",
    "text": "My Workflow\n\n\n\nYes, I am painfully aware this is disturbingly close to the PKM bellcurve meme. I am a mouth breather.\n\n\n\nNote taking: How to install and cite with Zotero and how I collect and read documents.\nComputational execution and reproducibility: Including how to install GIS and other computational environments (docker, python, ect.).\nData Driven Documentation & Digital communication: How to programattically dismantle a problem and then share your work on the web.\n\n\nA note: I run Windows 10 on a desktop, have admin privileges, own my router, and “back up to the cloud” using Dropbox. Cloud based hosting can be a little jankey, but when in doubt you can always pause sync for the day and let the changes roll across when you’re done processing. I will never regret paying for the off-site backup convenience, and at this point you might as well just consider everything digital as public anyways.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems"
    ]
  },
  {
    "objectID": "working.html#starting-20230908085243-the-hellscape-that-is-my-day",
    "href": "working.html#starting-20230908085243-the-hellscape-that-is-my-day",
    "title": "A system for productivity",
    "section": "Starting [[20230908085243]] The hellscape that is my day",
    "text": "Starting [[20230908085243]] The hellscape that is my day\n\nDesk and physical setup\n\nStanding desk\n3 monitors, one vertically mounted that almost always has notes & Zotero open. The second non-vertical one is just the icing on the cake.\n2 PC’s, the server is better off being a dedicated system.\n\nBuild a pc:\n\n\nData science is demanding, and a lower grade computer is not recommended. If you are looking to purchase a computer, I offer these general guidelines. I can not use a mac (I need the extra mouse button) so I give my soul to Brill Grates and run a windows 10 64-bit computer (x64) PC, and aside from that I like to prioritize CPU & RAM &gt; video & hard drive speed &gt; hard drive space (External hard drives are really cheap, and as long as the computer has a USB port you are good to go) &gt; RGB effects. For the last few years I’ve deferred to the pre-built PC choices on https://buildmypc.net/ or https://pcpartpicker.com/. In general, I tend to stay away from the “gaming” laptops. For those prices, you can build a much more powerful desktop and get a lot more mileage out of it, but that’s just me. If you insist on going that route, virtually all will meet your school related needs.\n\nSoftware:\n\nMiKTeX\nZettlr\nInstalling Zotero \nnotepad++\nGeospatial execution \n\n7zip\nMicrosoft C++\nvirtualbox\nR & RStudio\n\nVLC\ngimp\ndraw.io\nCARNAC (via https://github.com/bfritscher/carnac’s fork)\nVIA\ninstall blender\nIRFANVIEW\nNotion\n\nOther that I don’t want but have to…\n\nGit\n\nCheckout as-is, commit as-is\nMintty\nNo cred manager\nuncheck new features\n\nteams\nslack\nhttps://s3browser.com/download.aspx\nZoom (will just install itself please and thank you)\nSkype (As with Zoom, why is this still installing?)",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems"
    ]
  },
  {
    "objectID": "troute.html#howto",
    "href": "troute.html#howto",
    "title": "T-Route",
    "section": "HowTo",
    "text": "HowTo\n\nInstall"
  },
  {
    "objectID": "troute.html#tutorials",
    "href": "troute.html#tutorials",
    "title": "T-Route",
    "section": "Tutorials",
    "text": "Tutorials"
  },
  {
    "objectID": "troute.html#explanation",
    "href": "troute.html#explanation",
    "title": "T-Route",
    "section": "explanation",
    "text": "explanation"
  },
  {
    "objectID": "terrain.html",
    "href": "terrain.html",
    "title": "Terrain",
    "section": "",
    "text": "A DEM (Digital Elevation Model) Represents the bare-Earth surface, removing all natural and built features;\nA DTM (Digital Terrain Model) typically augments a DEM, by including vector features of the natural terrain, such as rivers and ridges. A DTM may be interpolated to generate a DEM, but not vice versa.\nDSM (Digital Surface Model) is a subset of a terrain model with a particular surface of interet called out. These can sometimes be Canopy models (DCM) or others along that same vein.\nREM - Reletive Elevation Model is an encoding in a cell whihc spesifies the particular elevation you would need to acheve to reach that location. This can also be called HAND (Hight Above Nearest Drainage), where the “nearest drainage” spesifies the elevation we are relative to. These are all subsets of a “cost distancec” raster, whihc spesifeis the cst needed to reach a given location. That cost can be in any number of units. https://geodetics.com/dem-dsm-dtm-digital-elevation-models/#:~:text=The%20DSM%20is%20represented%20by,hydrology%20and%20flow%2Ddirection%20studies."
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "Statistics",
    "section": "",
    "text": "It’s AI in the investor’s news letter, it’s ML in the grant application, it’s non-parametic in the classroom, and it’s linear fit in practice.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#non-parametric-statistics",
    "href": "stats.html#non-parametric-statistics",
    "title": "Statistics",
    "section": "Non-parametric statistics",
    "text": "Non-parametric statistics\nThis sentiment can be interpreted in a number of ways, but I really like to latch onto the fact that despite its simplicity, extrapolating in the short term works fairly well. While there’s nothing wrong with the practice, we can be a little better in that it’s not much harder to perform a non-parametric test on your data and get a much more statistically defensible and meaningfully interpretable result\n\nA post about Mann Kendall and Sens slope in GEE. At some point I’ll get around to doing something with this too.\n\n\nSens Slope\n\n\nMann Kendall",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#curve-fitting",
    "href": "stats.html#curve-fitting",
    "title": "Statistics",
    "section": "Curve fitting",
    "text": "Curve fitting\n(“The Broken Bridge Between Biologists and Statisticians  A Collection of Self-Starters for Nonlinear Regression in R” n.d.) (“The Broken Bridge Between Biologists and Statisticians  Self-starting Routines for Nonlinear Regression Models” n.d.)\nThe linear least-squares problem occurs in statistical regression analysis; it has a closed-form solution. The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.\nOccasionally we have to estimate a function which estimates a set of observations. Because observations are messy and we are modeling an otherwise interconnected system, the form that equation takes is not often particularly clear. Here’s a little template I use that sets up the calculation, reporting, and plotting of a range of curves. YMMV, and never show this to a mathematician.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#quick-fit-check",
    "href": "stats.html#quick-fit-check",
    "title": "Statistics",
    "section": "Quick fit check",
    "text": "Quick fit check\n\nlinear_fit &lt;- stats::lm(Y ~ X, data = data)  ## $y = mX + b$\n#braggs3_nls_fit &lt;- stats::nls(Y ~ aomisc::NLS.bragg.3(X, b, d, e), data = data)\n\ntryCatch( { \n  result &lt;- log(\"not a number\"); print(res) \n  }, \n  error = function(e) {\n    ggplot2::ggplot() + \n  ggplot2::geom_point(data = data, ggplot2::aes(x = X, y = Y), color = \"red\") + \n  ggplot2::geom_text(\n    data = data.frame(x = mean(data$X),y = mean(data$Y), label = \"DID NOT CONVERGE\"), \n    ggplot2::aes(x, y, label = label),\n    hjust = 0.5, vjust = 0.5, angle = 45, size = 11, color = \"red\",inherit.aes = FALSE) +\n  cowplot::theme_half_open() +\n  cowplot::background_grid()\n    }\n  )",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#quick-fit-check-1",
    "href": "stats.html#quick-fit-check-1",
    "title": "Statistics",
    "section": "Quick fit check",
    "text": "Quick fit check\n\n\n[1] \"test\"\n\n\nAnd for some slightly longer and more decorative explainers:",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#curves",
    "href": "stats.html#curves",
    "title": "Statistics",
    "section": "Curves",
    "text": "Curves",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#bell-curves",
    "href": "stats.html#bell-curves",
    "title": "Statistics",
    "section": "Bell curves",
    "text": "Bell curves\nThis function is connected to the normal (Gaussian) distribution and has a symmetric shape with a maximum equal to \\(d\\), that is reached when \\(X=e\\) and two inflection points. In this model, \\(b\\) relates to the slope at the inflection points; the response \\(Y\\) approaches 0 when X approaches \\(±∞\\): \\(y = d * exp[-b*(X - e)^2]\\), or with lower asymptotes \\(c \\ne 0\\) as \\(y = c + (d - c) * exp[-b*(X - e)^2]\\)\n\nOther useful tools:\n\nhttps://www.graphreader.com/\nhttps://mycurvefit.com/",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics"
    ]
  },
  {
    "objectID": "snow.html",
    "href": "snow.html",
    "title": "Snow",
    "section": "",
    "text": "is cold.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Snow"
    ]
  },
  {
    "objectID": "snow.html#tutorials",
    "href": "snow.html#tutorials",
    "title": "Snow",
    "section": "Tutorials",
    "text": "Tutorials",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Snow"
    ]
  },
  {
    "objectID": "snow.html#explainers",
    "href": "snow.html#explainers",
    "title": "Snow",
    "section": "Explainers",
    "text": "Explainers",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Snow"
    ]
  },
  {
    "objectID": "snow.html#references",
    "href": "snow.html#references",
    "title": "Snow",
    "section": "References",
    "text": "References\nSee the Google Earth Engine Book of Knowledge entry for an overview on how to use the platform, and a short tutorial on snow cover fractional classification and trend analysis. See the paper on MOD fractional snow cover accuracy at (Coll and Li 2018)",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Snow"
    ]
  },
  {
    "objectID": "sensemaking.html",
    "href": "sensemaking.html",
    "title": "Sensemaking",
    "section": "",
    "text": "From: It’s Always Sunny in Philadelphia - (“Tumblr_o16n2kBlpX1ta3qyvo1_1280-1200x675.jpg (1200675)” n.d.)\nIf you pause and ponder life for any amount of time, it seems inevitable that you’ll come to the conclusion that not a whole lot in our world makes sense. However, living in a fluid soup of nonsense is not a particularly useful or productive place to be; and an even worse place to start building the foundation needed to enact change and add some semblance of reason back into the world. Fortunately, there are far too many smart thinkers who have trod the ground before me so I don’t have to start from 0, I don’t much care for the ontological discussions and semantics of philosophical outlooks, and I’m whimsical enough to not mind a little mess in my day to day nonsense. But occasionally I do tend to get stuck in the mud spinning my tires trying to find a meaningful way to make forward progress. The best way I’ve found to stop that is to encode what little sense making I’ve managed to articulate here.\nIn a sentence, I like to think of sensemaking as the act of making explicit the set of implicit assumptions, simplifications, and reductions you applied to make sense of your problem, and how you go about communicating those findings with everyone else. To do that, I’ll start with a blanket statement that needs to be at the forefront of any adventure into interdisciplinary collaboration:\nAs much as I hate playing the semantics game, a shared vocabulary and context are critical to advancing anything.\nEstablishing that syntax requires communication at the very least, but can be rapidly accelerated with a conversation. To have a productive conversation you ought to know what it is you’d like to say, and while you can construct that knowledge in the more fleeting form of thoughts, the act of writing can more quickly accrete those thoughts, ironically in part because you’re forced to slow down and express them at the speed of your communication medium. By writing that down I’ve also taken that first step in providing the one-way dialog of my thoughts. I’ve had a lot of luck (and patience/grace from others) constructing my own system of documentation mimicking several unusual forms of notes and prominent note takers. I also don’t use computers like a sane human and I’ve got some interesting patterns that make my efforts just that tiny bit more reproducible and less redundant. That system gives my digital footprint the scaffolding form that enables me to more seamlessy accrete efforts across my subject matter in a form amenable to the defacto standards of expressing that knowledge, the scientific method and a scientific paper. I’d like to push back on that however.\nWhile that standard is great, the execution is riddled with flaws and static documents are a critical but incredibly under powered vehicle for transferring tangible knowledge to the different actors of this wicked mess. To wit, there’s 1/3 of the wordcount equivalent of the Bible here (220,595 at last render, and an extra 381,805 words of private insanity) and most of them are my own. I’d argue virtually none of that is fit for permanent encoding in a scientific journal or the $4,000 price tag that accompanies that carving of my advanced idiocracy. It’s bad enough that it takes up your limited time/attention and the space on a hard drive.\nAs much as I enjoy complaining about that, It’d be stupidly fruitless of me to just bitch without offering up a solution. Again here, I don’t have to reach very far to find a better way. This Zettlekasten system on top of the publishing pathway of quarto &gt; GitHub Pages is a true FOSS solution that produces just as shiny a document (with even more potential functionality) and none of the paywall (or readership base) that the traditional publishing pathways offer. Those trade-offs work for me; and so although I will continue to play the game set before me, expect to find a lot of overlap and redundancy between this and my published work. Although I’ve attempted not to self-plagerize, I’m sure I have.\nDespite having just railed against its implementation, the scientific method is a tried and time-tested means of ensuring that the knowledge that you collect is true. After we’ve established a common means of communication, the next step is to figure out what it is you’d like to accomplish and what the limitations and constraints of that mission are. Very often, this ends up being fuzzly related to a wicked problem. It’s my opinion that the technical side of this problem is the more direct side of this equation to solve, and a solid foundation in reproducable productivity is needed to close that gap between head work and hand work. This Atlas is my effort in that direction.",
    "crumbs": [
      "Atlas",
      "Sensemaking"
    ]
  },
  {
    "objectID": "sensemaking.html#soapboxes-20240517113742-metaconclusion-are-topics-to-develop",
    "href": "sensemaking.html#soapboxes-20240517113742-metaconclusion-are-topics-to-develop",
    "title": "Sensemaking",
    "section": "Soapboxes ",
    "text": "Soapboxes \nI’ve been trying to make sense for it a while now and I’m not pleased to report that I’m more at a loss now than I have ever been. What have I been able to determine? Also called soapboxes. My soapbox is pretty flimsy and a bit redundant but in short:\n\nWe ought to be more ambitious about observing the world; go outside and look at the system!\nModels and Maps are two distinct tools in our toolbox and a primary means of sense making.\n\nWe ought to be more explicit about what we are trying to model.\nWe ought to be clearer about what we’re mapping.\n\nYou don’t need to stay in your lane, but the cascading effects of your analysis and domain should.\nWe ought to be more patient (with ourselves); this is a very complex system, nothing will be solved, and work is never finished.\nWorking a wicked problem requires overcoming the bankrupt attention economy and explicit guardrails on the concerns of the domain and the steps of a goal.\nYour direct objective and goal is not intrinsically shared with others, vocalize it.\n\n\nA note about “what language do I program in?”\nI code out of necessity, not out of love, and I’ve been told my more than a few that I write awful code. If you forced me to “pick a language” I think I’d go with JavaScript, specifically the GEE flavored variation. Then it would be R. Then it would be Python. GUI driven tools are great for the one-offs or cartography, and I’ve recently made the switch to QGIS for almost everything exploratory. These days I don’t typically open an ESRI product in favor of those open source solutions (and my unwillingness to log into or subscribe to anything more than I already am). Almost everything else is markdown or a pdf in Zotero. Based on that ordering, you can probably infer how sweet I like my coffee (syntax sugar is key). However, software is simply the means of teaching those foundations and concepts; and it’s those that I hope to transfer to you. Regardless of the method, I’m Googling syntax errors in English so my real answer is “the one that works”. Where I have skill and time I’ve demonstrated how to accomplish a given task in as many different forms as possible, but if I were a real pro I would be doing this in PowerPoint using Word as my IDE. In a year this will all be outdated anyways, so again, focus on the concepts first and the vehicle second and you should be set up for success both now and 3 years from now once we’ve experienced a technical revolution or two. If you’re really interested, see my workflow for more on my setup.",
    "crumbs": [
      "Atlas",
      "Sensemaking"
    ]
  },
  {
    "objectID": "ripple1d.html",
    "href": "ripple1d.html",
    "title": "Ripple1D",
    "section": "",
    "text": "As one of the foundational hydraulic modeling systems, HEC-RAS needs no introduction. This modeling system is so prolific that it’s one and two dimensional flood modeling capabilities provide the authoritative baseline for flood insurance and the default against which other modeling efforts compare. One of the most critical outputs of a HEC-RAS model is a Flood Inundation Map. However, as a complex piece of engineering and water resource science software, it’s use is difficult for many and can be difficult to scale. Starting with compliant one-dimensional HEC-RAS floodplain models, the Ripple1D process can be used to create a library of flood depth inundation rasters for a range of storm water discharges and hydraulic conditions. These libraries can be used with the National Water Model. More details can be found on the official repository at https://github.com/Dewberry/ripple1d and the official docs"
  },
  {
    "objectID": "ripple1d.html#executive-summary",
    "href": "ripple1d.html#executive-summary",
    "title": "Ripple1D",
    "section": "",
    "text": "As one of the foundational hydraulic modeling systems, HEC-RAS needs no introduction. This modeling system is so prolific that it’s one and two dimensional flood modeling capabilities provide the authoritative baseline for flood insurance and the default against which other modeling efforts compare. One of the most critical outputs of a HEC-RAS model is a Flood Inundation Map. However, as a complex piece of engineering and water resource science software, it’s use is difficult for many and can be difficult to scale. Starting with compliant one-dimensional HEC-RAS floodplain models, the Ripple1D process can be used to create a library of flood depth inundation rasters for a range of storm water discharges and hydraulic conditions. These libraries can be used with the National Water Model. More details can be found on the official repository at https://github.com/Dewberry/ripple1d and the official docs"
  },
  {
    "objectID": "reveal_3slide_deck_template.html#cool-dude-slide-template",
    "href": "reveal_3slide_deck_template.html#cool-dude-slide-template",
    "title": "So long as you get the GIS&T of it",
    "section": "Cool dude slide template",
    "text": "Cool dude slide template\n\n\n\n\n\n\nGoals:\n\nCool guy stuff over here\n\nOutcomes and Takeaways:\n\nA template to use.\nDo a kickflip\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\nSlide layout:  default     Items linked/bordered in green are cited in the tooltip on hover.\n: narrative-oriented     Items linked/bordered in blue are hyperlinked to relevant resources.\n\n\nPreliminary\n\n\nTemplate testing, meeting notes and whatnot… Background image from: Water PNGs by Vecteezy, https://www.vecteezy.com/members/ahasanaraakter"
  },
  {
    "objectID": "reveal_3slide_deck_template.html#presentation-navigation-tips",
    "href": "reveal_3slide_deck_template.html#presentation-navigation-tips",
    "title": "So long as you get the GIS&T of it",
    "section": "Presentation Navigation Tips",
    "text": "Presentation Navigation Tips\n\n\n\nSlide layout: // | : This deck has interactive elements/: This deck is narrative-oriented\nThis deck has one axis, use any key to advance. This deck is gridded, use space or N, to advance, use arrows accordingly. This deck has vertical slides, use space, N, or down arrow to advance.\n\nItems linked/bordered in green are cited in the tooltip on hover.\nItems linked/bordered in blue are hyperlinked to relevant resources.\n\n Please, no photos |  Please hold your questions  Photos are Allowed |  Questions are Encouraged\n: ~10 minutes | Last updated:08/25/2025 21:12:14\n! PLEASE !\nInterrupt me and ask questions or clarifications.\nI’m here to talk with you now, not to these slides.\n\n\nControl tips\n\nMy preferred (FOSS) flavor of slidedecks, revealjs, has intuitive but none the less unconventional PowerPoint presentation controls:\n\nSlides dynamically resize to use the entirety of the browser window, but you can still fullscreen with F.\n\nThis slide has a red border indicating the content extent.\n\nSlide navigation is mode dependent. If there are vertical slides, press space, N, or the down arrow key, not the right arrow to advance slides.\n\nHere’s a link to this presentation in scroll view.\n\nPress N, space, or the down arrow key for the next slide, P goes to the previous slide.\nPress M to open to the menu, Press O for the slide deck overview, Press B to black out the presentation screen, Press S for a speaker view.\nYou can use the chalkboard to freemouse/touchpad draw.\nSlides should render as designed but you can press Alt/Opt + click on the slide to zoom in. Increase text size with Alt/Opt + +, Alt/Opt + - to decrease, and Alt/Opt + 0 reset to the desinged style.\nPress C to declare victory and head home1.\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\n\n\n\n\nSpeaker notes\n\nA favored quote from one of my giants: Dr. David Maidment"
  },
  {
    "objectID": "reveal_3slide_deck_template.html#some-more-slides",
    "href": "reveal_3slide_deck_template.html#some-more-slides",
    "title": "So long as you get the GIS&T of it",
    "section": "Some more slides",
    "text": "Some more slides"
  },
  {
    "objectID": "reveal_3slide_deck_template.html#whats-next",
    "href": "reveal_3slide_deck_template.html#whats-next",
    "title": "So long as you get the GIS&T of it",
    "section": "What’s next?",
    "text": "What’s next?\n\n\n\nOutcomes and Takeaways:\n\nI’ve been doing kickflips in the parking lot\n\nReal cool guy shit here\n\n\nNext Steps:\n\nFinally finish somethi\nSolve Hydrology.\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\nHow did you end up here?"
  },
  {
    "objectID": "remotesensing.html",
    "href": "remotesensing.html",
    "title": "Remote Sensing",
    "section": "",
    "text": "Although a strict delineation would put quite a lot of our measurements of the earth within the “remotely sensed” (as opposed to in-situ), many of the techniques of used across remote sensing are natural extensions or applications of raster math to satellite imagery, which is what this section of reference materials contains.\n\nBand Index (NDVI, tasseled cap, wetness index)\n(S and T PCA)\nImage Classification (Supervised/unsupervised classification)” ([[SG.qmd]] Segmentation (Segment mean shift, object-based image analysis) [[classification.qmd]] #####)\nImage Streaching “Arranges the display of an image by adjusting brightness, contrast, and gamma properties.”\nComposite Bands (Composite bands)\nPan-Sharpening\nAtmospheric Correction (Dark object subtraction, radiative transfer models, atmosphere correction)\nPhotogrammarty\nMensuration: “Measures the geometry of two and three-dimensional features in an image. (Angles, height, perimeter, volume)”\nRADAR (Synthetic aperture radar)",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing"
    ]
  },
  {
    "objectID": "ras2fim_sample.html",
    "href": "ras2fim_sample.html",
    "title": "ras2fim_sample",
    "section": "",
    "text": "If you need sample data, see the closing steps outlined in installing RAS2FIM.\n\npython ras2fim.py -w 10170204 -i C:\\HEC\\input_folder -o C:\\HEC\\output_folder -p EPSG:26915 -v False -n E:\\X-NWS\\X-National_Datasets -r \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.3\"\nWhile there are “smart defaults” in place for both versions of the program used for OWP’s use case in scaling the system for a nation, you are free (and probably should) manually populate all of them if you are running this locally. You also control runtime options using the ras2fim\\config\\r2f_config.env file."
  },
  {
    "objectID": "ras2fim_sample.html#howto-run-ras2fim-over-sample-data",
    "href": "ras2fim_sample.html#howto-run-ras2fim-over-sample-data",
    "title": "ras2fim_sample",
    "section": "",
    "text": "If you need sample data, see the closing steps outlined in installing RAS2FIM.\n\npython ras2fim.py -w 10170204 -i C:\\HEC\\input_folder -o C:\\HEC\\output_folder -p EPSG:26915 -v False -n E:\\X-NWS\\X-National_Datasets -r \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.3\"\nWhile there are “smart defaults” in place for both versions of the program used for OWP’s use case in scaling the system for a nation, you are free (and probably should) manually populate all of them if you are running this locally. You also control runtime options using the ras2fim\\config\\r2f_config.env file."
  },
  {
    "objectID": "ras2fim2d.html",
    "href": "ras2fim2d.html",
    "title": "RAS2FIM2D",
    "section": "",
    "text": "https://github.com/mikejohnson51/ras2fimVis/tree/main/R https://web.corral.tacc.utexas.edu/nfiedata/acarter/webmap.html"
  },
  {
    "objectID": "precip.html",
    "href": "precip.html",
    "title": "Precipitation",
    "section": "",
    "text": "Water flows down-gradient, and in the case of rain that’s typically towards the surface of the earth following a gravitational potential gradient.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Precipitation"
    ]
  },
  {
    "objectID": "precip.html#executive-summary",
    "href": "precip.html#executive-summary",
    "title": "Precipitation",
    "section": "",
    "text": "Water flows down-gradient, and in the case of rain that’s typically towards the surface of the earth following a gravitational potential gradient.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Precipitation"
    ]
  },
  {
    "objectID": "precip.html#references",
    "href": "precip.html#references",
    "title": "Precipitation",
    "section": "References",
    "text": "References\n\nUse the interactive application to rapidly pull PMP values for a point.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Precipitation"
    ]
  },
  {
    "objectID": "precip.html#tutorials",
    "href": "precip.html#tutorials",
    "title": "Precipitation",
    "section": "Tutorials",
    "text": "Tutorials",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Precipitation"
    ]
  },
  {
    "objectID": "precip.html#explanations",
    "href": "precip.html#explanations",
    "title": "Precipitation",
    "section": "Explanations:",
    "text": "Explanations:\nSee a verbose explainer following the method used to calculate Probable Maximum Precipitation.\n\nRelates\nWhat would be the other alternatives to get a precipitation return period / intensity",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Precipitation"
    ]
  },
  {
    "objectID": "placeholder.html",
    "href": "placeholder.html",
    "title": "placeholder",
    "section": "",
    "text": "_\n             | |\n             | |===( )   //////\n             |_|   |||  | o o|\n                    ||| ( c  )                  ____\n                     ||| \\= /                  ||   \\_\n                      ||||||                   ||     |\n                      ||||||                ...||__/|-\"\n                      ||||||             __|________|__\n                        |||             |______________|\n                        |||             || ||      || ||\n                        |||             || ||      || ||\n------------------------|||-------------||-||------||-||-------\n                        |__&gt;            || ||      || ||\n\n\n     In the todo pile: hit any key to continue",
    "crumbs": [
      "Physical Geography",
      "placeholder"
    ]
  },
  {
    "objectID": "otherformats.html",
    "href": "otherformats.html",
    "title": "Other formats",
    "section": "",
    "text": "While those standard representations provide as comprehensive a coverage about the different ways that data can be represented, sometimes the format of the data is such that, even though it might still be representative of those core data types, it’s form and the way we interact with it is such that it warrents a special call out here.\n\n\n\n\n\n Sidecar files are separate pieces of data that otherwise work in concert to represent a concept.\nIn the geospatial domain, one of the most common ways you’ll encounter this concept is the shapefile format. Popularized by ESRI, this format visually appears in the ESRI tool ecosystem as a single file, but on your hard drive this is actually composed of multiple files which all share the same base filename, but have different extensions. A graphical view of those formats is shown above.\nFor a shapefile to be consider “valid”, it’s common to require just the the .shp, .shx and.dbf files, which I’ve called out as “principle” here. The projection file is the definition of the projection the .sbn and .sbx files are used to optimize spatial analyzes, the .xml file defines metadata, and the .cpg file defines encoding. One of the common limitations of shape files is file size. The size of both .shp and .dbf component files cannot exceed 2 GB (or 231 bytes), which encompasses ~70,000,000 points. Another common limitation is that these files can have no more than 255 attributes (fields) and those field names are limited to 10 constrained characters. This limitation is commonly encountered with high resolution data and authoritative datasets at even the county to national level.\nA note on the definition of principal here. While the technical implementation of reading data from a shapefile requires no projection file to successfully read in, the way the resulting array is placed in space by virtually all software requires some form of spatial definition. At this point, most of the commonly used software will make a default guess or force you to make a declaration. While the latter is preferred in my view (and that of the sf implementations), the former is perfectly acceptable so long as you actually stop to read the notice it places at you and understand the implications. As I say, I prefer the sf implementation of that process. If you give me some data but don’t tell me where it goes, I can faff about all day but I’m not going to try and put it on a map until you tell me where it should go. This statement flies in stark contrast with my actions. See RRASSLER’s shotgun_proj_test.R function, and my inability to detach myself from datums.\nWhile not a strict expression of this format, in the hydraulic science the most common way you’ll encounter a variation on this concept is the HEC-RAS model format. HEC-RAS solves the energy &gt; elevation portion of the integrated modeling stack, and is primarily used to represent channels in 1 Dimension. To encode that representation and all the other inputs the model needs, that data is shared across many files.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Other formats"
    ]
  },
  {
    "objectID": "otherformats.html#shapefiles-and-sidecar-files",
    "href": "otherformats.html#shapefiles-and-sidecar-files",
    "title": "Other formats",
    "section": "",
    "text": "Sidecar files are separate pieces of data that otherwise work in concert to represent a concept.\nIn the geospatial domain, one of the most common ways you’ll encounter this concept is the shapefile format. Popularized by ESRI, this format visually appears in the ESRI tool ecosystem as a single file, but on your hard drive this is actually composed of multiple files which all share the same base filename, but have different extensions. A graphical view of those formats is shown above.\nFor a shapefile to be consider “valid”, it’s common to require just the the .shp, .shx and.dbf files, which I’ve called out as “principle” here. The projection file is the definition of the projection the .sbn and .sbx files are used to optimize spatial analyzes, the .xml file defines metadata, and the .cpg file defines encoding. One of the common limitations of shape files is file size. The size of both .shp and .dbf component files cannot exceed 2 GB (or 231 bytes), which encompasses ~70,000,000 points. Another common limitation is that these files can have no more than 255 attributes (fields) and those field names are limited to 10 constrained characters. This limitation is commonly encountered with high resolution data and authoritative datasets at even the county to national level.\nA note on the definition of principal here. While the technical implementation of reading data from a shapefile requires no projection file to successfully read in, the way the resulting array is placed in space by virtually all software requires some form of spatial definition. At this point, most of the commonly used software will make a default guess or force you to make a declaration. While the latter is preferred in my view (and that of the sf implementations), the former is perfectly acceptable so long as you actually stop to read the notice it places at you and understand the implications. As I say, I prefer the sf implementation of that process. If you give me some data but don’t tell me where it goes, I can faff about all day but I’m not going to try and put it on a map until you tell me where it should go. This statement flies in stark contrast with my actions. See RRASSLER’s shotgun_proj_test.R function, and my inability to detach myself from datums.\nWhile not a strict expression of this format, in the hydraulic science the most common way you’ll encounter a variation on this concept is the HEC-RAS model format. HEC-RAS solves the energy &gt; elevation portion of the integrated modeling stack, and is primarily used to represent channels in 1 Dimension. To encode that representation and all the other inputs the model needs, that data is shared across many files.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Other formats"
    ]
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "From here onward, you’re pretty deep into the weeds of my personal notes. Expect to find a lot more unintelligible, poorly formatted, or otherwise broken thoughts and links. Following in the footsteps of many of my references and inspirations, I’m exposing the nodes of my nonlinear manifesto and uncatagorized output here in the hopes that it makes my life easier, and presents a slightly more stimulating digital footprint for you to explore :) You might also go rummage though the junk drawer.\n\n\n[[20220630144912]] 02 Now and [[20220227202414]] 03 What I’m is pondering these days.\nThis system is a rebellion against my unruly digital footprint as I attempt to coherently tackle my wicked problem with water.\nExplainable hydrology and how to better communicate and execute.\nI want to ‘gamify’ water forecasting. To accomplish that, I need to be smarter and have better systems.\n\nMy ideal game allows players to seamlessly scale from bed (mattress) to bed (Bald Eagle Domain).\nIs a visually striking (representations of real world processes, exportable outputs) game (frictionless interactions).\nShould help facilitate multi-model depth enabled ensemble likelihood flood mapping capabilities usable for the entire country.1\n\nHow the founding values and constitution cross the digital divide.",
    "crumbs": [
      "Atlas",
      "Applications",
      "Now"
    ]
  },
  {
    "objectID": "now.html#right-now-and-at-the-top-of-my-mind",
    "href": "now.html#right-now-and-at-the-top-of-my-mind",
    "title": "Now",
    "section": "",
    "text": "From here onward, you’re pretty deep into the weeds of my personal notes. Expect to find a lot more unintelligible, poorly formatted, or otherwise broken thoughts and links. Following in the footsteps of many of my references and inspirations, I’m exposing the nodes of my nonlinear manifesto and uncatagorized output here in the hopes that it makes my life easier, and presents a slightly more stimulating digital footprint for you to explore :) You might also go rummage though the junk drawer.\n\n\n[[20220630144912]] 02 Now and [[20220227202414]] 03 What I’m is pondering these days.\nThis system is a rebellion against my unruly digital footprint as I attempt to coherently tackle my wicked problem with water.\nExplainable hydrology and how to better communicate and execute.\nI want to ‘gamify’ water forecasting. To accomplish that, I need to be smarter and have better systems.\n\nMy ideal game allows players to seamlessly scale from bed (mattress) to bed (Bald Eagle Domain).\nIs a visually striking (representations of real world processes, exportable outputs) game (frictionless interactions).\nShould help facilitate multi-model depth enabled ensemble likelihood flood mapping capabilities usable for the entire country.1\n\nHow the founding values and constitution cross the digital divide.",
    "crumbs": [
      "Atlas",
      "Applications",
      "Now"
    ]
  },
  {
    "objectID": "now.html#the-grill",
    "href": "now.html#the-grill",
    "title": "Now",
    "section": "The Grill",
    "text": "The Grill\n\n\n\n\n\n\nPresentation navigation tips\n\n\n\n\n\nMy preferred (FOSS) flavor of slidedecks, revealjs, has intuitive but none the less unconventional PowerPoint presentation controls:\n\nSlides dynamically resize to use the entirety of the browser window, but you can still fullscreen with F\nI try and call out my slide layout mode in each presentation but I use vertical slides too often; press space or the down arrow key, not the right arrow, for most viewing situations\n\nYou can press R or add /scroll-view/ to the end of the url to make revealjs presentations more amenable to doom-scrolling motions, and is usually denoted with the phone icon under a presentation\nPress N or space for the next slide, P goes to the previous slide\n\nPress M to open to the menu\nPress O for the slide deck overview\nYou can use the chalkboard to freemouse/touchpad draw\nPress B to black out the presentation screen\nPress S for a speaker view\nPress Alt/Opt + + to increase text size\nPress Alt/Opt + - to decrease text size\nPress Alt/Opt + 0 to reset the text size\nPress Alt/Opt + click on the slide to zoom in\nPress C to declare victory and head home\nPrinting is never recommended, particularly if there are interactive elements in a deck; but if you really need to, follow these instructions:\n\nAdd ?print-mode to the url &gt; Slides will load and look strange, that’s fine\n\nPrint(CTRL + P) and make sure you select the save to pdf option, not print with Microsoft Word\nSet margin to none\nCheck the background image box\nPrint!\n\n\n\n\n\n\n\n\nIcon helpers\n\n\n\n I use for slides,  is scroll view\n For videos,  For publications\n For maps,  For interactive maps\n For data downloads,  or  For attachments\n\n\n\n\n\n\n\n\n\n\n\n\n | \nThis presentation deck is my revolving door of hot-off-the-grill results that I use for informal updates or impromptu meetings. Don’t expect what you find here to remain for very long, or to not change for 2 years because nothing is ever done…",
    "crumbs": [
      "Atlas",
      "Applications",
      "Now"
    ]
  },
  {
    "objectID": "now.html#a-chronological-heap",
    "href": "now.html#a-chronological-heap",
    "title": "Now",
    "section": "A chronological heap",
    "text": "A chronological heap\nThe idea of keeping a “Blog” makes me shudder for some reason. However, it’s nice to keep a public and living record of what I’m up to and thinking about and this is as handy a place to start as any. When I can’t accrete my thoughts into a more meaningful form, or if I’m making a one-off presentation, a guest writeup, or self-trumpeted cross-post I’ll throw it on the top of the heap here:\n\nMy dissertation slides\nLets see how quickly this becomes irrelevant…\n\n\n\n\n\n |  |  | \n\n\nCIROH DevCon 2025\nAt the CIROH Developers Conference, Mike Johnson and the hydrofabric team gave a followup presentation on the advances made over the last year on the Hydrofabric data, processing tools, and the future iterations of the hydrofabric data model. Find that presentation here.\n\n\nAGU 2024\n \nCross-sectional representations of rivers are a critical form of data needed to understand and predict the behavior of the system, particularly for hydraulic and geomorphic applications. However, the collection and curation of these data are non-trivial; and the way they are stored, found, and interacted with are not well described, particularly at continental scale. This work describes the data sources, tools, and techniques used in developing a national scale cross section database. By constructing a foundation of geometric transects augmented with machine learned synthetic bathymetric geometries and a pathway to blend data from HEC-RAS 1D models and parameterizations off eHydro and terrain where feasible, we present an iteration of a 3D channel geometry database. The resulting data provides a standardized and scaled pattern and architecture which applications, which range from hydraulic routing and flood inundation mapping to river morphologic and environmental health and habitat indicators can be seeded. That representation is easily discovered and extracted for a given domain following well established hydrofabric access patterns, and provides a consistent and scaled baseline for the Next Generation Water Resource Modeling Framework and associated applications.\n\n\nRIMORPHIS Workshop 2024\nA presentation on CONUS scale bathy toolings.\n\n\nCIROH DevCon 2024\nAt the CIROH Developers Conference, Mike Johnson and the hydrofabric team gave a followup presentation on the advances made over the last year on the Hydrofabric data and processing tools, including an explicit refactoring and aggregation over the reference fabric and the tie into learned cross section data for channel routing as preparation for NextGen simulation runs. Find that tutorial here.\n\n\nAGU 2023\n\n\n\n\n\nThe evolution of computational sciences leads to new data and more efficient formats and processing architectures to store, distribute, consume, and archive the data that are critical to model execution. These advances often lead to new use cases for those data, which have a rich legacy and can be costly to collect, collate, and archive; but the form of that initial deployment sometimes makes integrating those original data into newer use case workflows difficult. Those conflicting values create a need to make those model data more interoperable and reusable, and a need to increase their findability and accessibility, a manifestation of the FAIR data principles. This presentation will demonstrate how legacy HEC-RAS one-dimensional model datasets are transformed and made more FAIR for uses in work streams across the NOAA flood inundation mapping efforts and in the development of a nationwide 3D hydrofabric dataset that provides channel geometry between deep channel and its floodplain. After attending, participants should leave with a deeper understanding of the challenges, solutions, and tools available to manipulate and integrate these data into frameworks that extend far beyond their initial spatial scale; with a heavy emphasis on the details needed to integrate cross section representations and their reuse in HEC-RAS adjacent applications. This also provides the opportunity to contribute and grow FAIR data transformation toolings and the communities that facilitate a bottom-up approach to scaling local hydraulic data to continental-scale efforts.\n\n\n\n\n\nAlongside David Weiss, presented an intercomparison between two geospatial means of constructing a Flood library (FLDPLN and HAND).\n\n\nBuilding for a moving target (ESIP 2023)\n\n\n\n\n\nLast week at ESIP’s 25th Annual Summer meeting in Burlington, VT , Lynker’s Mike Johnson, Jim Coll, Angus Watters, Justin Singh, and Rachel Bash presented on results from efforts funded through an ESIP Lab grant to improve the pathways and toolings around open data access representing their abstract “Building a Federated Data Catalog with Client Implementations, Meeting Data Where it Lives”. By demonstrating an operational implementation of an automatically updating access point with multi-language support, we lower the barriers to scientific inquiry and open the doors to and between the larger open science community. ESIP (Earth Science Information Partners) Meetings are collaborative, community-driven events that bring together federal partners and the most innovative thinkers & leaders around earth science data. Learn more about the Federated catalog at https://doi.org/10.6084/m9.figshare.23710017.v1, ESIP at their website here: https://www.esipfed.org/, and Lynker here: https://lynker.com/ (Johnson et al. 2023)\n\n\nESIP January Meeting\n\n\n\n\n\n\n\nAGU 2022\n\n\n\n\n\n\n\nField mapping with drones (KU Geology Department and Field Station)\nA link to an older presentation I gave as part of the centennial field camp.\n\n\nKU Field Station Science on Tap (Drones)\nYou can find a copy of that powerpoint here (as soon as I find it).\n\n\nAGU - 2019\n\n\n\nKansas Governers Conference on the Future of Water in Kansas - November 2021\nhttps://youtu.be/uapIeizWz2s?si=Dbj_kYleNiv22-DY&t=1005\n\n\nGSA 2014",
    "crumbs": [
      "Atlas",
      "Applications",
      "Now"
    ]
  },
  {
    "objectID": "now.html#footnotes",
    "href": "now.html#footnotes",
    "title": "Now",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ncredit to Derek via Tom for the mission↩︎",
    "crumbs": [
      "Atlas",
      "Applications",
      "Now"
    ]
  },
  {
    "objectID": "metro_primer.html",
    "href": "metro_primer.html",
    "title": "Metrology",
    "section": "",
    "text": "_1349f3bd3aabba5d6cc1a4c6dabeeaa4.png\n\n\n\nTo CoPilot: “Can you draw me a picture of Metrology and Surveying without people”\n\nOne of the most exciting parts of studying earth science is that we can generate a theory in the lab, and then just go outside and observe how that theory plays out! The next natural step is to start modeling that system, in an exercise to both confirm our understanding of the universe and to enable predictions and reflections. In order to model you need some numbers, and that’s where metrology steps in. Metrology is the science of measurement, and by being able to repeatably quantify the state and fluxes of the world we can accomplish a lot of really neat things! As with many things in life however, it’s never that simple. As the old adage goes: “Garbage in, Garbage out” attempts to speak to the need to accurately replicate “truth” in your model data. It can also be difficult to add geographic context to a lot of the data we collect, where a visual sniff test is typically the precursor to identifying that garbage. It’s also rather expensive. While technology continues to decrease the time needed to get a high-accuracy measurements of the shape of the landscape (surveying), measurement of hydrologic fluxes remains elusive and requires more specialized equipment and man hours are not cheap to come across, particularly when the collection of that data requires a great deal of skill and training to correctly acquire.\nI’d like to push back on some of those ideas however. One of the more visible manifestations of our technical revolution was the increase in accessibility and quality of cameras and sensors which converge on the consumer/prosumer market in the form of Commercial Off The Shelf (COTS) drone tech. While you can’t measure everything with computer vision, we can infer and back into a lot of things, and the hardware and software built into these drones enables almost anyone to fly safely and effectively. Processing chains are also a bit more accessible with the increase in Free and Open Source Software and computational capabilities.. For some help learning the ropes of earning your remote pilots licence, see my licence page. For help collecting and visualizing that data, see my helpers and how-to’s below:\n\nGPS\nSFM\nPIV",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology"
    ]
  },
  {
    "objectID": "landforms.html",
    "href": "landforms.html",
    "title": "Landforms",
    "section": "",
    "text": "https://www.biointeractive.org/classroom-resources/earthviewer",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Landforms"
    ]
  },
  {
    "objectID": "landforms.html#techtonics",
    "href": "landforms.html#techtonics",
    "title": "Landforms",
    "section": "",
    "text": "https://www.biointeractive.org/classroom-resources/earthviewer",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Landforms"
    ]
  },
  {
    "objectID": "introduction_slides.html#about-jim",
    "href": "introduction_slides.html#about-jim",
    "title": "So long as you get the GIS&T of it",
    "section": "About Jim",
    "text": "About Jim\n\n\n\n\n\n\nGoals:\n\nTo communicate a clearer picture of the technical friction experienced using HEC-RAS models.\n\nOutcomes and Takeaways:\n\nAn understanding of the sorts of questions that are hard to answer without something like RRASSLER.\n\n\n\n\n\n\n\n\n\n\nYou go gurl!\nBackground image from: OWP template / Water PNGs by Vecteezy, https://www.vecteezy.com/members/ahasanaraakter\nRelevant: _Water_color.png / _Water_bw.jpg (cover) _vecteezy_ai-generated-water-wave-splash.png / _vecteezy_ai-generated-water-wave-splash_bw.png / _vecteezy_ai-generated-water-wave-splash_bw_blur.png (contain)"
  },
  {
    "objectID": "install_zotero.html",
    "href": "install_zotero.html",
    "title": "Installing Zotero",
    "section": "",
    "text": "Reading an academic paper is unlike reading in a traditional sense because there are a set of objectives and key pieces of information you want to be able to easily reference and re-obtain. If you ever find that you need to cite something, Zotero is the way to go about doing that. In a really tight spot, citation machine will get you by, but it really is worth investing the time to do this the right way. I install Zotero and set the location to a folder hosted on the cloud (Dropbox). Although Zotero will complain about hosing its database on the cloud, I have yet to be bitten by a corrupt database and have many times been bitten by a failing hard drive, accidental delete, or computer gremlins. As I mentioned I don’t use computers the right way, and I keep a very strange mix of notes linked through markdown using Zotero and Zettlr, loosely informed off of how both Hendrik Erz and Andy Matuschak take and form their notes and digital footprint. Despite how little it seems to have produced and how terrible I am, I’m rather fond of this pattern and documenting it keeps me more consistent. If you are only interested in using Zotero the “correct” way, download the app and browser connector, it’s solid software that just works. Otherwise, I’ve outlined how I use this to take notes as part of my data driven workflow.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Zotero"
    ]
  },
  {
    "objectID": "install_zotero.html#install-and-setup",
    "href": "install_zotero.html#install-and-setup",
    "title": "Installing Zotero",
    "section": "Install and setup",
    "text": "Install and setup\n\nDownload and install Zotero 7, at this point the chrome extension is probably following your Google account but if not the connector plugin as well.\nDownload “Mandatory” extensions (.xpi files), which at as of Zotero 6.0.26+ only include better-bibtex for cls export.\n\nDirect link: https://github.com/retorquere/zotero-better-bibtex/\n\nInstall addons in zotero by clicking tools &gt; Add-ons and then on the gear icon on the left click Install add-on from file\n(optional) I set up the annotation export to more seamlessly mesh with my Zettlr-esque markdown layout by searching for “annotation” and change the following:\n\nextensions.zotero.annotations.noteTemplates.highlight : &lt;p&gt;&lt;blockquote&gt;{{highlight}}&lt;/blockquote&gt; {{citation}} {{if tags}} - #{{tags join=' #'}}{{endif}} &lt;br&gt; {{comment}}&lt;/p&gt;\nextensions.zotero.annotations.noteTemplates.note : &lt;p&gt;{{citation}} {{if tags}} - #{{tags join=' #'}}{{endif}} &lt;br&gt; {{comment}}&lt;/p&gt;\n\n\n\n\nTips for a more portable “remote” work setup\nif I’m away for a longer trip and don’t have the comfort of my desktop to work on, one of the better ways to maintain productivity is to catch up on that reading backlog. As noted above, you can kill two birds with one stone by setting Zotero to save in a cloud location which both gives you that remote accessibility and off-site backup checks, and although it may complain about writing in the cloud I have yet to regret that decision. In Edit &gt; Settings &gt; Advanced, you should set your “Data Directory Location” to that folder (…/ZoteroDBase in my case). Although this doesn’t allow you to seamlessly transition between computers, it does make it pretty painless to zip, ship, unzip, read and annotate, rezip, reship, and merge without tracking individual files and with only a Zotero install on the remote end of that.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Zotero"
    ]
  },
  {
    "objectID": "install_zotero.html#workflow",
    "href": "install_zotero.html#workflow",
    "title": "Installing Zotero",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\n\n\n\nExpand for Quick Cheatsheet\n\n\n\n\n\nTags:\n\nMETA:toread\nMETA:annotated\n\nMETA:reference_note\n\nMETA:presented\nMETA:extracted\n\nMETA:source\nMETA:abstract\nMETA:cleanedtext\n\n\n\nZotero NotesZettlr Notes\n\n\n\nMETA:reference_note\n\n```{md}\n# Reference note (literature):\n## What was the question, problem or purpose of the study?\n## What methods are deployed?\n### What data did they use?\n### What noteable codebase/tools were developed/deployed?\n## What are the authors’ major findings/conclusions?\n## What evidence supports their interpretations?\n## Why are the findings of this research important?\n## Explicit next steps they suggest\n## Did I find anything surprising?\n## What am I confused about?\n## Interesting figures?\n## Promising References\n# Text annotations\n\n# Reference note (tech):\n## What was the issue they were solving\n## How did they solve it\n## How did I apply that to what I was doing.\n## What is the teaching application this could go to?\n\n# Reference note (random):\n## What was I doing when I found this / Why did I want to save it?\n## How do I think this will be applicable beyond this?\n```\n\n\n```{md}\n---\ntitle: \"&lt;paper title&gt;\"\nid: \nkeywords:\n  - Drone\n  - lo_q-med_q-hi_q\n...\n[citation]\n# &lt;paper title&gt;\n# Self Synthisis\n## Extracted Abstract\n# Text Summeries\n## Mine\n## MaxF\n## TFIDF\n# Annotations\n```",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Zotero"
    ]
  },
  {
    "objectID": "install_zotero.html#finding-documents",
    "href": "install_zotero.html#finding-documents",
    "title": "Installing Zotero",
    "section": "Finding documents",
    "text": "Finding documents\nGoogle, Reference section of good papers that you read, Litmaps and an open source alternative: CitationGecko\n\nOnce I’ve found it I’ll add it directly to Zotero (either by drag-and-drop) or by adding it with the connector. Tag all incoming papers with appropriate tags but most always the META:toread tag, which makes sorting your references easier.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Zotero"
    ]
  },
  {
    "objectID": "install_zotero.html#reading-a-document",
    "href": "install_zotero.html#reading-a-document",
    "title": "Installing Zotero",
    "section": "Reading a document",
    "text": "Reading a document\n\nI place an empty note at the top of the document and paste the template below into that. Tag that annotation with META:reference_note\n\n```{md}\n# Reference note (literature):\n## What was the question, problem or purpose of the study?\n## What methods are deployed?\n### What data did they use?\n### What noteable codebase/tools were developed/deployed?\n## What are the authors’ major findings/conclusions?\n## What evidence supports their interpretations?\n## Why are the findings of this research important?\n## Explicit next steps they suggest\n## Did I find anything surprising?\n## What am I confused about?\n## Interesting figures?\n## Promising References\n# Text annotations\n\n# Reference note (tech):\n## What was the issue they were solving\n## How did they solve it\n## How did I apply that to what I was doing.\n## What is the teaching application this could go to?\n\n# Reference note (random):\n## What was I doing when I found this / Why did I want to save it?\n## How do I think this will be applicable beyond this?\n```\n\nHighlight and annotate the paper as I see fit, correcting the notes/extractions as best I can that first pass.\nonce done, tag the annotations as I see fit.\nRight click on the file and Add Note from Annotation.\nTag reference as META:annotated",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Zotero"
    ]
  },
  {
    "objectID": "install_zotero.html#auto-summerizing",
    "href": "install_zotero.html#auto-summerizing",
    "title": "Installing Zotero",
    "section": "Auto-summerizing",
    "text": "Auto-summerizing\nWhile I usually prefer to do things the hard way, one of the easy ways to accelerate the process of reading a text is to clean it as you go. Using markitdown to automatically copy the text out of a PDF and into a markdown file meets two of my objectives at the same time but you still need to clean that text and so there’s still the deliberate process of ingesting and interacting with the knowledge in a document that you would otherwise skip in the comparably shallower read we perform with Natural Language Processing. Do this by…\n\nExtracting text\n\nRun markitdown over the source file:\n\n```{anaconda prompt}\nset PYTHONUTF8=1   # Only need to do this once if you see an encoding error\nmarkitdown \".\\ZoteroDBase\\storage\\9JF26F9X\\Coll and Li - 2018 - Comprehensive accuracy assessment of MODIS daily s.pdf\" &gt; .\\text.md\n```\n\nFor Abstract.md, keep just the text.\nFor text.md, keep all headers (e.g. “1. Introduction:”) and figure titles, clean the rest up.\nAttach these to the original reference item in Zotero.\nTag the reference with: META:extracted.\nTag the PDF with META:source.\nRun SummerizeR over those files, and copy-paste the outputs into the literature note.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Zotero"
    ]
  },
  {
    "objectID": "install_zotero.html#adding-that-to-my-literature-notes",
    "href": "install_zotero.html#adding-that-to-my-literature-notes",
    "title": "Installing Zotero",
    "section": "Adding that to my literature notes",
    "text": "Adding that to my literature notes\n\nCreate a new file in the literature_notes section and add the template header.\n\n```{md}\n---\ntitle: \"&lt;paper title&gt;\"\nid: \nkeywords:\n  - Drone\n  - lo_q-med_q-hi_q\n...\n[citation]\n# Summary\n## Mine\n## MaxF\n## TFIDF\n# Annotations\n```\n\nCopy-paste the relevant details and annotation into the “literature_notes” folder\n\nFrom Brian Lagotte:\n\nIsolate relevant passages from the literature relevant to your research.\n\ncollecting passages that strike us as interesting and related\n\nConsolidate relevant passages from the literature relevant to your research.\n\ndiscarding passages that are suddenly not so relevant\nmaking source files\n\nIntegrate relevant passages from the literature relevant to your research.\n\nsynthesizing passages that reveal something interesting for you\ncreating paragraph outlines to see which ones make the paper\ndiscover your throughline",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Zotero"
    ]
  },
  {
    "objectID": "install_zotero.html#accreting-that-into-knowledge",
    "href": "install_zotero.html#accreting-that-into-knowledge",
    "title": "Installing Zotero",
    "section": "Accreting that into knowledge",
    "text": "Accreting that into knowledge\nMoving from literature notes to more concrete and meaningful output is more elaborately explained in my workflow outline.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Zotero"
    ]
  },
  {
    "objectID": "install_docker.html",
    "href": "install_docker.html",
    "title": "Installing Docker",
    "section": "",
    "text": "Docker is, at least in theory, a really neat way to manage your environment. Like computers, occasionally that’s true and it’s really handy. More often than not, you’ll wonder why this needed to be this complicated as you cry curled up in a ball on the floor at 1 in the morning (case in point: https://docs.docker.com/build/architecture/#install-buildx). Below I’ll walk you though my use case, ecosystem, and how I eventually got it up off the ground.\n\n\n\n\n\n\nThe futility of documentation\n\n\n\nPart of my struggle is that of the “early” adapter. As I sit here in March of 2023, there are key setup steps in windows that are no longer critical to perform that were at the start of the year, including transitions from wsl to wsl2 and off brand distros that I believe are no longer necessary after systemd was added. What I have now works for the most part so I’m unwilling to break it to test these changes, but I’m sure in 6 months something else will have changed anyways.\nHaving spent the day unsuccessfully getting docker to install (it would load an image once and then never start again), Based on this post and the Windows WSL docs, here is the process that finally worked for me. See intro note above related to my issues here.\n1) Reset any previous wsl install following https://pureinfotech.com/reset-wsl2-linux-distro-windows-10/\n2) We need a special version of wsl that has the systemcdl command, I chose distrod.\n3) Install ubuntu with focal\n4) reboot pc\n5) !Success\n\nEdit: Case in point. I rebuilt my environment 3 months later and it’s 4 lines. Why do I even bother…",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Docker"
    ]
  },
  {
    "objectID": "install_docker.html#windows-only-enable-wsl",
    "href": "install_docker.html#windows-only-enable-wsl",
    "title": "Installing Docker",
    "section": "Windows only, enable wsl",
    "text": "Windows only, enable wsl\nIn administrative PowerShell: wsl --install\n\nMisc.: WSL maintinence\n```{bash}\nsudo apt update && sudo apt upgrade\n```",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Docker"
    ]
  },
  {
    "objectID": "install_docker.html#install-buildx",
    "href": "install_docker.html#install-buildx",
    "title": "Installing Docker",
    "section": "Install buildx?",
    "text": "Install buildx?\nDoes not appear to be strictly necessary at this point but the warning does indicate this step will be required sooner or later.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Docker"
    ]
  },
  {
    "objectID": "install_docker.html#install-docker",
    "href": "install_docker.html#install-docker",
    "title": "Installing Docker",
    "section": "Install docker",
    "text": "Install docker\n```{bash}\nsudo apt-get update\nsudo apt install docker.io\nsudo snap install docker\n```\n\nTesting a “Hello world”\n```{bash}\ndocker --version\ndocker run hello-world\n```",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Docker"
    ]
  },
  {
    "objectID": "install_docker.html#running-docker-containers-in-vs-code",
    "href": "install_docker.html#running-docker-containers-in-vs-code",
    "title": "Installing Docker",
    "section": "Running Docker containers in VS Code",
    "text": "Running Docker containers in VS Code\nLaunch VS code on local GFE Install extensions Remote-SSH Remote-SSH: Editing Conf Docker Python\nSelect Remote Window status bar at the bottom left corner of the VS Code GUI: Open SSH configuration file -&gt; C:&lt;user&gt;.sshand update the following:\n```{md}\nHost &lt;VM name&gt;\n    HostName &lt;VM name&gt;\n    User &lt;firstname.lastname&gt;\n    IdentityFile ~/.ssh/id_rsa (generate this in Moba from the VM terminal)\n```\nSelect Remote Window status bar -&gt; Connect to Host. You should see a list of all available Hosts from the config file. A new GUI will open and you should see the SSH connection in the Remote Window status bar. From this GUI, opening a terminal will connect to the VM and opening a file/folder will reference the local VM directory. Running a new docker container or attaching to an existing container in the terminal will connect to the container and you should be able to run files and lines of code from the editor in the container.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Docker"
    ]
  },
  {
    "objectID": "install_docker.html#accessing-jupyter-notebook-in-docker-image",
    "href": "install_docker.html#accessing-jupyter-notebook-in-docker-image",
    "title": "Installing Docker",
    "section": "Accessing Jupyter Notebook in docker image",
    "text": "Accessing Jupyter Notebook in docker image\n```{bash}\ndocker run --rm -it --name &lt;name&gt; -v &lt;path to repo&gt;:/repo/ -v &lt;path to data&gt;:/data/ --expose 8080 -p 8080:8080 &lt;image:version&gt;\njupyter-notebook --no-browser --ip 0.0.0.0 --port=8069 --allow-root\n```",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Docker"
    ]
  },
  {
    "objectID": "install_docker.html#adding-docker-user-to-group",
    "href": "install_docker.html#adding-docker-user-to-group",
    "title": "Installing Docker",
    "section": "Adding docker user to group",
    "text": "Adding docker user to group\n```{bash}\n# check /etc/group to see if the docker group exists\ncat /etc/group | grep docker\n\n# create a docker group if it does not exist\nsudo groupadd docker\n\n# add yourself to the docker group\nsudo usermod -aG docker $(whoami)\n```",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Docker"
    ]
  },
  {
    "objectID": "install_docker.html#python",
    "href": "install_docker.html#python",
    "title": "Installing Docker",
    "section": "Python",
    "text": "Python\n\nmore minimal: https://github.com/datawire/hello-world-python\n\n\nFrom FIM\n\ngit clone https://github.com/NOAA-OWP/inundation-mapping.git\nBuild Docker Image : docker build -f Dockerfile -t owpfim:1.0 &lt;path/to/inundation-mapping&gt;\n‘docker run –rm -it –name  -v /:/foss_fim -v /:/outputs -v /:/fim_temp -v :/data owp_fim:1.0’\nCreate FIM group on host machine: groupadd -g 1370800178 fim\nChange group ownership of repo (needs to be redone when a new file occurs in the repo): chgrp -R fim &lt;path/to/repository&gt;\njupyter-notebook --no-browser --ip 0.0.0.0 --port=8787 --allow-root\n\n\n\nFrom PANGEO\n\ngit clone https://github.com/pangeo-data/pangeo-docker-images.git\nBuild Docker Image : docker build -f Dockerfile -t pangeo:1.0 /path/to/pangeo-docker-images\nLaunch a notebook: docker run -it --rm -p 8888:8888 pangeo:1.0 jupyter lab --ip 0.0.0.0\n\n\n\nFrom R Docker (& personal dev env - geodev)\n\ngit clone https://github.com/rocker-org/rocker-versioned2.git\n[optional] extend the Dockerfile to install desired packages.\n\nFrom top of repo, build Docker Image:\n\nMine: docker build -f ./dockerfiles/geodev.Dockerfile -t geodev:1.0 /path/to/rocker-versioned2\nRStudio: docker build -f ./dockerfiles/rstudio_4.3.1.Dockerfile -t rstudio:1.0 /path/to/rocker-versioned2\n\nLaunch the image: docker run -it --rm -p 8787:8787 -p 4200:4200 -e PASSWORD=YOURNEWPASSWORD -e ROOT=TRUE geodev:1.0\nPoint to localhost:port, (http://localhost:8787 in our example as specified in the dockerfile and exposed in the image mount)\n\nusername: rstudio\npassword: YOURNEWPASSWORD\n\n\n\nRestarting rstudio-server\nI break RStudio often and have a habit of not saving. Unlike a local env, if you just close without having saved you may lose changes. First, SAVE OFTEN. Second, One potential solution is to “log” into the container as root and restart the daemon. The operating system for rocker/rstudio:4.2.2 is Ubuntu and the steps below should work for Ubuntu/Debian.\n\nlog into the container, change the container ID/name accordingly: docker exec -it rstudio_server /bin/bash\nlist services: service --status-all\nrestart RStudio: service rstudio-server restart\nexit and log back into RStudio\nHopefully when you visit the RStudio Server page again the page is responsive again you are ok, and SAVE MORE OFTEN!\n\n\n\n\nAside: Site Building with Quarto\n\n\n\n\n\n\nImportant\n\n\n\nAnyone know how to get reticulate to work in docker?\n\n\n\nSome misc. setup:\n\nTest PDF rendering: \nExtension installation for revealjs\n\nI like the ability to increase font when folks complain that the text is too small to see1 : quarto add gadenbuie/revealjs-text-resizer\nI tend to overuse QR codes when I need to deploy them, so to make that aspect a little more seamless I add: quarto install extension jmbuhr/quarto-qrcode \nQuizzes are an important tool in the learning process: https://github.com/parmsam/quarto-quiz\nFont awesome helpers: quarto add quarto-ext/fontawesome\nAnd even more flair to express myself: quarto add ArthurData/quarto-confetti\n\n\nQuarto docs\nSet up Zotero so citations are inline &gt; quarto preview --host 0.0.0.0 --port 4200 --no-browser &gt; http://localhost:4200/ &gt; quarto render &gt; git push docs\n[[20241119183423]] Quarto-revealjs helpers\n\n\n\nAside: Building R packages\n\nsee https://r-pkgs.org/whole-game.html#write-the-first-function, https://yonicd.github.io/sinew/articles/motivation.html, https://github.com/jthomasmock/pkg-building, and https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/\n\nI have no intention of taking a deep dive into this (docs | cheatshet) but in general you should:\n\nMake a new package in a new folder, separate functions out.\nBuild your README.rmd using this template:\n\n```{md}\n---\noutput: github_document\n---\n\n&lt;!-- README.md is generated from README.Rmd. Please edit that file --&gt;\n\n#`#`#`{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  eval = FALSE,\n  comment = \"#&gt;\",\n  fig.path = \"man/figures/README-\",\n  out.width = \"100%\"\n)\n#`#`#`  &lt;!-- remove the # --&gt;\n\nmarkdown here...\n```\n\nAttribute with: withr::with_dir(getwd(), usethis::use_mit_license(name = \"Cornholio\"))\nMake HTML with: usethis::use_pkgdown()\nAppend missing namespace with: sinew::pretty_namespace(getwd(),overwrite = TRUE)\nMake first cut headers with:\n\n```{r}\nusethis::use_pkgdown()\nusethis::use_pkgdown_github_pages()\nsinew::sinew_opts$set(markdown_links = TRUE)\nsinew::makeOxyFile(input = getwd(), overwrite = TRUE, verbose = FALSE)\n```\n\nYou may need to manually add imports such as these like so:\n\n```{r}\n#' @import magrittr\n#' @import data.table\n#' @importFrom foreach `%do%`\n#' @importFrom foreach `%dopar%`\n\nmarco &lt;- function(in_value=TRUE) {\n  # sinew::moga(file.path(getwd(),\"R/hello.R\"),overwrite = TRUE)\n  # devtools::document()\n  # pkgdown::build_site(new_process=TRUE)\n  # devtools::load_all()\n  # \n  # marco(in_value=TRUE)\n  \n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  # -- start -----------------------------------------------------------------------------------------------------\n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  \n  print(\" ⚠ WARNING WIZARD ⚠  \")\n  print(\"                     \")\n  print(\"    ⚠              \")\n  print(\" (∩｀-´)⊃━☆ﾟ.*･｡ﾟ  \" )\n\n  return(TRUE)\n}\n```\n\nAs we iterate we:\n\nTest functions with: devtools::load_all()\nMake new headers with: sinew::moga(file.path(getwd(),\"R/hello.R\"),overwrite = TRUE) and copy output into the file.\nRecreate .Rd files with: devtools::document()\nDelete the markdown version of the readme and run pkgdown::build_site(new_process = TRUE) or press the “knit” button.\n\nremove the docs line from .gitignore and push. Publish that as the github pages. If you are pushing to a newly created empty repo that will look something like:\n\n```{bash}\ngit init\ngit remote add origin https://github.com/JimColl/RRASSLER.git\ngit commit -m \"first commit\"\ngit branch --move master main\ngit push --set-upstream origin main\n```",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Docker"
    ]
  },
  {
    "objectID": "install_docker.html#footnotes",
    "href": "install_docker.html#footnotes",
    "title": "Installing Docker",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nas they sit in the back of the room looking down at their phone↩︎",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Docker"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "The world presents us with its unbounded complexity, and it is up to each of us to deal with this in our own way. While simplification might save us from sensory overload and decision paralysis, these same simplifications can often be a massive source of friction between the current moment and the desired outcome, and a huge stumbling block in communicating when they are not shared. I truly believe that a well-trained geographer has many of the skills and toolsets needed to slide through that friction to address these complexities, tackling the most pressing questions this world has to offer in a coherent and intelligent manner. GIScience and Technology (GIS&T) is a critical subset of those tools, and a great way to help frame and analyze the space around you, both literally and figuratively. I’ve gotten a lot of utility and benefit from learning, using, documenting, and teaching these topics and it is my hope that I can pass some of that knowledge along to you. I’ve placed a lions share of my teaching portfolio and notes here for the benefit of all and my own selfish desire to streamline my digital footprint. This site, like myself, is a constant work in progress. I’ve attempted to standardize my formatting but you have landed on the accumulation and synthesis of almost every project I’ve worked on, class I’ve taught, and more than a quarter of a century of technical development since I first attempted (and miserably failed) to host a website. That is to say, you might find a few inconsistencies. I also write this site in markdown-ish using at least 2 apps, so spellchekc and local testing only take me so far. If you find errors (spelling, formatting, or otherwise), or if something needs clarification or fixing please reach out!\nAs a general note: While I reread and revisit the links you’ll find across this site frequently; I point to them as resources and references and you do not need to buy, subscribe, or log in to anything you do not want to. Otherwise, feel free to lurk along with my adventure! I’ve found GIS to be a profoundly useful tool that helps me add structure, reason, and logic to this otherwise variable world, and my hope is that you’ll find your time here well spent, or at the very least take away a few useful tidbits and a flash of inspiration.\n\nUpdate:\nHaving rebuilt my system and transitioned into a consultant whose job seems to revolve around mimicking a software engineer, operationalizing the new and shiny, and delivering value in micro-increments as fast as possible; as opposed to an academic whose goal is to synthesize the domain for instruction and purposeful process improvements, more of my time now goes to updating my Atlas and not my classes. After losing a hard drive and the source files they were generated from, and a fundamental paradigm shift in content and form, they are in very rough shape. As soon as I find that time machine I’d like to get them back into a serviceable form, but for now, they are what they are. Please, reach out if you think getting them back into use is something you can help catalyze!",
    "crumbs": [
      "The GIS&T of it"
    ]
  },
  {
    "objectID": "hydrofab_vis.html",
    "href": "hydrofab_vis.html",
    "title": "Hydrofabric Visualizations",
    "section": "",
    "text": "We’ll demo these with a toy AOI but keep in mind that this could be any desired AOI.\n\n\nReading layer `HUC2' from data source \n  `/home/rstudio/Dropbox/root/database/hosted/water/HUC2.fgb' \n  using driver `FlatGeobuf'\nSimple feature collection with 22 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.2297 ymin: -14.61019 xmax: 179.8567 ymax: 71.43957\nGeodetic CRS:  WGS 84\n\n\nReading layer `tl_2022_us_state' from data source \n  `/home/rstudio/Dropbox/root/database/hosted/admin/tiger/TIGER2022/STATE/tl_2022_us_state/tl_2022_us_state.fgb' \n  using driver `FlatGeobuf'\nSimple feature collection with 56 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.2311 ymin: -14.60181 xmax: 179.8597 ymax: 71.43979\nGeodetic CRS:  NAD83\n\n\nReading layer `pois' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\n\n\nReading layer `network' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\n\n\nReading layer `flowpaths' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\nSimple feature collection with 20667 features and 12 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 1825498 ymin: 2215930 xmax: 2255727 ymax: 3012977\nProjected CRS: NAD83 / Conus Albers\n\n\nReading layer `divides' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\nSimple feature collection with 20641 features and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1824825 ymin: 2215605 xmax: 2258235 ymax: 3086895\nProjected CRS: NAD83 / Conus Albers\n\n\nReading layer `flowlines' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\nSimple feature collection with 64148 features and 17 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 1825498 ymin: 2215930 xmax: 2255727 ymax: 3012977\nProjected CRS: NAD83 / Conus Albers\n\n\nReading layer `flowline_divides' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\nSimple feature collection with 42009 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1824825 ymin: 2215935 xmax: 2255715 ymax: 3086895\nProjected CRS: NAD83 / Conus Albers\n\n\n\nAOI &lt;- AOI::aoi_get(sf::st_buffer(divides[divides$divide_id == 16071,],units::as_units(100000,'m')))\n# AOI &lt;- AOI::aoi_get(county = \"Hillsborough\",state = \"NH\")\n\nTests\n\n\n\nOverviewsNetworksTasteys\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: H - HUC02 Overview Map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: H - HUC02 Traditional Overview\n\n\n\n\n\n\n\nThis is a caption for image 3.\n\n\nThis is a caption for image 4.\n\n\nThis is a caption for image 5.\n\n\nThis is a caption for image 6.\n\n\nThis is a caption for image 7.\n\n\n\n\nHere’s some content for the second tab. You can put anything you like here!\n\n\nMore content for the third tab.\n\n\n\n\n\n\n\n\n\n\n\nNo more, go home"
  },
  {
    "objectID": "hydrofab_vis.html#the-community-hydrofabric-visualization-menu",
    "href": "hydrofab_vis.html#the-community-hydrofabric-visualization-menu",
    "title": "Hydrofabric Visualizations",
    "section": "",
    "text": "We’ll demo these with a toy AOI but keep in mind that this could be any desired AOI.\n\n\nReading layer `HUC2' from data source \n  `/home/rstudio/Dropbox/root/database/hosted/water/HUC2.fgb' \n  using driver `FlatGeobuf'\nSimple feature collection with 22 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.2297 ymin: -14.61019 xmax: 179.8567 ymax: 71.43957\nGeodetic CRS:  WGS 84\n\n\nReading layer `tl_2022_us_state' from data source \n  `/home/rstudio/Dropbox/root/database/hosted/admin/tiger/TIGER2022/STATE/tl_2022_us_state/tl_2022_us_state.fgb' \n  using driver `FlatGeobuf'\nSimple feature collection with 56 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.2311 ymin: -14.60181 xmax: 179.8597 ymax: 71.43979\nGeodetic CRS:  NAD83\n\n\nReading layer `pois' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\n\n\nReading layer `network' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\n\n\nReading layer `flowpaths' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\nSimple feature collection with 20667 features and 12 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 1825498 ymin: 2215930 xmax: 2255727 ymax: 3012977\nProjected CRS: NAD83 / Conus Albers\n\n\nReading layer `divides' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\nSimple feature collection with 20641 features and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1824825 ymin: 2215605 xmax: 2258235 ymax: 3086895\nProjected CRS: NAD83 / Conus Albers\n\n\nReading layer `flowlines' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\nSimple feature collection with 64148 features and 17 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 1825498 ymin: 2215930 xmax: 2255727 ymax: 3012977\nProjected CRS: NAD83 / Conus Albers\n\n\nReading layer `flowline_divides' from data source \n  `/home/rstudio/data/raw/hydrofabric/prototype/01.gpkg' using driver `GPKG'\nSimple feature collection with 42009 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1824825 ymin: 2215935 xmax: 2255715 ymax: 3086895\nProjected CRS: NAD83 / Conus Albers\n\n\n\nAOI &lt;- AOI::aoi_get(sf::st_buffer(divides[divides$divide_id == 16071,],units::as_units(100000,'m')))\n# AOI &lt;- AOI::aoi_get(county = \"Hillsborough\",state = \"NH\")\n\nTests\n\n\n\nOverviewsNetworksTasteys\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: H - HUC02 Overview Map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: H - HUC02 Traditional Overview\n\n\n\n\n\n\n\nThis is a caption for image 3.\n\n\nThis is a caption for image 4.\n\n\nThis is a caption for image 5.\n\n\nThis is a caption for image 6.\n\n\nThis is a caption for image 7.\n\n\n\n\nHere’s some content for the second tab. You can put anything you like here!\n\n\nMore content for the third tab.\n\n\n\n\n\n\n\n\n\n\n\nNo more, go home"
  },
  {
    "objectID": "hydrofab_dm.html",
    "href": "hydrofab_dm.html",
    "title": "Hydrofabric data model",
    "section": "",
    "text": "While the official vignette gives you a more formal definition of the term “hydrofabric”, if that doesn’t do it for you, the short(er) answer I’d give you is:\n“The Hydrofabric data are the geometric features delineating channel flow and contributing drainage area, the topologic relationships linking the network together, and the attributes providing the critical landscape attributes needed for hydrologic and hydraulic model execution.”\nWhile the term “hydrofabric” has been used to describe concepts as narrow in scope as “a set of cartographic lines” to the entire data model used in the NextGen water modeling framework all the way up to entire branches of teams and workstreams, the most succinct and appropriate way to use this term is in describing both 1) a conceptual standard as defined by the OGC, and 2) the flowline –&gt; catchment discretization of the landscape (contra: Semantics are important, I just hate them)\nThis page deals with the first of those. If you have an immediate task in front of you you likely want to jump straight into the data so just note that the theoretical foundation lives here if you get confused about what you find."
  },
  {
    "objectID": "hydrofab_dm.html#hydrofabric-as-an-ogc-standard",
    "href": "hydrofab_dm.html#hydrofabric-as-an-ogc-standard",
    "title": "Hydrofabric data model",
    "section": "Hydrofabric as an OGC standard",
    "text": "Hydrofabric as an OGC standard\na class of data needed to represent the land surface using flowline/catchment topologies.\n“Hydrofabric” is the term used to describe the data model and associated product of flow-path representations and catchments and is typically derrived from mapping applications at 1:24000 scale or greater (1 inch on the map is ~0.3 miles in the world). In the US, the authoritative source of that data stems from the NHD Flowlines (formally termed flownetwork) are not a part of NHD but have been added in as a derrived product id3dhp is unique to all hydrofabric and database and links to both flownetwork, catchments, and waterbodies. Mainstem and drainage basins are a self-similar tree that helps us cross scales. By doing this we have a backbone to work against.\nHY_Features is a conceptual model of hydrogarphy, the hydrofabric is a realization of that that follows better current computational and implementation requirements Mainstems are catchment unique levelpaths\nA tool we can use to cross production boundaries (HUC4’s)\nIt’s worth taking a quick detour to describe the history of the current implementation of the “reference” hydrofabric. In the 60’s, the us census beuro and the usgs created map scale topo maps for quadrants, and had field personnel verify them as accurate. From those, the blue line river representation was used as a starting geometry. Using those lines as a starting point, the nhd program spun off in the 90’s to modernize and digitize the waterways and was then improved for the nhdplushighres which goes through the usgs water mission areas."
  },
  {
    "objectID": "hydrofab_dm.html#references",
    "href": "hydrofab_dm.html#references",
    "title": "Hydrofabric data model",
    "section": "References",
    "text": "References\nWriting layers out into gpkg from a cloud store: [[20240831204113]] Hydrofabric manipulations [[20241014200531]] Differentiating Stream Order vs mainstem vs levelpath"
  },
  {
    "objectID": "hydrofab_dm.html#explainers",
    "href": "hydrofab_dm.html#explainers",
    "title": "Hydrofabric data model",
    "section": "Explainers",
    "text": "Explainers\n[[20241014200956]] What does incorrectly labeling a catchment mean physically?"
  },
  {
    "objectID": "hydrofab_dm.html#hydrofabric-as-datasets",
    "href": "hydrofab_dm.html#hydrofabric-as-datasets",
    "title": "Hydrofabric data model",
    "section": "Hydrofabric as datasets",
    "text": "Hydrofabric as datasets\nBecause everything is done in table space as opposed to geographic space, the operations tend to be lightning fast and more memory efficient and so you are able to scale your analysis much faster than you otherwise could on the same hardware, same timeframe, and therefore same cost. This enables applications like CONUS scale flood mapping, USGS water prediction, and associated hydrofabric operations to be executed on common hardware\n\nHydrofabric for hydralics\nIt’s occasionally inappropriate to use hydrofabric as a hydraulic modeling framework.  Hydraulics and flying water are incompatible. \n\n\nWhat is a HUC?"
  },
  {
    "objectID": "hydrofab_dm.html#d-hydrofab",
    "href": "hydrofab_dm.html#d-hydrofab",
    "title": "Hydrofabric data model",
    "section": "“3D” Hydrofab",
    "text": "“3D” Hydrofab\nThis is where you lose me but using those more time-efficient [[20240818074751]]Hydraulicc geometries [[20240718223131]] Cross sectional representations of the river-scape [[20241016192617]] Hydrofabric3D Data Model"
  },
  {
    "objectID": "hydrofab.html",
    "href": "hydrofab.html",
    "title": "Hydrofabric data",
    "section": "",
    "text": "As previously mentioned, the term “hydrofabric” can be used to describe …\n1) the flowline –&gt; catchment discretization of the landscape (contra: Semantics are important, I just hate them)\n2) a conceptual standard to describe those Semantics as defined by the OGC (“Hydrologic Modeling and River Corridor Applications of HY_Features Concepts” (n.d.)), and\n3) The data model used to describe those semantics.\nThis page deals with the second. See the Hydrofabric data model page for the foundation and theory that this database is intended to represent.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources",
      "Hydrofabric data"
    ]
  },
  {
    "objectID": "hydrofab.html#hydrofabric-as-datasets",
    "href": "hydrofab.html#hydrofabric-as-datasets",
    "title": "Hydrofabric data",
    "section": "Hydrofabric as datasets",
    "text": "Hydrofabric as datasets\nBecause everything is done in table space as opposed to geographic space, the operations tend to be lightning fast and more memory efficient and so you are able to scale your analysis much faster than you otherwise could on the same hardware, same timeframe, and therefore same cost. This enables applications like CONUS scale flood mapping, USGS water prediction, and associated hydrofabric operations to be executed on common hardware.\nI should get back to work",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources",
      "Hydrofabric data"
    ]
  },
  {
    "objectID": "howtopmp1.html",
    "href": "howtopmp1.html",
    "title": "Calculating Probable Maximum Precipitation",
    "section": "",
    "text": "Much of this page was built in collaboration with Dr. Kenneth Ekpetere, Dr. Jude Kastens, and Dr. Xingong Li. See the publications lead by Dr. Ekpetere here: Ekpetere, Coll, and Mehta (2025)."
  },
  {
    "objectID": "howtopmp1.html#probable-maximum-precipitation",
    "href": "howtopmp1.html#probable-maximum-precipitation",
    "title": "Calculating Probable Maximum Precipitation",
    "section": "Probable Maximum Precipitation",
    "text": "Probable Maximum Precipitation\nOne of the most important limitations we can place on the hydrologic cycle is an upper bounds on the amount of water we can expect to place on a system (within a given time frame). This concept, called probable maximum precipitation, is formally defined as the maximum amount of water (in units of depth) that could be expected for a given location over a given duration. This value is used, in conjunction with design storm specifications and return periods, to appropriately size and design water resource control features ranging in scales from stormwater management to basin wide mega-engineering structures.\nOne of the most popular means of estimating this metric is with the World Meteorological Organization (WMO) endorsed modification on a procedure known as the Hershfield statistical approach. Hershfield’s initial approach involved taking the general frequency equation X_t = X_n + KS_n where X_t is the rainfall for a return period t, \\overline{X_n} and S_m are the mean and standard deviation of a series of annual maximum observations, and K is a frequency dependent scalar based on frequency distribution fittings. If that maximum observed rainfall value (K_m) is substituted for X_t and K_m for k, we arrive at Hershfield’s first proposed form X_t = X_n + K_m S_n.\nGiven the form of 3, it’s clear that the frequency factor (K^{*}_{t}) exercises significant power over the final value calculated. Without correction, the standard statistical implementation can result in illogical inversions in the calculated depth for a location as you increase the length (e.g. we calculate the depth for a 2 hour storm as 5 in., but the depth for a 6 hour storm might only result in 4.85 in.). To address accuracy and consistency problems that commonly occur with the Hershfield method, the WMO PMP method utilizes a two-stage statistical adjustment. The first stage was designed to mitigate variations in extreme value representation among the precipitation annual maximum value time series (R_t). It’s been shown that as mean precipitation decreases, so too does frequency factor (Sarkar and Maity 2020), which also contributes to an inverted calculation. A second correction was developed to account for low bias in maximum value of a time series. This bias is caused by the discrete observational time step, which limits the number of spans that can be examined for a given duration, and would possibly miss the “true” rainfall maximum for that duration. This adjustment factor is calculated based on lookup values found in off charts originally published in (Hershfield 1961), and were reproduced in (World Meteorological Organization 2009).\nThe first step of the WMO method involves adjusting the mean and standard deviation of the annual series to compensate for record length as shown in Figure 4.4 of (World Meteorological Organization 2009). Since no functions describing these series were provided in either the original or updated methodologies, values were manually selected off the charts presented, and an equation was fit which could be plugged into our analysis as shown in Figure 1.\n\n\n\n\n\n\n\n\nHersh44.png\n\n\n\n\nWhat points were used?\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\nx\n\n\nm\n\n\nsd\n\n\n\n\n\n\n10\n\n\n105.0\n\n\n125.0\n\n\n\n\n15\n\n\n103.0\n\n\n113.0\n\n\n\n\n20\n\n\n102.0\n\n\n108.0\n\n\n\n\n25\n\n\n101.0\n\n\n105.0\n\n\n\n\n30\n\n\n100.5\n\n\n103.5\n\n\n\n\n40\n\n\n100.0\n\n\n101.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\nFigure 1: (left): A screen shot of Figure 4.4 from WMO (itself a mishmash of Figures 3 A and B from Hershfield), and associated points selected shown in Table 1 (a) and (right) a derivation of that graph where points called out are the visual markers chosen and fits calculated.\n\n\n\nBased on these derivation, the final forms chosen were:\nADJ_{mean} = 99.5704 + (112.3788 - 99.5704)*e^{(-X/11.6079)}\nADJ_{stdd} = 101.5168 + (188.8722 - 101.5168)*e^{(-X/7.5705)}\n\n\n\n\n\n\nWhy chose that form?\n\n\n\n\n\nThere are a lot of ways in which we could have decided to fit this function. While this is unarguably overkill, it is instructive to take a quick look at a few others just to put foundation and context on the forms we’ll chose moving forward. As we see in tables Table 2 (a) and Table 2 (b), there are several forms which might satisfy an arbitrarily chosen R^2 threshold, but as Figure 3 shows, exponential decay was the form which most adequately conforms to our desired line and was therefore the form chosen as we moved through this analysis.\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\nform\n\n\nresiduals\n\n\n\n\n\n\ny = -0.16*X + 105.65\n\n\n0.8677966\n\n\n\n\ny = 113.1469 + -3.679*log(X)\n\n\n0.9988108\n\n\n\n\ny = 105.7385*e^(-X/0.0016)\n\n\n0.9941566\n\n\n\n\ny = 105.7389*e^(-X/631.6671)\n\n\n0.8727181\n\n\n\n\ny = 99.5704 + (112.3788 - 99.5704)*e^(-X/11.6079)\n\n\n0.9980505\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nform\n\n\nresiduals\n\n\n\n\n\n\ny = -0.7057*X + 125.8\n\n\n0.7750619\n\n\n\n\ny = 160.2942 + -16.6945*log(X)\n\n\n0.9208749\n\n\n\n\ny = 127.6376*e^(-X/0.0067)\n\n\n0.7991150\n\n\n\n\ny = 127.6377*e^(-X/148.5792)\n\n\n0.7991150\n\n\n\n\ny = 101.5168 + (188.8722 - 101.5168)*e^(-X/7.5705)\n\n\n0.9978673\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: left: the forms calculated for the mean series and right: the forms calculated for the standard deviation series\n\n\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\nWe next need the adjustment factor for both the mean (Figure 4 (b)) and the standard deviation (Figure 5 (b)) based on figures 4.2 and 4.3 in (World Meteorological Organization 2009), themselves transpositions (and a conversion of the axis representation) of Figures 1 and 3 in (Hershfield 1961). Given the linear-ish nature of the graphs, relationship forms were easier to derive.\n\n\n\n\n\n\n\n\n_Hersh42.png\n\n\n\n\nWhat points were used?\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\nx\n\n\nx_10\n\n\nx_15\n\n\nx_20\n\n\nx_30\n\n\nx_50\n\n\nx_60\n\n\n\n\n\n\n7\n\n\n0.721\n\n\nNA\n\n\n77.3\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n1\n\n\n0.722\n\n\n78.3\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n23\n\n\n0.724\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n74.0\n\n\nNA\n\n\n\n\n13\n\n\n0.725\n\n\nNA\n\n\nNA\n\n\n76.6\n\n\nNA\n\n\nNA\n\n\n73.8\n\n\n\n\n18\n\n\n0.726\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n75.3\n\n\nNA\n\n\nNA\n\n\n\n\n2\n\n\n0.749\n\n\n81.0\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n8\n\n\n0.761\n\n\nNA\n\n\n81.3\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n14\n\n\n0.762\n\n\nNA\n\n\nNA\n\n\n80.3\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n19\n\n\n0.773\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n80.0\n\n\nNA\n\n\nNA\n\n\n\n\n24\n\n\n0.775\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n79.1\n\n\nNA\n\n\n\n\n28\n\n\n0.785\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n79.8\n\n\n\n\n3\n\n\n0.800\n\n\n86.4\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n20\n\n\n0.804\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n83.2\n\n\nNA\n\n\nNA\n\n\n\n\n15\n\n\n0.814\n\n\nNA\n\n\nNA\n\n\n85.6\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n9\n\n\n0.821\n\n\nNA\n\n\n87.4\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n83.6\n\n\n\n\n25\n\n\n0.833\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n84.9\n\n\nNA\n\n\n\n\n21\n\n\n0.863\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n89.1\n\n\nNA\n\n\nNA\n\n\n\n\n4\n\n\n0.864\n\n\n93.2\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n29\n\n\n0.871\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n88.6\n\n\n\n\n10\n\n\n0.881\n\n\nNA\n\n\n93.6\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n16\n\n\n0.883\n\n\nNA\n\n\nNA\n\n\n92.5\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n26\n\n\n0.904\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n92.2\n\n\nNA\n\n\n\n\n22\n\n\n0.932\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n96.0\n\n\nNA\n\n\nNA\n\n\n\n\n30\n\n\n0.938\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n95.4\n\n\n\n\n5\n\n\n0.942\n\n\n101.4\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n11\n\n\n0.945\n\n\nNA\n\n\n100.3\n\n\n98.9\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n27\n\n\n0.952\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n97.0\n\n\nNA\n\n\n\n\n17\n\n\n0.997\n\n\nNA\n\n\nNA\n\n\n104.0\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n12\n\n\n0.999\n\n\nNA\n\n\n105.8\n\n\nNA\n\n\n102.8\n\n\n101.8\n\n\n101.6\n\n\n\n\n6\n\n\n1.000\n\n\n107.4\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\nFigure 4: (left): A screen shot of Figure 1 from (Hershfield 1961) (See also Figure 4.2 from (World Meteorological Organization 2009)) and (right) a derivation of that graph where points called out are the visual markers chosen and fits calculated.\n\n\n\n\n\n\n\n\n\n\n\n_Hersh43.png\n\n\n\n\nWhat points were used?\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\nx\n\n\nx_10\n\n\nx_15\n\n\nx_30\n\n\nx_50\n\n\nx_60\n\n\n\n\n\n\n1\n\n\n0.260\n\n\n32.3\n\n\n31.5\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n12\n\n\n0.286\n\n\nNA\n\n\nNA\n\n\n32.0\n\n\nNA\n\n\nNA\n\n\n\n\n18\n\n\n0.301\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n32.0\n\n\nNA\n\n\n\n\n23\n\n\n0.313\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n32.0\n\n\n\n\n2\n\n\n0.343\n\n\n42.0\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n7\n\n\n0.362\n\n\nNA\n\n\n42.8\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n24\n\n\n0.402\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n41.9\n\n\n\n\n3\n\n\n0.403\n\n\n49.3\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n19\n\n\n0.410\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n44.0\n\n\nNA\n\n\n\n\n13\n\n\n0.434\n\n\nNA\n\n\nNA\n\n\n48.7\n\n\nNA\n\n\nNA\n\n\n\n\n8\n\n\n0.478\n\n\nNA\n\n\n56.8\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n25\n\n\n0.525\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n55.8\n\n\n\n\n20\n\n\n0.547\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n59.7\n\n\nNA\n\n\n\n\n14\n\n\n0.552\n\n\nNA\n\n\nNA\n\n\n62.5\n\n\nNA\n\n\nNA\n\n\n\n\n4\n\n\n0.609\n\n\n74.5\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n9\n\n\n0.660\n\n\nNA\n\n\n78.6\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n15\n\n\n0.695\n\n\nNA\n\n\nNA\n\n\n78.8\n\n\nNA\n\n\nNA\n\n\n\n\n26\n\n\n0.705\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n76.2\n\n\n\n\n21\n\n\n0.715\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n78.7\n\n\nNA\n\n\n\n\n5\n\n\n0.803\n\n\n98.0\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n10\n\n\n0.815\n\n\nNA\n\n\n97.2\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n22\n\n\n0.831\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n92.1\n\n\nNA\n\n\n\n\n16\n\n\n0.851\n\n\nNA\n\n\nNA\n\n\n97.2\n\n\nNA\n\n\n92.4\n\n\n\n\n11\n\n\n0.990\n\n\nNA\n\n\n117.5\n\n\nNA\n\n\n109.5\n\n\n107.5\n\n\n\n\n6\n\n\n0.993\n\n\n121.1\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n17\n\n\n0.996\n\n\nNA\n\n\nNA\n\n\n113.6\n\n\nNA\n\n\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\nFigure 5: (left): A screen shot of Figure 4 from Hershfield (See also Figure 4.3 from (World Meteorological Organization 2009)) and (right) a derivation of that graph where points called out are the visual markers chosen and fits calculated.\n\n\n\nwhich results in the following sets of equations for the mean (left) and standard deviation (right).\n\n\n\n\n\n\nTable 1\n\n\n\n\n\n\n\nseries\n\n\nform\n\n\nresiduals\n\n\n\n\n\n\n10 years\n\n\ny = 105.054*X + 2.3901\n\n\n0.9999748\n\n\n\n\n15 years\n\n\ny = 102.7805*X + 3.1069\n\n\n0.9999603\n\n\n\n\n20 years\n\n\ny = 100.9696*X + 3.3883\n\n\n0.9999731\n\n\n\n\n30 years\n\n\ny = 100.6655*X + 2.218\n\n\n0.9999905\n\n\n\n\n50 years\n\n\ny = 101.1731*X + 0.7025\n\n\n0.9999805\n\n\n\n\n60 years\n\n\ny = 101.52*X + 0.1815\n\n\n0.9999792\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n\n\n\n\n\nseries\n\n\nform\n\n\nresiduals\n\n\n\n\n\n\n10 years\n\n\ny = 121.4169*X + 0.5078\n\n\n0.9999841\n\n\n\n\n15 years\n\n\ny = 118.5259*X + 0.3092\n\n\n0.9999173\n\n\n\n\n30 years\n\n\ny = 115.2533*X + -1.1293\n\n\n0.9999656\n\n\n\n\n50 years\n\n\ny = 112.9592*X + -2.0945\n\n\n0.9999497\n\n\n\n\n60 years\n\n\ny = 111.8472*X + -2.9423\n\n\n0.9999514\n\n\n\n\n\n\n\n\n\n\n\nSince the length of the record we wish to interrogate is the variable we are looking to plug in and because these corrections are small and constrained relative to the range of our inputs, and due to the already overly analytical approach we are taking to empirical graph fitting, we’ll fix the leading coefficient of the mean to the average of our derived lines and rearrange the result to be a function of the record length, resulting in the following equation: 102.0271*X + (-0.0588*record_length (years) + 3.8102). The standard deviation is a bit harder to nail down, so we’ll find the linear rate of change in both the slope and the intercept and call it close enough for science. This results in the following, record-length dependent equation: -0.1764*record_length (years) + 121.8217*X+-0.0686*record_length (years) + 1.1929\nFinally, the last adjustment factor to derive off the graphs presented is shown in Figure 4.1 in (World Meteorological Organization 2009) and recreated below in Figure 6 (b). Given our need to densify this graph (there are only 4 “observations” of a given duration - note that the 6 hour curve here is itself an interpolation - and we are inter- and extrapolating over a series of 10 periods), we’ll find a curve which satisfies a general form of those.\n\n\n\n\n\n\n\n\nWhat points were used?\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\nx\n\n\nx5_min\n\n\nx1_hour\n\n\nx6_hour\n\n\nx24_hour\n\n\n\n\n\n\n0\n\n\n20.00\n\n\n20.0\n\n\n20.00\n\n\n20.00\n\n\n\n\n5\n\n\n16.75\n\n\n18.5\n\n\n19.00\n\n\n19.60\n\n\n\n\n10\n\n\n14.05\n\n\n17.2\n\n\n18.05\n\n\n19.25\n\n\n\n\n20\n\n\nNA\n\n\n15.0\n\n\n16.75\n\n\n18.80\n\n\n\n\n30\n\n\nNA\n\n\n13.1\n\n\n15.80\n\n\n18.20\n\n\n\n\n50\n\n\nNA\n\n\n10.0\n\n\n14.40\n\n\n17.20\n\n\n\n\n100\n\n\nNA\n\n\n5.0\n\n\n11.50\n\n\n15.20\n\n\n\n\n150\n\n\nNA\n\n\nNA\n\n\n9.05\n\n\n13.50\n\n\n\n\n200\n\n\nNA\n\n\nNA\n\n\n7.15\n\n\n12.10\n\n\n\n\n250\n\n\nNA\n\n\nNA\n\n\n5.20\n\n\n10.90\n\n\n\n\n300\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n9.90\n\n\n\n\n350\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n8.90\n\n\n\n\n400\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n7.95\n\n\n\n\n450\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n7.10\n\n\n\n\n500\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n6.50\n\n\n\n\n550\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n5.90\n\n\n\n\n600\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n5.45\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\nFigure 6: (left): A screen shot of Figure 4 from Hershfield (See also Figure 4.3 from (World Meteorological Organization 2009)) and (right) a derivation of that graph where points called out are the visual markers chosen and fits calculated.\n\n\n\nwhich results in the final fit forms shown in Table 3:\n\n\n\n\nTable 3\n\n\n\n\n\n\n\nSeries.\n\n\nForm.\n\n\nR2.\n\n\n\n\n\n\n5 minutes:\n\n\nY = 19.9689*e^(-x/0.0352)\n\n\n1.0000\n\n\n\n\n1 hour:\n\n\nY = 19.7695*e^(-x/0.0137)\n\n\n1.0000\n\n\n\n\n6 hour:\n\n\nY = 18.8677*e^(-x/0.005)\n\n\n0.9957\n\n\n\n\n24 hour:\n\n\nY = 19.4549*e^(-x/0.0022)\n\n\n0.9977\n\n\n\n\n\n\n\n\n\nTo make the problem more tractable, we fix the leading coefficient at 19.5 (an empirically even, number close to the average of the calculated coefficients) given the relative (un)importance it seems to demonstrate here, the range over which we are estimating (locations with &lt; 10 mm of mean annual maximum rainfall), and the empirical nature of graphical curve fitting. Fitting the remaining points in defined in those equations b as specified above as a function of PMP time step (shown in Figure 7 (a)) gives us an equation that looks like so, and the evaluation of that for our particular duration as shown in Table 7 (b):\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nSeries.\n\n\nForm.\n\n\n\n\n\n\n5 minute:\n\n\n19.5 * exp(X * -0.0353)\n\n\n\n\n30 minute:\n\n\n19.5 * exp(X * -0.0191)\n\n\n\n\n1 hour:\n\n\n19.5 * exp(X * -0.0138)\n\n\n\n\n2 hour:\n\n\n19.5 * exp(X * -0.0096)\n\n\n\n\n3 hour:\n\n\n19.5 * exp(X * -0.0077)\n\n\n\n\n6 hour:\n\n\n19.5 * exp(X * -0.0052)\n\n\n\n\n12 hour:\n\n\n19.5 * exp(X * -0.0035)\n\n\n\n\n24 hour:\n\n\n19.5 * exp(X * -0.0023)\n\n\n\n\n48 hour:\n\n\n19.5 * exp(X * -0.0015)\n\n\n\n\n72 hour:\n\n\n19.5 * exp(X * -0.0012)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: (left): A visual derivation of the form of of b we’ll use to calculate Figure 4.1 curves. (right) the form of the function defining Figure 4.1 for the PMP calculation for our given durations.\n\n\n\nGiven this form, we can substitute in, rederive the series, and densify them for our intermediate durations to construct our final form of Figure 4.1, shown in Figure 8 using the following equation, as expanded in Table 7 (b) above):\n\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\nHaving found all the lookup values and equations now, we are finally ready to calculate the PMP value as such:\n\nFor a given hour based time duration \\tau \\in \\intercal, e.g. \\lbrace 0.5, 1, 2, 3, 6, 12, 24 \\rbrace, define R_t = \\lbrace R_{t,1},...,R_{t,Y} \\rbrace as the maximum precipitation value observed over the period of duration (itself defined by the rolling sum of all individual observations which fall within the period before the time of analysis) for each year (_Y)\nCalculate the frequency factor K^{*}_{t}=\\frac{R_{t^{\\prime} \\space max}-\\overline{R_{t^{\\prime}-1}}}{S_{t^{\\prime}-1}} where: R_{t \\space max} is the largest value in the series, \\overline{R_{t^{\\prime}-1}} is the mean of the series excluding the single largest value, and S_{t^{\\prime}-1} is the standard deviation of that trimmed series.\nFinally, calculate the Hershfield PMP as P_{t}^{*}=\\overline{R_t} + K^{*}_{t} * S_{t} where \\overline{R_t} and S_{t} are the mean and standard deviation of the series respectively.\n\n\n\n\n\n\nflowchart TD\n    N(Standard Deviation) --&gt; L(Adjustment Factor for standard deviation)\n    M(Standard Deviation Ratio) --&gt; L(Adjustment Factor for standard deviation)\n    L(Adjustment Factor for standard deviation) --&gt; J(Adjusted Standard deviation)\n    K(Adjustment Factor for standard deviation record length) --&gt; J(Adjusted Standard deviation)\n    J(Adjusted Standard deviation) --&gt; A(WMO PMP)\n    I(Sample mean) --&gt; G(mean ratio)\n    H(Trimmed Sample mean) --&gt; G(mean ratio)\n    G(mean ratio) --&gt; F(Adjustment Factor for mean ratio)\n    F(Adjustment Factor for mean ratio) --&gt; D(Adjusted Mean)\n    E(Adjustment Factor for mean record length) --&gt; D(Adjusted Mean)\n    D(Adjusted Mean) --&gt; C(Frequency Factor)\n    D(Adjusted Mean) --&gt; A(WMO PMP)\n    C(Frequency Factor) --&gt; A(WMO PMP)\n    B(Sampling Adjustment Factor) --&gt; A(WMO PMP)"
  },
  {
    "objectID": "howtopmp1.html#putting-it-together",
    "href": "howtopmp1.html#putting-it-together",
    "title": "Calculating Probable Maximum Precipitation",
    "section": "Putting it together",
    "text": "Putting it together\n\nPull values by year &lt;- https://code.earthengine.google.com/3c11ddf8020a1217658846e71fcd15ae\n\nAlt: https://code.earthengine.google.com/ab4abfbc3ac91782de3563692a05f348\n\nRolling window sum &gt; max, for m lenghts (30 min, 1,2,3,6,12,24,48 hour) are common\nRepeat for n years to get n values per m series\nRun PMP\nSuccess"
  },
  {
    "objectID": "hec_data.html",
    "href": "hec_data.html",
    "title": "HEC-RAS model data",
    "section": "",
    "text": "Probably delete?"
  },
  {
    "objectID": "h_h.html",
    "href": "h_h.html",
    "title": "H&H modeling",
    "section": "",
    "text": "One of the only ways to approach this shortfall is to create an integrated hydrologic modeling stack",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "H&H modeling"
    ]
  },
  {
    "objectID": "h_h.html#the-national-water-model-as-an-integrated-hydrologic-stack",
    "href": "h_h.html#the-national-water-model-as-an-integrated-hydrologic-stack",
    "title": "H&H modeling",
    "section": "The National Water Model as an “Integrated Hydrologic Stack”",
    "text": "The National Water Model as an “Integrated Hydrologic Stack”\nRelated: [[NWM.qmd]] National Water Model\nAs alluded to above, H&H is not a “new” approach to water resource forecasting. Indeed, the HEC series of models (HEC-HMS & HEC-RAS) in many regards still represent the gold standard of forecasting, particularly for high flow and design storm approaches. However, one of the key shortcommings in this methodology is that the various components are serial and independent. That limitation, along with the limited scales of analysis most H&H models encompass, was a critical shortfall addressed with the National Water Model.\n\nThe National Water Model (NWM) is a hydrologic modeling framework that simulates observed and forecast streamflow over the entire continental United States (CONUS), southern Alaska (Cook Inlet, Copper River Basin, and Prince William Sound regions), Hawaii, Puerto Rico and the US Virgin Islands. Additionally, it produces total water level guidance for the coastlines of those same regions except Alaska. … The NWM produces hydrologic guidance at a very fine spatial and temporal scale. It complements official NWS river forecasts at approximately 4000 locations across the CONUS and produces guidance at millions of other locations that do not have a traditional river forecast. (“Office of Water Prediction” n.d.)",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "H&H modeling"
    ]
  },
  {
    "objectID": "geom.html",
    "href": "geom.html",
    "title": "Geometries",
    "section": "",
    "text": "Having learned how we represent coordinates systems, we can define how geometries can be described using these coordinate systems. This chapter will explain:\n\nsimple features, a standard that describes point, line, and polygon geometries along with operations on them,\noperations on geometries,\ncoverages, functions of space or space-time,\ntesselations, sub-divisions of larger regions into sub-regions, and\nnetworks.\n\nGeometries on the sphere are discussed in ?@sec-spherical, rasters and other rectangular sub-divisions of space or space time are discussed in ?@sec-datacube.\n\n\nSimple feature geometries are a way to describe the geometries of features. By features we mean things that have a geometry, potentially implicitly some time properties, and further attributes that could include labels describing the thing and/or values quantitatively measuring it. The main application of simple feature geometries is to describe geometries in two-dimensional space by points, lines, or polygons. The “simple” adjective refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect.\nSimple features access is a standard (sfa?; sfa2?; iso?) for describing simple feature geometries. It includes:\n\n\na class hierarchy\na set of operations\nbinary and text encodings\n\nWe will first discuss the seven most common simple feature geometry types.\n\n\n \nThe most common simple feature geometries used to represent a single feature are:\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above\n\n\n\n\n\nCode\nlibrary(sf) |&gt; suppressPackageStartupMessages()\npar(mfrow = c(2,4))\npar(mar = c(1,1,1.2,1))\n\n# 1\np &lt;- st_point(0:1)\nplot(p, pch = 16)\ntitle(\"point\")\nbox(col = 'grey')\n\n# 2\nmp &lt;- st_multipoint(rbind(c(1,1), c(2, 2), c(4, 1), c(2, 3), c(1,4)))\nplot(mp, pch = 16)\ntitle(\"multipoint\")\nbox(col = 'grey')\n\n# 3\nls &lt;- st_linestring(rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)))\nplot(ls, lwd = 2)\ntitle(\"linestring\")\nbox(col = 'grey')\n\n# 4\nmls &lt;- st_multilinestring(list(\n  rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)),\n  rbind(c(3,0), c(4,1), c(2,1))))\nplot(mls, lwd = 2)\ntitle(\"multilinestring\")\nbox(col = 'grey')\n\n# 5 polygon\npo &lt;- st_polygon(list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),\n    rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))))\nplot(po, border = 'black', col = '#ff8888', lwd = 2)\ntitle(\"polygon\")\nbox(col = 'grey')\n\n# 6 multipolygon\nmpo &lt;- st_multipolygon(list(\n    list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),\n        rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))),\n    list(rbind(c(3,7), c(4,7), c(5,8), c(3,9), c(2,8), c(3,7)))))\nplot(mpo, border = 'black', col = '#ff8888', lwd = 2)\ntitle(\"multipolygon\")\nbox(col = 'grey')\n\n# 7 geometrycollection\ngc &lt;- st_geometrycollection(list(po, ls + c(0,5), st_point(c(2,5)), st_point(c(5,4))))\nplot(gc, border = 'black', col = '#ff6666', pch = 16, lwd = 2)\ntitle(\"geometrycollection\")\nbox(col = 'grey')\n\n\n\n\n\n\n\n\nFigure 1: Sketches of the main simple feature geometry types\n\n\n\n\n\nFigure 1 shows examples of these basic geometry types. The human-readable, “well-known text” (WKT) representation of the geometries plotted are:\n \n\n\nCode\np\nmp\nls\nmls\npo\nmpo\ngc\n\n\nPOINT (0 1)\nMULTIPOINT ((1 1), (2 2), (4 1), (2 3), (1 4))\nLINESTRING (1 1, 5 5, 5 6, 4 6, 3 4, 2 3)\nMULTILINESTRING ((1 1, 5 5, 5 6, 4 6, 3 4, 2 3), (3 0, 4 1, 2 1))\nPOLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),\n    (2 2, 3 3, 4 3, 4 2, 2 2))\nMULTIPOLYGON (((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),\n    (2 2, 3 3, 4 3, 4 2, 2 2)), ((3 7, 4 7, 5 8, 3 9, 2 8, 3 7)))\nGEOMETRYCOLLECTION (\n    POLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),\n      (2 2 , 3 3, 4 3, 4 2, 2 2)),\n    LINESTRING (1 6, 5 10, 5 11, 4 11, 3 9, 2 8),\n    POINT (2 5),\n    POINT (5 4)\n)\nIn this representation, coordinates are separated by space, and points by commas. Sets are grouped by parentheses, and separated by commas. Polygons consist of an outer ring followed by zero or more inner rings denoting holes. Individual points in a geometry contain at least two coordinates: \\(x\\) and \\(y\\), in that order. If these coordinates refer to ellipsoidal coordinates, \\(x\\) and \\(y\\) usually refer to longitude and latitude, respectively, although sometimes to latitude and longitude (see ?@sec-projlib and ?@sec-axisorder).\n\n\n\n \nLinestrings are called simple when they do not self-intersect:\n\n\nCode\n(ls &lt;- st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0))))\n\n\nLINESTRING (0 0, 1 1, 2 2, 0 2, 1 1, 2 0)\n\n\nCode\nc(is_simple = st_is_simple(ls))\n\n\nis_simple \n    FALSE \n\n\nValid polygons and multi-polygons obey all of the following properties:\n\npolygon rings are closed (the last point equals the first)\npolygon holes (inner rings) are inside their exterior ring\npolygon inner rings maximally touch the exterior ring in single points, not over a line\na polygon ring does not repeat its own path\nin a multi-polygon, an external ring maximally touches another exterior ring in single points, not over a line\n\nIf this is not the case, the geometry concerned is not valid. Invalid geometries typically cause errors when they are processed, but they can usually be repaired to make them valid.\nA further convention is that the outer ring of a polygon is winded counter-clockwise, while the holes are winded clockwise, but polygons for which this is not the case are still considered valid. For polygons on the sphere, the “clockwise” concept is not very useful: if for instance we take the equator as polygon, is the Northern Hemisphere or the Southern Hemisphere “inside”? The convention taken here is to consider the area on the left while traversing the polygon is considered the polygon’s inside (see also ?@sec-ccw).\n\n\n\n \nIn addition to X and Y coordinates, single points (vertices) of simple feature geometries may have:\n\na Z coordinate, denoting altitude, and/or * an M value, denoting some “measure”\n\nThe M attribute shall be a property of the vertex. It sounds attractive to encode a time stamp in it for instance to pack movement data (trajectories) in LINESTRINGs. These become however invalid (or “non-simple”) once the trajectory self-intersects, which happens when only X and Y are considered for self-intersections.\nBoth Z and M are not found often, and software support to do something useful with them is (still) rare. Their WKT representations are fairly easily understood:\n\n\nCode\nst_point(c(1,3,2))\n\n\nPOINT Z (1 3 2)\n\n\nCode\nst_point(c(1,3,2), dim = \"XYM\")\n\n\nPOINT M (1 3 2)\n\n\nCode\nst_linestring(rbind(c(3,1,2,4), c(4,4,2,2)))\n\n\nLINESTRING ZM (3 1 2 4, 4 4 2 2)\n\n\n\n\n\n \nA very important concept in the feature geometry framework is that of the empty geometry.\nEmpty geometries arise naturally when we do geometrical operations (Section 1.2), for instance when we want to know the intersection of POINT (0 0) and POINT (1 1):\n\n\nCode\n(e &lt;- st_intersection(st_point(c(0,0)), st_point(c(1,1))))\n\n\nGEOMETRYCOLLECTION EMPTY\n\n\nand it represents essentially the empty set: when combining (unioning) an empty point with other non-empty geometries, it vanishes.\nAll geometry types have a special value representing the empty (typed) geometry, like\n\n\nCode\nst_point()\n\n\nPOINT EMPTY\n\n\nCode\nst_linestring(matrix(1,1,3)[0,], dim = \"XYM\")\n\n\nLINESTRING M EMPTY\n\n\nand so on, but they all point to the empty set, differing only in their dimension (Section 1.2.2).\n\n\n\nThere are 10 more geometry types which are more rare, but increasingly find implementation:\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nCIRCULARSTRING\nThe CircularString is the basic curve type, similar to a LineString in the linear world. A single segment requires three points, the start and end points (first and third) and any other point on the arc. The exception to this is for a closed circle, where the start and end points are the same. In this case the second point MUST be the centre of the arc, i.e., the opposite side of the circle. To chain arcs together, the last point of the previous arc becomes the first point of the next arc, just like in LineString. This means that a valid circular string must have an odd number of points greater than 1.\n\n\nCOMPOUNDCURVE\nA CompoundCurve is a single, continuous curve that has both curved (circular) segments and linear segments. That means that in addition to having well-formed components, the end point of every component (except the last) must be coincident with the start point of the following component.\n\n\nCURVEPOLYGON\nExample compound curve in a curve polygon: CURVEPOLYGON( COMPOUNDCURVE( CIRCULARSTRING(0 0,2 0, 2 1, 2 3, 4 3),(4 3, 4 5, 1 4, 0 0)), CIRCULARSTRING(1.7 1, 1.4 0.4, 1.6 0.4, 1.6 0.5, 1.7 1))\n\n\nMULTICURVE\nA MultiCurve is a 1 dimensional GeometryCollection whose elements are Curves. It can include linear strings, circular strings, or compound strings.\n\n\nMULTISURFACE\nA MultiSurface is a 2 dimensional GeometryCollection whose elements are Surfaces, all using coordinates from the same coordinate reference system.\n\n\nCURVE\nA Curve is a 1 dimensional geometric object usually stored as a sequence of Points, with the subtype of Curve specifying the form of the interpolation between Points\n\n\nSURFACE\nA Surface is a 2 dimensional geometric object\n\n\nPOLYHEDRALSURFACE\nA PolyhedralSurface is a contiguous collection of polygons, which share common boundary segments\n\n\nTIN\nA TIN (triangulated irregular network) is a PolyhedralSurface consisting only of Triangle patches.\n\n\nTRIANGLE\nA Triangle is a polygon with three distinct, non-collinear vertices and no interior boundary\n\n\n\nCIRCULARSTRING, COMPOUNDCURVE and CURVEPOLYGON are not described in the SFA standard, but in the SQL-MM part 3 standard. The descriptions above were copied from the PostGIS manual.\n \n\n\n\n \nPart of the simple feature standard are two encodings: a text and a binary encoding. The well-known text encoding, used above, is human-readable. The well-known binary encoding is machine-readable. Well-known binary (WKB) encodings are lossless and typically faster to work with than text encoding (and decoding), and they are used for instance in all communications between R package sf and the GDAL, GEOS, liblwgeom, and s2geometry libraries (?@fig-gdal-fig-nodetails).\n\n\n\n\n\nSimple feature geometries can be queried for properties, or transformed or combined into new geometries, and combinations of geometries can be queried for further properties. This section gives an overview of the operations entirely focusing on geometrical properties. ?@sec-featureattributes focuses on the analysis of non-geometrical feature properties, in relationship to their geometries. Some of the material in this section appeared in (rjsf?).\nWe can categorise operations on geometries in terms of what they take as input, and what they return as output. In terms of output we have operations that return:\n\npredicates: a logical asserting a certain property is TRUE\nmeasures: a quantity (a numeric value, possibly with measurement unit)\ntransformations: newly generated geometries\n\nand in terms of what they operate on, we distinguish operations that are:\n\nunary when they work on a single geometry\nbinary when they work on pairs of geometries\nn-ary when they work on sets of geometries\n\n\n\n \nUnary predicates describe a certain property of a geometry. The predicates is_simple, is_valid, and is_empty return respectively whether a geometry is simple, valid, or empty. Given a coordinate reference system, is_longlat returns whether the coordinates are geographic or projected. is(geometry, class) checks whether a geometry belongs to a particular class.\n \n\n\n\n \nThe Dimensionally Extended Nine-Intersection Model (DE-9IM, de9im1?; de9im2?) is a model that describes the qualitative relation between any two geometries in two-dimensional space (\\(R^2\\)). Any geometry has a dimension value that is:\n\n0 for points,\n1 for linear geometries,\n2 for polygonal geometries, and\nF (false) for empty geometries\n\nAny geometry also has an inside (I), a boundary (B), and an exterior (E); these roles are obvious for polygons, however, for:\n\nlines the boundary is formed by the end points, and the interior by all non-end points on the line\npoints have a zero-dimensional inside but no boundary\n\n\n\nCode\nlibrary(sf)\npolygon &lt;- po &lt;- st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0))))\np0 &lt;- st_polygon(list(rbind(c(-1,-1), c(2,-1), c(2,2), c(-1,2), c(-1,-1))))\nline &lt;- li &lt;- st_linestring(rbind(c(.5, -.5), c(.5, 0.5)))\ns &lt;- st_sfc(po, li)\n\npar(mfrow = c(3,3))\npar(mar = c(1,1,1,1))\n\n# \"1020F1102\"\n# 1: 1\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"I(pol)\",intersect(),\"I(line) = 1\")))\nlines(rbind(c(.5,0), c(.5,.495)), col = 'red', lwd = 2)\npoints(0.5, 0.5, pch = 1)\n\n# 2: 0\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"I(pol)\",intersect(),\"B(line) = 0\")))\npoints(0.5, 0.5, col = 'red', pch = 16)\n\n# 3: 2\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"I(pol)\",intersect(),\"E(line) = 2\")))\nplot(po, col = '#ff8888', add = TRUE)\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', add = TRUE)\n\n# 4: 0\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"B(pol)\",intersect(),\"I(line) = 0\")))\npoints(.5, 0, col = 'red', pch = 16)\n\n# 5: F\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"B(pol)\",intersect(),\"B(line) = F\")))\n\n# 6: 1\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"B(pol)\",intersect(),\"E(line) = 1\")))\nplot(po, border = 'red', col = NA, add = TRUE, lwd = 2)\n\n# 7: 1\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"E(pol)\",intersect(),\"I(line) = 1\")))\nlines(rbind(c(.5, -.5), c(.5, 0)), col = 'red', lwd = 2)\n\n# 8: 0\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"E(pol)\",intersect(),\"B(line) = 0\")))\npoints(.5, -.5, col = 'red', pch = 16)\n\n# 9: 2\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"E(pol)\",intersect(),\"E(line) = 2\")))\nplot(p0 / po, col = '#ff8888', add = TRUE)\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', add = TRUE)\n\n\n\n\n\n\n\n\nFigure 2: DE-9IM: intersections between the interior, boundary, and exterior of a polygon (rows) and of a linestring (columns) indicated by red\n\n\n\n\n\nFigure 2 shows the intersections between the I, B, and E components of a polygon and a linestring indicated by red; the sub-plot title gives the dimension of these intersections (0, 1, 2 or F). The relationship between the polygon and the line geometry is the concatenation of these dimensions:\n\n\nCode\nst_relate(polygon, line)\n\n\n     [,1]       \n[1,] \"1020F1102\"\n\n\nwhere the first three characters are associated with the inside of the first geometry (polygon): Figure 2 is summarised row-wise. Using this ability to express relationships, we can also query pairs of geometries about particular conditions expressed in a mask string. As an example, the string \"*0*******\" would evaluate TRUE when the second geometry has one or more boundary points in common with the interior of the first geometry; the symbol * standing for “any dimensionality” (0, 1, 2 or F). The mask string \"T********\" matches pairs of geometry with intersecting interiors, where the symbol T stands for any non-empty intersection of dimensionality 0, 1, or 2.\nBinary predicates are further described using normal-language verbs, using DE-9IM definitions. For instance, the predicate equals corresponds to the relationship \"T*F**FFF*\". If any two geometries obey this relationship, they are (topologically) equal, but may have a different ordering of nodes.\nA list of binary predicates is:\n\n\n\n\n\n\n\n\npredicate\nmeaning\ninverse of\n\n\n\n\ncontains\nNone of the points of A are outside B\nwithin\n\n\ncontains_properly\nA contains B and B has no points in common with the boundary of A\n\n\n\ncovers\nNo points of B lie in the exterior of A\ncovered_by\n\n\ncovered_by\nInverse of covers\n\n\n\ncrosses\nA and B have some but not all interior points in common\n\n\n\ndisjoint\nA and B have no points in common\nintersects\n\n\nequals\nA and B are topologically equal: node order or number of nodes may differ; identical to A contains B and A within B\n\n\n\nequals_exact\nA and B are geometrically equal, and have identical node order\n\n\n\nintersects\nA and B are not disjoint\ndisjoint\n\n\nis_within_distance\nA is closer to B than a given distance\n\n\n\nwithin\nNone of the points of B are outside A\ncontains\n\n\ntouches\nA and B have at least one boundary point in common, but no interior points\n\n\n\noverlaps\nA and B have some points in common; the dimension of these is identical to that of A and B\n\n\n\nrelate\nGiven a mask pattern, return whether A and B adhere to this pattern\n\n\n\n\n \nThe Wikipedia DE-9IM page provides the relate patterns for each of these verbs. They are important to check out; for instance covers and contains (and their inverses) are often not completely intuitive:\n\nif A contains B, B has no points in common with the exterior or boundary of A\nif A covers B, B has no points in common with the exterior of A\n\n\n\n\n\nUnary measures return a measure or quantity that describes a property of the geometry:\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\n \n\n\n\n\ndistance returns the distance between pairs of geometries. The qualitative measure relate (without mask) gives the relation pattern. A description of the geometrical relationship between two geometries is given in Section 1.2.2.\n \n\n\n\n\nUnary transformations work on a per-geometry basis, and return for each geometry a new geometry.\n\n\n\n\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nwrap_dateline\ncut into pieces that no longer cover or cross the dateline\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry (Figure 3)\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith a (arbitrary) point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system (?@sec-cs)\n\n\ntriangulate\nwith Delauney triangulated polygon(s) (Figure 3)\n\n\nvoronoi\nwith the Voronoi tessellation of an input geometry (Figure 3)\n\n\nzm\nwith removed or added Z and/or M coordinates\n\n\ncollection_extract\nwith sub-geometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n+\nthat is shifted over a given vector\n\n\n*\nthat is multiplied by a scalar or matrix\n\n\n\n\n\nCode\npar(mar = rep(0,4), mfrow = c(1, 3))\nset.seed(133331)\nmp &lt;- st_multipoint(matrix(runif(20), 10))\nplot(mp, cex = 2)\nplot(st_convex_hull(mp), add = TRUE, col = NA, border = 'red')\nbox()\nplot(mp, cex = 2)\nplot(st_voronoi(mp), add = TRUE, col = NA, border = 'red')\nbox()\nplot(mp, cex = 2)\nplot(st_triangulate(mp), add = TRUE, col = NA, border = 'darkgreen')\nbox()\n\n\n\n\n\n\n\n\nFigure 3: For a set of points, left: convex hull (red); middle: Voronoi polygons; right: Delauney triangulation\n\n\n\n\n\n \n\n\n\n\nBinary transformers are functions that return a geometry based on operating on a pair of geometries. They include:\n\n\n\n\n\n\n\n\nfunction\nreturns\ninfix operator\n\n\n\n\nintersection\nthe overlapping geometries for pair of geometries\n&\n\n\nunion\nthe combination of the geometries; removes internal boundaries and duplicate points, nodes or line pieces\n|\n\n\ndifference\nthe geometries of the first after removing the overlap with the second geometry\n/\n\n\nsym_difference\nthe combinations of the geometries after removing where they intersect; the negation (opposite) of intersection\n%/%\n\n\n\n \n\n\n\n\nN-ary transformers operate on sets of geometries. union can be applied to a set of geometries to return its geometrical union. Otherwise, any set of geometries can be combined into a MULTI-type geometry when they have equal dimension, or else into a GEOMETRYCOLLECTION. Without unioning, this may lead to a geometry that is not valid, for instance when two polygon rings have a boundary line in common.\n \nN-ary intersection and difference take a single argument but operate (sequentially) on all pairs, triples, quadruples, etc. Consider the plot in Figure 4: how do we identify the area where all three boxes overlap? Using binary intersections gives us intersections for all pairs: 1-1, 1-2, 1-3, 2-1, 2-2, 2-3, 3-1, 3-2, 3-3, but that does not let us identify areas where more than two geometries intersect. Figure 4 (right) shows the n-ary intersection: the seven unique, non-overlapping geometries originating from intersection of one, two, or more geometries.\n\n\nCode\npar(mar = rep(.1, 4), mfrow = c(1, 2))\nsq &lt;- function(pt, sz = 1) st_polygon(list(rbind(c(pt - sz), \n  c(pt[1] + sz, pt[2] - sz), c(pt + sz), c(pt[1] - sz, pt[2] + sz), c(pt - sz))))\nx &lt;- st_sf(box = 1:3, st_sfc(sq(c(0, 0)), sq(c(1.7, -0.5)), sq(c(0.5, 1))))\nplot(st_geometry(x), col = NA, border = sf.colors(3, categorical = TRUE), lwd = 3)\nplot(st_intersection(st_geometry(x)), col = sf.colors(7, categorical=TRUE, alpha = .5))\n\n\n\n\n\n\n\n\nFigure 4: Left: three overlapping squares – how do we identify the small box where all three overlap? Right: unique, non-overlapping n-ary intersections\n\n\n\n\n\nSimilarly, one can compute an n-ary difference from a set \\(\\{s_1, s_2,\ns_3, ...\\}\\) by creating differences \\(\\{s_1, s_2-s_1, s_3-s_2-s_1,\n...\\}\\). This is shown in Figure 5, (left) for the original set, and (right) for the set after reversing its order to make clear that the result here depends on the ordering of the input geometries. Again, resulting geometries do not overlap.\n\n\nCode\npar(mar = rep(.1, 4), mfrow = c(1, 2)) \nxg &lt;- st_geometry(x)\nplot(st_difference(xg), col = sf.colors(3, alpha = .5, categorical=TRUE))\nplot(st_difference(xg[3:1]), col = sf.colors(3, alpha = .5, categorical=TRUE))\n\n\n\n\n\n\n\n\nFigure 5: Difference between subsequent boxes, left: in original order; right: in reverse order\n\n\n\n\n\n\n\n\n\n \nGeometrical operations, such as finding out whether a certain point is on a line, may fail when coordinates are represented by double precision floating point numbers, such as 8-byte doubles used in R. An often chosen remedy is to limit the precision of the coordinates before the operation. For this, a precision model is adopted; the most common is to choose a factor \\(p\\) and compute rounded coordinates \\(c'\\) from original coordinates \\(c\\) by \\[c' = \\mbox{round}(p \\cdot c) / p\\]\nRounding of this kind brings the coordinates to points on a regular grid with spacing \\(1/p\\), which is beneficial for geometric computations. Of course, it also affects all computations like areas and distances, and may turn valid geometries into invalid ones. Which precision values are best for which application is often a matter of common sense combined with trial and error.\n\n\n\n\nThe Open Geospatial Consortium defines a coverage as a “feature that acts as a function to return values from its range for any direct position within its spatiotemporal domain” (ogccov?). Having a function implies that for every space time “point”, every combination of a spatial point and a moment in time of the spatiotemporal domain, we have a single value for the range. This is a very common situation for spatiotemporal phenomena, a few examples can be given:\n\nboundary disputes aside, at a give time every point in a region (domain) belongs to a single administrative unit (range)\nat any given moment in time, every point in a region (domain) has a certain land cover type (range)\nevery point in an area (domain) has a single surface elevation (range), which could be measured with respect to a given mean sea level surface\nevery spatiotemporal point in a three-dimensional body of air (domain) has single value for temperature (range)\n\nA caveat here is that because observation or measurement always takes time and requires space, measured values are always an average over a spatiotemporal volume, and hence range variables can rarely be measured for true, zero-volume “points”; for many practical cases however the measured volume is small enough to be considered a “point”. For a variable like land cover type the volume needs to be chosen such that the types distinguished make sense with respect to the measured areal units.\nIn the first two of the given examples the range variable is categorical, in the last two the range variable is continuous. For categorical range variables, if large connected areas have a constant range value, an efficient way to represent these data is by storing the boundaries of the areas with constant value, such as country boundaries. Although this can be done (and is often done) by a set of simple feature geometries (polygons or multi-polygons), this brings along some challenges:\n\nit is hard to guarantee for such a set of simple feature polygons that they do not overlap, or that there are no unwanted gaps between them\nsimple features have no way of assigning points on the boundary of two adjacent polygons uniquely to a single polygon, which conflicts with the interpretation as coverage\n\n\n\n\nA data model that guarantees no inadvertent gaps or overlaps of polygonal coverages is the topological model, examples of which are found in geographic information systems (GIS) like GRASS GIS or ArcGIS. Topological models store boundaries between polygons only once and register which polygonal area is on either side of a boundary.\nDeriving the set of (multi)polygons for each area with a constant range value from a topological model is straightforward; the other way around, reconstructing topology from a set of polygons typically involves setting thresholds on errors and handling gaps or overlaps.\n\n\n\n \nA tessellation is a sub-division of a space (area, volume) into smaller elements by ways of polygons. A regular tessellation does this with regular polygons: triangles, squares, or hexagons. Tessellations using squares are commonly used for spatial data and are called raster data. Raster data tessellate each spatial dimension \\(d\\) into regular cells, formed by left-closed and right-open intervals \\(d_i\\): \\[\\begin{equation}\nd_i = d_0 + [i \\times \\delta, (i+1) \\times \\delta)\n\\end{equation}\\] with \\(d_0\\) an offset, \\(\\delta\\) the interval (cell or pixel) size, and where the cell index \\(i\\) is an arbitrary but consecutive set of integers. The \\(\\delta\\) value is often taken negative for the \\(y\\)-axis (Northing), indicating that raster row numbers increasing Southwards correspond to \\(y\\)-coordinates increasing Northwards.\nWhereas in arbitrary polygon tessellations the assignment of points to polygons is ambiguous for points falling on a boundary shared by two polygons, using left-closed “[” and right-open “)” intervals in regular tessellations removes this ambiguity. This means that for rasters with negative \\(\\delta\\) values for the \\(y\\)-coordinate and positive for the \\(x\\)-coordinate, only the top-left corner point is part of each raster cell. An artifact resulting from this is shown in Figure 6.\n\n\nCode\nlibrary(stars) |&gt; suppressPackageStartupMessages()\npar(mar = rep(1, 4))\nls &lt;- st_sf(a = 2, st_sfc(st_linestring(rbind(c(0.1, 0), c(1, .9)))))\ngrd &lt;- st_as_stars(st_bbox(ls), nx = 10, ny = 10, xlim = c(0, 1.0), ylim = c(0, 1),\n   values = -1)\nr &lt;- st_rasterize(ls, grd, options = \"ALL_TOUCHED=TRUE\")\nr[r == -1] &lt;- NA\nplot(st_geometry(st_as_sf(grd)), border = 'orange', col = NA, \n     reset = FALSE, key.pos = NULL)\nplot(r, axes = FALSE, add = TRUE, breaks = \"equal\", main = NA) # ALL_TOUCHED=FALSE;\nplot(ls, add = TRUE, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 6: Rasterization artifact: as only top-left corners are part of the raster cell, only cells touching the red line below the diagonal line are rasterized\n\n\n\n\n\n\nTessellating the time dimension with left-closed right-open intervals is very common, and it reflects the implicit assumption underlying time series software such as the xts package in R, where time stamps indicate the start of time intervals. Different models can be combined: one could use simple feature polygons to tessellate space and combine this with a regular tessellation of time in order to cover a space time vector data cube. Raster and vector data cubes are discussed in ?@sec-datacube.\nAs mentioned above, besides square cells the other two shapes that can lead to regular tessellations of \\(R^2\\) are triangles and hexagons. On the sphere, there are a few more, including cube, octahedron, icosahedron, and dodecahedron. A spatial index that builds on the cube is s2geometry, the H3 library uses the icosahedron and densifies that with (mostly) hexagons. Mosaics that cover the entire Earth are also called discrete global grids.\n\n\n\n\n\nSpatial networks are typically composed of linear (LINESTRING) elements, but possess further topological properties describing the network coherence:\n\nstart- and end-points of a linestring may be connected to other linestring start or end points, forming a set of nodes and edges\nedges may be directed, to only allow for connection (flow, transport) in one way\n\nR packages including osmar (R-osmar?), stplanr (R-stplanr?), and sfnetworks (R-sfnetworks?) provide functionality for constructing network objects, and working with them, including computation of shortest or fastest routes through a network. Package spatstat (R-spatstat?; baddeley2015spatial?) has infrastructure for analysing point patterns on linear networks (?@sec-pointpatterns). Chapter 12 of (geocomp?) has a transportation application using networks.\n\n\n \n\n\n\n\nFor the following exercises, use R where possible.\n\nGive two examples of geometries in 2-D (flat) space that cannot be represented as simple feature geometries, and create a plot of them.\nRecompute the coordinates 10.542, 0.01, 45321.6789 using precision values 1, 1e3, 1e6, and 1e-2.\nDescribe a practical problem for which an n-ary intersection would be needed.\nHow can you create a Voronoi diagram (Figure 3) that has one closed polygons for every single point?\nGive the unary measure dimension for geometries POINT Z (0 1 1), LINESTRING Z (0 0 1,1 1 2), and POLYGON Z ((0 0 0,1 0 0,1 1 0,0 0 0))\nGive the DE-9IM relation between LINESTRING(0 0,1 0) and LINESTRING(0.5 0,0.5 1); explain the individual characters.\nCan a set of simple feature polygons form a coverage? If so, under which constraints?\nFor the nc counties in the dataset that comes with R package sf, find the points touched by four counties.\nHow would Figure 6 look like if \\(\\delta\\) for the \\(y\\)-coordinate was positive?"
  },
  {
    "objectID": "geom.html#simple-feature-geometries",
    "href": "geom.html#simple-feature-geometries",
    "title": "Geometries",
    "section": "",
    "text": "Simple feature geometries are a way to describe the geometries of features. By features we mean things that have a geometry, potentially implicitly some time properties, and further attributes that could include labels describing the thing and/or values quantitatively measuring it. The main application of simple feature geometries is to describe geometries in two-dimensional space by points, lines, or polygons. The “simple” adjective refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect.\nSimple features access is a standard (sfa?; sfa2?; iso?) for describing simple feature geometries. It includes:\n\n\na class hierarchy\na set of operations\nbinary and text encodings\n\nWe will first discuss the seven most common simple feature geometry types.\n\n\n \nThe most common simple feature geometries used to represent a single feature are:\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above\n\n\n\n\n\nCode\nlibrary(sf) |&gt; suppressPackageStartupMessages()\npar(mfrow = c(2,4))\npar(mar = c(1,1,1.2,1))\n\n# 1\np &lt;- st_point(0:1)\nplot(p, pch = 16)\ntitle(\"point\")\nbox(col = 'grey')\n\n# 2\nmp &lt;- st_multipoint(rbind(c(1,1), c(2, 2), c(4, 1), c(2, 3), c(1,4)))\nplot(mp, pch = 16)\ntitle(\"multipoint\")\nbox(col = 'grey')\n\n# 3\nls &lt;- st_linestring(rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)))\nplot(ls, lwd = 2)\ntitle(\"linestring\")\nbox(col = 'grey')\n\n# 4\nmls &lt;- st_multilinestring(list(\n  rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)),\n  rbind(c(3,0), c(4,1), c(2,1))))\nplot(mls, lwd = 2)\ntitle(\"multilinestring\")\nbox(col = 'grey')\n\n# 5 polygon\npo &lt;- st_polygon(list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),\n    rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))))\nplot(po, border = 'black', col = '#ff8888', lwd = 2)\ntitle(\"polygon\")\nbox(col = 'grey')\n\n# 6 multipolygon\nmpo &lt;- st_multipolygon(list(\n    list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),\n        rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))),\n    list(rbind(c(3,7), c(4,7), c(5,8), c(3,9), c(2,8), c(3,7)))))\nplot(mpo, border = 'black', col = '#ff8888', lwd = 2)\ntitle(\"multipolygon\")\nbox(col = 'grey')\n\n# 7 geometrycollection\ngc &lt;- st_geometrycollection(list(po, ls + c(0,5), st_point(c(2,5)), st_point(c(5,4))))\nplot(gc, border = 'black', col = '#ff6666', pch = 16, lwd = 2)\ntitle(\"geometrycollection\")\nbox(col = 'grey')\n\n\n\n\n\n\n\n\nFigure 1: Sketches of the main simple feature geometry types\n\n\n\n\n\nFigure 1 shows examples of these basic geometry types. The human-readable, “well-known text” (WKT) representation of the geometries plotted are:\n \n\n\nCode\np\nmp\nls\nmls\npo\nmpo\ngc\n\n\nPOINT (0 1)\nMULTIPOINT ((1 1), (2 2), (4 1), (2 3), (1 4))\nLINESTRING (1 1, 5 5, 5 6, 4 6, 3 4, 2 3)\nMULTILINESTRING ((1 1, 5 5, 5 6, 4 6, 3 4, 2 3), (3 0, 4 1, 2 1))\nPOLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),\n    (2 2, 3 3, 4 3, 4 2, 2 2))\nMULTIPOLYGON (((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),\n    (2 2, 3 3, 4 3, 4 2, 2 2)), ((3 7, 4 7, 5 8, 3 9, 2 8, 3 7)))\nGEOMETRYCOLLECTION (\n    POLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1),\n      (2 2 , 3 3, 4 3, 4 2, 2 2)),\n    LINESTRING (1 6, 5 10, 5 11, 4 11, 3 9, 2 8),\n    POINT (2 5),\n    POINT (5 4)\n)\nIn this representation, coordinates are separated by space, and points by commas. Sets are grouped by parentheses, and separated by commas. Polygons consist of an outer ring followed by zero or more inner rings denoting holes. Individual points in a geometry contain at least two coordinates: \\(x\\) and \\(y\\), in that order. If these coordinates refer to ellipsoidal coordinates, \\(x\\) and \\(y\\) usually refer to longitude and latitude, respectively, although sometimes to latitude and longitude (see ?@sec-projlib and ?@sec-axisorder).\n\n\n\n \nLinestrings are called simple when they do not self-intersect:\n\n\nCode\n(ls &lt;- st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0))))\n\n\nLINESTRING (0 0, 1 1, 2 2, 0 2, 1 1, 2 0)\n\n\nCode\nc(is_simple = st_is_simple(ls))\n\n\nis_simple \n    FALSE \n\n\nValid polygons and multi-polygons obey all of the following properties:\n\npolygon rings are closed (the last point equals the first)\npolygon holes (inner rings) are inside their exterior ring\npolygon inner rings maximally touch the exterior ring in single points, not over a line\na polygon ring does not repeat its own path\nin a multi-polygon, an external ring maximally touches another exterior ring in single points, not over a line\n\nIf this is not the case, the geometry concerned is not valid. Invalid geometries typically cause errors when they are processed, but they can usually be repaired to make them valid.\nA further convention is that the outer ring of a polygon is winded counter-clockwise, while the holes are winded clockwise, but polygons for which this is not the case are still considered valid. For polygons on the sphere, the “clockwise” concept is not very useful: if for instance we take the equator as polygon, is the Northern Hemisphere or the Southern Hemisphere “inside”? The convention taken here is to consider the area on the left while traversing the polygon is considered the polygon’s inside (see also ?@sec-ccw).\n\n\n\n \nIn addition to X and Y coordinates, single points (vertices) of simple feature geometries may have:\n\na Z coordinate, denoting altitude, and/or * an M value, denoting some “measure”\n\nThe M attribute shall be a property of the vertex. It sounds attractive to encode a time stamp in it for instance to pack movement data (trajectories) in LINESTRINGs. These become however invalid (or “non-simple”) once the trajectory self-intersects, which happens when only X and Y are considered for self-intersections.\nBoth Z and M are not found often, and software support to do something useful with them is (still) rare. Their WKT representations are fairly easily understood:\n\n\nCode\nst_point(c(1,3,2))\n\n\nPOINT Z (1 3 2)\n\n\nCode\nst_point(c(1,3,2), dim = \"XYM\")\n\n\nPOINT M (1 3 2)\n\n\nCode\nst_linestring(rbind(c(3,1,2,4), c(4,4,2,2)))\n\n\nLINESTRING ZM (3 1 2 4, 4 4 2 2)\n\n\n\n\n\n \nA very important concept in the feature geometry framework is that of the empty geometry.\nEmpty geometries arise naturally when we do geometrical operations (Section 1.2), for instance when we want to know the intersection of POINT (0 0) and POINT (1 1):\n\n\nCode\n(e &lt;- st_intersection(st_point(c(0,0)), st_point(c(1,1))))\n\n\nGEOMETRYCOLLECTION EMPTY\n\n\nand it represents essentially the empty set: when combining (unioning) an empty point with other non-empty geometries, it vanishes.\nAll geometry types have a special value representing the empty (typed) geometry, like\n\n\nCode\nst_point()\n\n\nPOINT EMPTY\n\n\nCode\nst_linestring(matrix(1,1,3)[0,], dim = \"XYM\")\n\n\nLINESTRING M EMPTY\n\n\nand so on, but they all point to the empty set, differing only in their dimension (Section 1.2.2).\n\n\n\nThere are 10 more geometry types which are more rare, but increasingly find implementation:\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nCIRCULARSTRING\nThe CircularString is the basic curve type, similar to a LineString in the linear world. A single segment requires three points, the start and end points (first and third) and any other point on the arc. The exception to this is for a closed circle, where the start and end points are the same. In this case the second point MUST be the centre of the arc, i.e., the opposite side of the circle. To chain arcs together, the last point of the previous arc becomes the first point of the next arc, just like in LineString. This means that a valid circular string must have an odd number of points greater than 1.\n\n\nCOMPOUNDCURVE\nA CompoundCurve is a single, continuous curve that has both curved (circular) segments and linear segments. That means that in addition to having well-formed components, the end point of every component (except the last) must be coincident with the start point of the following component.\n\n\nCURVEPOLYGON\nExample compound curve in a curve polygon: CURVEPOLYGON( COMPOUNDCURVE( CIRCULARSTRING(0 0,2 0, 2 1, 2 3, 4 3),(4 3, 4 5, 1 4, 0 0)), CIRCULARSTRING(1.7 1, 1.4 0.4, 1.6 0.4, 1.6 0.5, 1.7 1))\n\n\nMULTICURVE\nA MultiCurve is a 1 dimensional GeometryCollection whose elements are Curves. It can include linear strings, circular strings, or compound strings.\n\n\nMULTISURFACE\nA MultiSurface is a 2 dimensional GeometryCollection whose elements are Surfaces, all using coordinates from the same coordinate reference system.\n\n\nCURVE\nA Curve is a 1 dimensional geometric object usually stored as a sequence of Points, with the subtype of Curve specifying the form of the interpolation between Points\n\n\nSURFACE\nA Surface is a 2 dimensional geometric object\n\n\nPOLYHEDRALSURFACE\nA PolyhedralSurface is a contiguous collection of polygons, which share common boundary segments\n\n\nTIN\nA TIN (triangulated irregular network) is a PolyhedralSurface consisting only of Triangle patches.\n\n\nTRIANGLE\nA Triangle is a polygon with three distinct, non-collinear vertices and no interior boundary\n\n\n\nCIRCULARSTRING, COMPOUNDCURVE and CURVEPOLYGON are not described in the SFA standard, but in the SQL-MM part 3 standard. The descriptions above were copied from the PostGIS manual.\n \n\n\n\n \nPart of the simple feature standard are two encodings: a text and a binary encoding. The well-known text encoding, used above, is human-readable. The well-known binary encoding is machine-readable. Well-known binary (WKB) encodings are lossless and typically faster to work with than text encoding (and decoding), and they are used for instance in all communications between R package sf and the GDAL, GEOS, liblwgeom, and s2geometry libraries (?@fig-gdal-fig-nodetails)."
  },
  {
    "objectID": "geom.html#sec-opgeom",
    "href": "geom.html#sec-opgeom",
    "title": "Geometries",
    "section": "",
    "text": "Simple feature geometries can be queried for properties, or transformed or combined into new geometries, and combinations of geometries can be queried for further properties. This section gives an overview of the operations entirely focusing on geometrical properties. ?@sec-featureattributes focuses on the analysis of non-geometrical feature properties, in relationship to their geometries. Some of the material in this section appeared in (rjsf?).\nWe can categorise operations on geometries in terms of what they take as input, and what they return as output. In terms of output we have operations that return:\n\npredicates: a logical asserting a certain property is TRUE\nmeasures: a quantity (a numeric value, possibly with measurement unit)\ntransformations: newly generated geometries\n\nand in terms of what they operate on, we distinguish operations that are:\n\nunary when they work on a single geometry\nbinary when they work on pairs of geometries\nn-ary when they work on sets of geometries\n\n\n\n \nUnary predicates describe a certain property of a geometry. The predicates is_simple, is_valid, and is_empty return respectively whether a geometry is simple, valid, or empty. Given a coordinate reference system, is_longlat returns whether the coordinates are geographic or projected. is(geometry, class) checks whether a geometry belongs to a particular class.\n \n\n\n\n \nThe Dimensionally Extended Nine-Intersection Model (DE-9IM, de9im1?; de9im2?) is a model that describes the qualitative relation between any two geometries in two-dimensional space (\\(R^2\\)). Any geometry has a dimension value that is:\n\n0 for points,\n1 for linear geometries,\n2 for polygonal geometries, and\nF (false) for empty geometries\n\nAny geometry also has an inside (I), a boundary (B), and an exterior (E); these roles are obvious for polygons, however, for:\n\nlines the boundary is formed by the end points, and the interior by all non-end points on the line\npoints have a zero-dimensional inside but no boundary\n\n\n\nCode\nlibrary(sf)\npolygon &lt;- po &lt;- st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0))))\np0 &lt;- st_polygon(list(rbind(c(-1,-1), c(2,-1), c(2,2), c(-1,2), c(-1,-1))))\nline &lt;- li &lt;- st_linestring(rbind(c(.5, -.5), c(.5, 0.5)))\ns &lt;- st_sfc(po, li)\n\npar(mfrow = c(3,3))\npar(mar = c(1,1,1,1))\n\n# \"1020F1102\"\n# 1: 1\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"I(pol)\",intersect(),\"I(line) = 1\")))\nlines(rbind(c(.5,0), c(.5,.495)), col = 'red', lwd = 2)\npoints(0.5, 0.5, pch = 1)\n\n# 2: 0\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"I(pol)\",intersect(),\"B(line) = 0\")))\npoints(0.5, 0.5, col = 'red', pch = 16)\n\n# 3: 2\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"I(pol)\",intersect(),\"E(line) = 2\")))\nplot(po, col = '#ff8888', add = TRUE)\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', add = TRUE)\n\n# 4: 0\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"B(pol)\",intersect(),\"I(line) = 0\")))\npoints(.5, 0, col = 'red', pch = 16)\n\n# 5: F\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"B(pol)\",intersect(),\"B(line) = F\")))\n\n# 6: 1\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"B(pol)\",intersect(),\"E(line) = 1\")))\nplot(po, border = 'red', col = NA, add = TRUE, lwd = 2)\n\n# 7: 1\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"E(pol)\",intersect(),\"I(line) = 1\")))\nlines(rbind(c(.5, -.5), c(.5, 0)), col = 'red', lwd = 2)\n\n# 8: 0\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"E(pol)\",intersect(),\"B(line) = 0\")))\npoints(.5, -.5, col = 'red', pch = 16)\n\n# 9: 2\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', main = expression(paste(\"E(pol)\",intersect(),\"E(line) = 2\")))\nplot(p0 / po, col = '#ff8888', add = TRUE)\nplot(s, col = c(NA, 'darkgreen'), border = 'blue', add = TRUE)\n\n\n\n\n\n\n\n\nFigure 2: DE-9IM: intersections between the interior, boundary, and exterior of a polygon (rows) and of a linestring (columns) indicated by red\n\n\n\n\n\nFigure 2 shows the intersections between the I, B, and E components of a polygon and a linestring indicated by red; the sub-plot title gives the dimension of these intersections (0, 1, 2 or F). The relationship between the polygon and the line geometry is the concatenation of these dimensions:\n\n\nCode\nst_relate(polygon, line)\n\n\n     [,1]       \n[1,] \"1020F1102\"\n\n\nwhere the first three characters are associated with the inside of the first geometry (polygon): Figure 2 is summarised row-wise. Using this ability to express relationships, we can also query pairs of geometries about particular conditions expressed in a mask string. As an example, the string \"*0*******\" would evaluate TRUE when the second geometry has one or more boundary points in common with the interior of the first geometry; the symbol * standing for “any dimensionality” (0, 1, 2 or F). The mask string \"T********\" matches pairs of geometry with intersecting interiors, where the symbol T stands for any non-empty intersection of dimensionality 0, 1, or 2.\nBinary predicates are further described using normal-language verbs, using DE-9IM definitions. For instance, the predicate equals corresponds to the relationship \"T*F**FFF*\". If any two geometries obey this relationship, they are (topologically) equal, but may have a different ordering of nodes.\nA list of binary predicates is:\n\n\n\n\n\n\n\n\npredicate\nmeaning\ninverse of\n\n\n\n\ncontains\nNone of the points of A are outside B\nwithin\n\n\ncontains_properly\nA contains B and B has no points in common with the boundary of A\n\n\n\ncovers\nNo points of B lie in the exterior of A\ncovered_by\n\n\ncovered_by\nInverse of covers\n\n\n\ncrosses\nA and B have some but not all interior points in common\n\n\n\ndisjoint\nA and B have no points in common\nintersects\n\n\nequals\nA and B are topologically equal: node order or number of nodes may differ; identical to A contains B and A within B\n\n\n\nequals_exact\nA and B are geometrically equal, and have identical node order\n\n\n\nintersects\nA and B are not disjoint\ndisjoint\n\n\nis_within_distance\nA is closer to B than a given distance\n\n\n\nwithin\nNone of the points of B are outside A\ncontains\n\n\ntouches\nA and B have at least one boundary point in common, but no interior points\n\n\n\noverlaps\nA and B have some points in common; the dimension of these is identical to that of A and B\n\n\n\nrelate\nGiven a mask pattern, return whether A and B adhere to this pattern\n\n\n\n\n \nThe Wikipedia DE-9IM page provides the relate patterns for each of these verbs. They are important to check out; for instance covers and contains (and their inverses) are often not completely intuitive:\n\nif A contains B, B has no points in common with the exterior or boundary of A\nif A covers B, B has no points in common with the exterior of A\n\n\n\n\n\nUnary measures return a measure or quantity that describes a property of the geometry:\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\n \n\n\n\n\ndistance returns the distance between pairs of geometries. The qualitative measure relate (without mask) gives the relation pattern. A description of the geometrical relationship between two geometries is given in Section 1.2.2.\n \n\n\n\n\nUnary transformations work on a per-geometry basis, and return for each geometry a new geometry.\n\n\n\n\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nwrap_dateline\ncut into pieces that no longer cover or cross the dateline\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry (Figure 3)\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith a (arbitrary) point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system (?@sec-cs)\n\n\ntriangulate\nwith Delauney triangulated polygon(s) (Figure 3)\n\n\nvoronoi\nwith the Voronoi tessellation of an input geometry (Figure 3)\n\n\nzm\nwith removed or added Z and/or M coordinates\n\n\ncollection_extract\nwith sub-geometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n+\nthat is shifted over a given vector\n\n\n*\nthat is multiplied by a scalar or matrix\n\n\n\n\n\nCode\npar(mar = rep(0,4), mfrow = c(1, 3))\nset.seed(133331)\nmp &lt;- st_multipoint(matrix(runif(20), 10))\nplot(mp, cex = 2)\nplot(st_convex_hull(mp), add = TRUE, col = NA, border = 'red')\nbox()\nplot(mp, cex = 2)\nplot(st_voronoi(mp), add = TRUE, col = NA, border = 'red')\nbox()\nplot(mp, cex = 2)\nplot(st_triangulate(mp), add = TRUE, col = NA, border = 'darkgreen')\nbox()\n\n\n\n\n\n\n\n\nFigure 3: For a set of points, left: convex hull (red); middle: Voronoi polygons; right: Delauney triangulation\n\n\n\n\n\n \n\n\n\n\nBinary transformers are functions that return a geometry based on operating on a pair of geometries. They include:\n\n\n\n\n\n\n\n\nfunction\nreturns\ninfix operator\n\n\n\n\nintersection\nthe overlapping geometries for pair of geometries\n&\n\n\nunion\nthe combination of the geometries; removes internal boundaries and duplicate points, nodes or line pieces\n|\n\n\ndifference\nthe geometries of the first after removing the overlap with the second geometry\n/\n\n\nsym_difference\nthe combinations of the geometries after removing where they intersect; the negation (opposite) of intersection\n%/%\n\n\n\n \n\n\n\n\nN-ary transformers operate on sets of geometries. union can be applied to a set of geometries to return its geometrical union. Otherwise, any set of geometries can be combined into a MULTI-type geometry when they have equal dimension, or else into a GEOMETRYCOLLECTION. Without unioning, this may lead to a geometry that is not valid, for instance when two polygon rings have a boundary line in common.\n \nN-ary intersection and difference take a single argument but operate (sequentially) on all pairs, triples, quadruples, etc. Consider the plot in Figure 4: how do we identify the area where all three boxes overlap? Using binary intersections gives us intersections for all pairs: 1-1, 1-2, 1-3, 2-1, 2-2, 2-3, 3-1, 3-2, 3-3, but that does not let us identify areas where more than two geometries intersect. Figure 4 (right) shows the n-ary intersection: the seven unique, non-overlapping geometries originating from intersection of one, two, or more geometries.\n\n\nCode\npar(mar = rep(.1, 4), mfrow = c(1, 2))\nsq &lt;- function(pt, sz = 1) st_polygon(list(rbind(c(pt - sz), \n  c(pt[1] + sz, pt[2] - sz), c(pt + sz), c(pt[1] - sz, pt[2] + sz), c(pt - sz))))\nx &lt;- st_sf(box = 1:3, st_sfc(sq(c(0, 0)), sq(c(1.7, -0.5)), sq(c(0.5, 1))))\nplot(st_geometry(x), col = NA, border = sf.colors(3, categorical = TRUE), lwd = 3)\nplot(st_intersection(st_geometry(x)), col = sf.colors(7, categorical=TRUE, alpha = .5))\n\n\n\n\n\n\n\n\nFigure 4: Left: three overlapping squares – how do we identify the small box where all three overlap? Right: unique, non-overlapping n-ary intersections\n\n\n\n\n\nSimilarly, one can compute an n-ary difference from a set \\(\\{s_1, s_2,\ns_3, ...\\}\\) by creating differences \\(\\{s_1, s_2-s_1, s_3-s_2-s_1,\n...\\}\\). This is shown in Figure 5, (left) for the original set, and (right) for the set after reversing its order to make clear that the result here depends on the ordering of the input geometries. Again, resulting geometries do not overlap.\n\n\nCode\npar(mar = rep(.1, 4), mfrow = c(1, 2)) \nxg &lt;- st_geometry(x)\nplot(st_difference(xg), col = sf.colors(3, alpha = .5, categorical=TRUE))\nplot(st_difference(xg[3:1]), col = sf.colors(3, alpha = .5, categorical=TRUE))\n\n\n\n\n\n\n\n\nFigure 5: Difference between subsequent boxes, left: in original order; right: in reverse order"
  },
  {
    "objectID": "geom.html#sec-precision",
    "href": "geom.html#sec-precision",
    "title": "Geometries",
    "section": "",
    "text": "Geometrical operations, such as finding out whether a certain point is on a line, may fail when coordinates are represented by double precision floating point numbers, such as 8-byte doubles used in R. An often chosen remedy is to limit the precision of the coordinates before the operation. For this, a precision model is adopted; the most common is to choose a factor \\(p\\) and compute rounded coordinates \\(c'\\) from original coordinates \\(c\\) by \\[c' = \\mbox{round}(p \\cdot c) / p\\]\nRounding of this kind brings the coordinates to points on a regular grid with spacing \\(1/p\\), which is beneficial for geometric computations. Of course, it also affects all computations like areas and distances, and may turn valid geometries into invalid ones. Which precision values are best for which application is often a matter of common sense combined with trial and error."
  },
  {
    "objectID": "geom.html#sec-coverages",
    "href": "geom.html#sec-coverages",
    "title": "Geometries",
    "section": "",
    "text": "The Open Geospatial Consortium defines a coverage as a “feature that acts as a function to return values from its range for any direct position within its spatiotemporal domain” (ogccov?). Having a function implies that for every space time “point”, every combination of a spatial point and a moment in time of the spatiotemporal domain, we have a single value for the range. This is a very common situation for spatiotemporal phenomena, a few examples can be given:\n\nboundary disputes aside, at a give time every point in a region (domain) belongs to a single administrative unit (range)\nat any given moment in time, every point in a region (domain) has a certain land cover type (range)\nevery point in an area (domain) has a single surface elevation (range), which could be measured with respect to a given mean sea level surface\nevery spatiotemporal point in a three-dimensional body of air (domain) has single value for temperature (range)\n\nA caveat here is that because observation or measurement always takes time and requires space, measured values are always an average over a spatiotemporal volume, and hence range variables can rarely be measured for true, zero-volume “points”; for many practical cases however the measured volume is small enough to be considered a “point”. For a variable like land cover type the volume needs to be chosen such that the types distinguished make sense with respect to the measured areal units.\nIn the first two of the given examples the range variable is categorical, in the last two the range variable is continuous. For categorical range variables, if large connected areas have a constant range value, an efficient way to represent these data is by storing the boundaries of the areas with constant value, such as country boundaries. Although this can be done (and is often done) by a set of simple feature geometries (polygons or multi-polygons), this brings along some challenges:\n\nit is hard to guarantee for such a set of simple feature polygons that they do not overlap, or that there are no unwanted gaps between them\nsimple features have no way of assigning points on the boundary of two adjacent polygons uniquely to a single polygon, which conflicts with the interpretation as coverage\n\n\n\n\nA data model that guarantees no inadvertent gaps or overlaps of polygonal coverages is the topological model, examples of which are found in geographic information systems (GIS) like GRASS GIS or ArcGIS. Topological models store boundaries between polygons only once and register which polygonal area is on either side of a boundary.\nDeriving the set of (multi)polygons for each area with a constant range value from a topological model is straightforward; the other way around, reconstructing topology from a set of polygons typically involves setting thresholds on errors and handling gaps or overlaps.\n\n\n\n \nA tessellation is a sub-division of a space (area, volume) into smaller elements by ways of polygons. A regular tessellation does this with regular polygons: triangles, squares, or hexagons. Tessellations using squares are commonly used for spatial data and are called raster data. Raster data tessellate each spatial dimension \\(d\\) into regular cells, formed by left-closed and right-open intervals \\(d_i\\): \\[\\begin{equation}\nd_i = d_0 + [i \\times \\delta, (i+1) \\times \\delta)\n\\end{equation}\\] with \\(d_0\\) an offset, \\(\\delta\\) the interval (cell or pixel) size, and where the cell index \\(i\\) is an arbitrary but consecutive set of integers. The \\(\\delta\\) value is often taken negative for the \\(y\\)-axis (Northing), indicating that raster row numbers increasing Southwards correspond to \\(y\\)-coordinates increasing Northwards.\nWhereas in arbitrary polygon tessellations the assignment of points to polygons is ambiguous for points falling on a boundary shared by two polygons, using left-closed “[” and right-open “)” intervals in regular tessellations removes this ambiguity. This means that for rasters with negative \\(\\delta\\) values for the \\(y\\)-coordinate and positive for the \\(x\\)-coordinate, only the top-left corner point is part of each raster cell. An artifact resulting from this is shown in Figure 6.\n\n\nCode\nlibrary(stars) |&gt; suppressPackageStartupMessages()\npar(mar = rep(1, 4))\nls &lt;- st_sf(a = 2, st_sfc(st_linestring(rbind(c(0.1, 0), c(1, .9)))))\ngrd &lt;- st_as_stars(st_bbox(ls), nx = 10, ny = 10, xlim = c(0, 1.0), ylim = c(0, 1),\n   values = -1)\nr &lt;- st_rasterize(ls, grd, options = \"ALL_TOUCHED=TRUE\")\nr[r == -1] &lt;- NA\nplot(st_geometry(st_as_sf(grd)), border = 'orange', col = NA, \n     reset = FALSE, key.pos = NULL)\nplot(r, axes = FALSE, add = TRUE, breaks = \"equal\", main = NA) # ALL_TOUCHED=FALSE;\nplot(ls, add = TRUE, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 6: Rasterization artifact: as only top-left corners are part of the raster cell, only cells touching the red line below the diagonal line are rasterized\n\n\n\n\n\n\nTessellating the time dimension with left-closed right-open intervals is very common, and it reflects the implicit assumption underlying time series software such as the xts package in R, where time stamps indicate the start of time intervals. Different models can be combined: one could use simple feature polygons to tessellate space and combine this with a regular tessellation of time in order to cover a space time vector data cube. Raster and vector data cubes are discussed in ?@sec-datacube.\nAs mentioned above, besides square cells the other two shapes that can lead to regular tessellations of \\(R^2\\) are triangles and hexagons. On the sphere, there are a few more, including cube, octahedron, icosahedron, and dodecahedron. A spatial index that builds on the cube is s2geometry, the H3 library uses the icosahedron and densifies that with (mostly) hexagons. Mosaics that cover the entire Earth are also called discrete global grids."
  },
  {
    "objectID": "geom.html#networks",
    "href": "geom.html#networks",
    "title": "Geometries",
    "section": "",
    "text": "Spatial networks are typically composed of linear (LINESTRING) elements, but possess further topological properties describing the network coherence:\n\nstart- and end-points of a linestring may be connected to other linestring start or end points, forming a set of nodes and edges\nedges may be directed, to only allow for connection (flow, transport) in one way\n\nR packages including osmar (R-osmar?), stplanr (R-stplanr?), and sfnetworks (R-sfnetworks?) provide functionality for constructing network objects, and working with them, including computation of shortest or fastest routes through a network. Package spatstat (R-spatstat?; baddeley2015spatial?) has infrastructure for analysing point patterns on linear networks (?@sec-pointpatterns). Chapter 12 of (geocomp?) has a transportation application using networks."
  },
  {
    "objectID": "geom.html#exercises",
    "href": "geom.html#exercises",
    "title": "Geometries",
    "section": "",
    "text": "For the following exercises, use R where possible.\n\nGive two examples of geometries in 2-D (flat) space that cannot be represented as simple feature geometries, and create a plot of them.\nRecompute the coordinates 10.542, 0.01, 45321.6789 using precision values 1, 1e3, 1e6, and 1e-2.\nDescribe a practical problem for which an n-ary intersection would be needed.\nHow can you create a Voronoi diagram (Figure 3) that has one closed polygons for every single point?\nGive the unary measure dimension for geometries POINT Z (0 1 1), LINESTRING Z (0 0 1,1 1 2), and POLYGON Z ((0 0 0,1 0 0,1 1 0,0 0 0))\nGive the DE-9IM relation between LINESTRING(0 0,1 0) and LINESTRING(0.5 0,0.5 1); explain the individual characters.\nCan a set of simple feature polygons form a coverage? If so, under which constraints?\nFor the nc counties in the dataset that comes with R package sf, find the points touched by four counties.\nHow would Figure 6 look like if \\(\\delta\\) for the \\(y\\)-coordinate was positive?"
  },
  {
    "objectID": "fldpln.html#executive-summary",
    "href": "fldpln.html#executive-summary",
    "title": "fldpln",
    "section": "Executive Summary",
    "text": "Executive Summary\nAs a pixel based, multi-source flood inundation library, FLDPLN represents the next step up in terms of complexity and process representation in flood inundation library creation by representing both backfill and spillover flooding processes using geospatial approximations. The high resolution nature of the library combined with the flexible library access patterns makes FLDPLN more suited to flexible access patterns, high-interest areas, and anyone who could potential benefit from a more complex approach to flood inundation mapping without the need to include expensive shallow water equation solvers. See the libraries in action at the Kansas Flood Mapping Dashboard.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "fldpln"
    ]
  },
  {
    "objectID": "fileio.html",
    "href": "fileio.html",
    "title": "FILE I/O",
    "section": "",
    "text": "Figure 1: AI generated image prompt: Can you draw me a wizard doing battle with a computer who is on fire?",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#critical-differences-in-language-semantics",
    "href": "fileio.html#critical-differences-in-language-semantics",
    "title": "FILE I/O",
    "section": "Critical differences in language semantics",
    "text": "Critical differences in language semantics\n\nIndexingNomenclature: Libraries and packagesInstall and Import PackagesData typingTruthinessTypes of nothing\n\n\nIndexing in Python starts with 0. In R it starts with 1.\n\n\n\nR\n\nmy_vector &lt;- c(10, 20, 30, 40, 50)\n# Accessing the first element\nfirst_element &lt;- my_vector[1] \nprint(first_element)\n\n[1] 10\n\nlast_element = my_vector[length(my_vector)] \nprint(last_element)\n\n[1] 50\n\n\n\n\n\nPython\n\nmy_list = [10, 20, 30, 40, 50]\n# Accessing the first element\nfirst_element = my_list[0] \nprint(first_element)\n\n10\n\nlast_element = my_list[-1] \nprint(last_element)\n\n50\n\n\n\n\n\n\n\nMost will use the terms interchangeably, everyone knows the principle you are aiming to communicate, and only the most nitpicky will care to correct you. The sematic distinction is…\n\n\n\n\n\n\n\nR - Libraries\nPython - Packages\n\n\n\n\nIn R, packages are the collections of functions and other data bundled together and distributed via either GitHub, or CRAN (The Comprehensive R Archive Network). Packages extend the functionality of R by providing additional functions and tools for specific tasks or domains. Libraries are the term used for the directory in which a packages functions are stored.\nIn Python, packages are directories that contain multiple modules and a special init.py file. Packages provide a way to organize related modules into a hierarchical structure. They enable you to create reusable code libraries and distribute them for others to use and are distributed via the Python Package Index (PyPI) repository for Python packages, and the pip package manager or conda environment are used to install and manage packages. Libraries are groups of packages that make accomplishing a particular task easier.\n\n\n\n\n\n\n\n\nR\n```{r}\nutils::remove.packages(\"package\")\n\ninstall.packages(\"package\")\nremotes.install_github(\"NOAA-OWP/RRASSLER@branch_name\")\n\nlibrary(\"package_name\") \n```\nIf you get an error along the lines of cannot open URL 'https://api.github.com/repos/&lt;repo_here&gt;/tarball/HEAD, you can try the tips I outline here. \n\n\n\nPython\n```{python}\n# With conda\nconda remove package_name\nconda install package_name\n\n# Or with pip\npip uninstall package_name\npip install package_name\npip install package_name=='#.##'\n\n&gt;&gt;&gt; import package_name \n```\n\n\n\n\n\n\n\n\nR\n\n# Define variables with different data types\nname &lt;- \"Bob\"  # String\nage &lt;- 25  # Integer\nheight &lt;- 3.14159  # Double (float)\nis_student &lt;- TRUE  # Boolean\n\n# Use the class() function to check data types\nclass(name)  # Character\n\n[1] \"character\"\n\nclass(age)   # Integer\n\n[1] \"numeric\"\n\nclass(height) # Numeric\n\n[1] \"numeric\"\n\nclass(is_student) # Logical\n\n[1] \"logical\"\n\n# Alternatively, use typeof() for more detailed info\ntypeof(name)  # character\n\n[1] \"character\"\n\ntypeof(age)   # integer\n\n[1] \"double\"\n\ntypeof(height) # double\n\n[1] \"double\"\n\ntypeof(is_student) # logical\n\n[1] \"logical\"\n\n# if (inherit(X, \"Spatial\"))\n#   X &lt;- sf::st_as_sf(X)\n# \n# if (inherit(Y, \"Raster\"))\n#   Y &lt;- stars::st_as_stars(Y)\n# \n# if (inherit(Y, \"SpatRaster\"))\n#   Y &lt;- stars::st_as_stars(Y) # that one does not exist do it yourself\n# \n# if(!is(X, \"sf\"))\n#    stop(\"Y has to be a spatial object \")\n# \n# if(!is(Y, \"stars\")\n#    stop(\"Y has to be a raster object\")\n# \n# geom = sf::st_geometry(X)\n# \n# if (!is(geom, \"sfc_POLYGON\") & !is(geom, \"sfc_MULTIPOLYGON\"))\n#    stop(\"Y has to be a spatial polygon object \")\n\n\n\n\nPython\n\n# Define variables with different data types\nname = \"Bob\"  # String\nage = 25  # Integer\nheight = 3.14159  # Float\nis_student = True  # Boolean\n\n# Use the type() function to check data types\nprint(f\"Name (string): {type(name)}\")\n\nName (string): &lt;class 'str'&gt;\n\nprint(f\"Age (integer): {type(age)}\")\n\nAge (integer): &lt;class 'int'&gt;\n\nprint(f\"Pi (float): {type(height)}\")\n\nPi (float): &lt;class 'float'&gt;\n\nprint(f\"Is Adult (boolean): {type(is_student)}\")\n\nIs Adult (boolean): &lt;class 'bool'&gt;\n\n\n\n\n\n\n\nIn R, only a single logical TRUE is considered true in conditions like if(). FALSE is false. Numeric 0 is not automatically false, and non-zero numbers are not automatically true in if. NA in a condition usually results in an error or missing value. Using vectors in if usually results in a warning, as if expects a single value. Logical operations (&, |) work element-wise on vectors.\n\nTrue: TRUE, 1\nFalse: FALSE, 0\n\nIn Python, many things have inherent “truthiness”:\n\nTrue: True, non-empty sequences (lists, tuples, strings, dicts, sets), non-zero numbers.\nFalse: False, None, empty sequences, zero ( 0, 0.0, 0j).\n\n\n\n\nR\n\n# Explicit logical values\nif (TRUE) { message(\"TRUE is true\") }\n\nTRUE is true\n\nif (!FALSE) { message(\"FALSE is false\") }\n\nFALSE is false\n\n# Numeric values in `if` - Generally avoid this\nif (1) { message(\"1 is true\") } # This works but is bad practice\n\n1 is true\n\nif (0) { message(\"0 is true\") } # This block is NOT executed\nif (-1) { message(\"-1 is true\") } # This works but is bad practice\n\n-1 is true\n\n# Using logical vectors (element-wise)\nlogical_vec &lt;- c(TRUE, FALSE, TRUE)\nnumeric_vec &lt;- c(1, 0, -5)\n\nprint(logical_vec & c(TRUE, TRUE, FALSE)) # Element-wise AND -&gt; TRUE FALSE FALSE\n\n[1]  TRUE FALSE FALSE\n\nprint(logical_vec | c(FALSE, FALSE, FALSE)) # Element-wise OR -&gt; TRUE FALSE TRUE\n\n[1]  TRUE FALSE  TRUE\n\nprint(!logical_vec) # Element-wise NOT -&gt; FALSE TRUE FALSE\n\n[1] FALSE  TRUE FALSE\n\n# Be careful with NA\nval &lt;- NA\n# if (val) { message(\"NA is true?\") } # ERROR: argument is of length zero\n# if (!val) { message(\"NA is false?\") } # ERROR: argument is of length zero\nprint(is.na(val)) # TRUE\n\n[1] TRUE\n\n# Use identical() for exact comparison\nprint(identical(TRUE, TRUE)) # TRUE\n\n[1] TRUE\n\nprint(identical(1, TRUE)) # FALSE - Types differ\n\n[1] FALSE\n\n# Testing values that might represent boolean concepts\nvalues &lt;- c(TRUE, FALSE, TRUE, FALSE) # Logical\nprint(values == TRUE)  # TRUE FALSE TRUE FALSE\n\n[1]  TRUE FALSE  TRUE FALSE\n\nprint(values == FALSE) # FALSE TRUE FALSE TRUE\n\n[1] FALSE  TRUE FALSE  TRUE\n\nstrings &lt;- c(\"True\", \"False\", \"TRUE\", \"FALSE\", NA) # Character\nprint(strings == \"True\") # TRUE FALSE FALSE FALSE NA (Case-sensitive)\n\n[1]  TRUE FALSE FALSE FALSE    NA\n\nprint(tolower(strings) == \"true\") # TRUE FALSE TRUE FALSE NA (Case-insensitive)\n\n[1]  TRUE FALSE  TRUE FALSE    NA\n\nnumbers &lt;- c(1, 0, 1, 0, NA) # Numeric\nprint(numbers == 1) # TRUE FALSE TRUE FALSE NA\n\n[1]  TRUE FALSE  TRUE FALSE    NA\n\nprint(numbers == 0) # FALSE TRUE FALSE TRUE NA\n\n[1] FALSE  TRUE FALSE  TRUE    NA\n\n# Check for NA specifically\nprint(is.na(strings)) # FALSE FALSE FALSE FALSE TRUE\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nprint(is.na(numbers)) # FALSE FALSE FALSE FALSE TRUE\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\n\n\n\n\nPython\n\n#import pandas as pd\n#import numpy as np\n\n# Basic truthiness\nif True: print(\"True is truthy\")\n\nTrue is truthy\n\nif \"hello\": print(\"'hello' is truthy\")\n\n'hello' is truthy\n\nif [1, 2]: print(\"[1, 2] is truthy\")\n\n[1, 2] is truthy\n\nif {\"a\": 1}: print(\"{'a': 1} is truthy\")\n\n{'a': 1} is truthy\n\nif 1: print(\"1 is truthy\")\n\n1 is truthy\n\nif -1: print(\"-1 is truthy\")\n\n-1 is truthy\n\nif not False: print(\"False is falsy\")\n\nFalse is falsy\n\nif not None: print(\"None is falsy\")\n\nNone is falsy\n\nif not \"\": print(\"'' is falsy\")\n\n'' is falsy\n\nif not []: print(\"[] is falsy\")\n\n[] is falsy\n\nif not {}: print(\"{} is falsy\")\n\n{} is falsy\n\nif not 0: print(\"0 is falsy\")\n\n0 is falsy\n\nif not 0.0: print(\"0.0 is falsy\")\n\n0.0 is falsy\n\n\n# Pandas/Numpy context (similar to your example)\n# data = {'col_bool': [True, False, True, False, None], \n#         'col_str': ['True', 'False', 'TRUE', 'FALSE', None],\n#         'col_num': [1, 0, 1, 0, np.nan],\n#         'col_nan': [np.nan, False, True, np.nan, None]}\n# df = pd.DataFrame(data)\n# \n# # Testing boolean type values (handling potential None)\n# print(\"\\nTesting boolean type values:\")\n# print(df['col_bool'] == True)   # Compares element-wise, None -&gt; False\n# print(df['col_bool'].eq(True))  # Another way, same result\n# print(df['col_bool'].astype(bool)) # Converts None -&gt; False (often not desired!)\n# print(df['col_bool'].fillna(False).astype(bool)) # Explicit NA handling\n# \n# # Testing string type values (case-sensitive, None becomes NaN -&gt; False)\n# print(\"\\nTesting string type values (case-sensitive):\")\n# print(df['col_str'] == 'True') \n# print(df['col_str'].str.lower() == 'true') # Handles case, NA -&gt; False\n# \n# # Testing numeric type values (NaN -&gt; False)\n# print(\"\\nTesting numeric type values:\")\n# print(df['col_num'] == 1) \n# print(df['col_num'] == 0) \n# \n# # Test for NaN/None values specifically\n# print(\"\\nTesting for NaN/None values:\")\n# print(df['col_nan'].isna()) # Correct way to check for NaN or None in Pandas\n# print(df['col_nan'].isnull()) # Alias for isna()\n\n\n\n\n\n\nR has several ways to represent “nothing”:\nNULL: Represents the absence of an object or an undefined value. It’s often returned by functions that might fail or have no result. It is not typically used for missing data points within a vector. is.null() checks for it. NULL has length 0.\nNA: “Not Available”. R’s primary indicator for missing data within vectors, factors, matrices, data frames. It acts as a placeholder. Most computations involving NA result in NA. is.na() checks for it.\nNaN: “Not a Number”. A specific type of NA resulting from undefined mathematical operations (e.g., 0/0, sqrt(-1)). is.nan() checks for it. is.na(NaN) is also TRUE.\nPython uses primarily:\nNone: Python’s null object singleton. Represents the absence of a value. Functions that don’t explicitly return something return None. Use is None (identity check) for comparison.\nfloat(‘nan’) or math.nan or numpy.nan: Represents “Not a Number”, the floating-point representation for undefined mathematical results. Behaves according to IEEE 754 standard (e.g., nan != nan). Use math.isnan(), numpy.isnan(), or pandas.isna(). Pandas uses None, np.nan, and pd.NaT (for datetime) somewhat interchangeably to represent missing data, depending on the column’s data type.\n\n\n\nR\n\n# NULL\nnull_obj &lt;- NULL\nmessage(\"Is null_obj NULL? \", is.null(null_obj)) # TRUE\n\nIs null_obj NULL? TRUE\n\nmessage(\"Length of null_obj: \", length(null_obj)) # 0\n\nLength of null_obj: 0\n\nvec_with_null &lt;- c(1, 2, NULL, 4) # NULL is dropped!\nmessage(\"Vector with NULL: \", paste(vec_with_null, collapse=\", \")) # 1, 2, 4\n\nVector with NULL: 1, 2, 4\n\n# NA (Missing Value)\nna_val &lt;- NA\nmessage(\"Is na_val NA? \", is.na(na_val)) # TRUE\n\nIs na_val NA? TRUE\n\nvec_with_na &lt;- c(1, 2, NA, 4)\nmessage(\"Vector with NA: \", paste(vec_with_na, collapse=\", \")) # 1, 2, NA, 4\n\nVector with NA: 1, 2, NA, 4\n\nmessage(\"Length of vec_with_na: \", length(vec_with_na)) # 4\n\nLength of vec_with_na: 4\n\nmessage(\"Sum of vec_with_na: \", sum(vec_with_na)) # NA\n\nSum of vec_with_na: NA\n\nmessage(\"Sum excluding NA: \", sum(vec_with_na, na.rm = TRUE)) # 7\n\nSum excluding NA: 7\n\n# NaN (Not a Number)\nnan_val &lt;- 0/0\nmessage(\"Is nan_val NaN? \", is.nan(nan_val)) # TRUE\n\nIs nan_val NaN? TRUE\n\nmessage(\"Is nan_val NA? \", is.na(nan_val))   # TRUE (NaN is a type of NA)\n\nIs nan_val NA? TRUE\n\nvec_with_nan &lt;- c(1, 2, NaN, 4)\nmessage(\"Vector with NaN: \", paste(vec_with_nan, collapse=\", \")) # 1, 2, NaN, 4\n\nVector with NaN: 1, 2, NaN, 4\n\nmessage(\"Sum of vec_with_nan: \", sum(vec_with_nan)) # NaN\n\nSum of vec_with_nan: NaN\n\nmessage(\"Sum excluding NaN/NA: \", sum(vec_with_nan, na.rm = TRUE)) # 7\n\nSum excluding NaN/NA: 7\n\n# Distinguishing NA and NULL in lists\nmy_list &lt;- list(a = 1, b = NULL, c = NA, d = \"hello\")\nprint(my_list)\n\n$a\n[1] 1\n\n$b\nNULL\n\n$c\n[1] NA\n\n$d\n[1] \"hello\"\n\nmessage(\"Is element 'b' NULL? \", is.null(my_list$b)) # TRUE\n\nIs element 'b' NULL? TRUE\n\nmessage(\"Is element 'c' NA? \", is.na(my_list$c))     # TRUE\n\nIs element 'c' NA? TRUE\n\n\n\n\n\nPython\n```{python}\nimport math\nimport numpy as np\nimport pandas as pd\n\n# None\nnone_obj = None\nprint(f\"Is none_obj None? {none_obj is None}\") # True (use 'is' for None)\nprint(f\"Type of None: {type(none_obj)}\") # &lt;class 'NoneType'&gt;\n# list_with_none = [1, 2, None, 4] # None is kept\n# print(f\"List with None: {list_with_none}\") # [1, 2, None, 4]\n# print(f\"Length of list_with_none: {len(list_with_none)}\") # 4\n\n# NaN (Not a Number)\nnan_val = float('nan') # Or math.nan, np.nan\nprint(f\"Is nan_val NaN? {math.isnan(nan_val)}\") # True\nprint(f\"Is nan_val == nan_val? {nan_val == nan_val}\") # False! NaN compares unequal to everything, including itself.\nprint(f\"Is nan_val None? {nan_val is None}\") # False\n\n# numpy array with NaN\narr_with_nan = np.array([1.0, 2.0, np.nan, 4.0])\nprint(f\"Array with NaN: {arr_with_nan}\") # [ 1.  2. nan  4.]\nprint(f\"Is NaN in array? {np.isnan(arr_with_nan)}\") # [False False  True False]\nprint(f\"Sum of array: {np.sum(arr_with_nan)}\") # nan\nprint(f\"Sum excluding NaN: {np.nansum(arr_with_nan)}\") # 7.0\n\n# Pandas Series/DataFrame (handles missing data gracefully)\ns_mixed = pd.Series([1, 2, None, np.nan, \"hello\", None])\nprint(\"\\nPandas Series with mixed missing types:\")\nprint(s_mixed)\nprint(\"\\nChecking for missing with isna():\")\nprint(s_mixed.isna()) # True for None and np.nan\n# print(s_mixed.isnull()) # Alias for isna()\n\ns_numeric = pd.Series([1.0, 2.0, np.nan, 4.0])\nprint(f\"\\nSum of numeric series: {s_numeric.sum()}\") # 7.0 (default skipna=True)\nprint(f\"Sum including NA: {s_numeric.sum(skipna=False)}\") # nan\n\n# Distinguishing None and NaN\nprint(f\"\\nIs the first NaN None? {arr_with_nan[2] is None}\") # False\nprint(f\"Is the first None in Series None? {s_mixed[2] is None}\") # True\n```\n\n\n\n\nComments, Prints, & Logs\nComment syntax is surprisingly functinaly identical across the languages.\n\n\n\n\nR\n\nis_verbose &lt;- TRUE\n# In line comment\n\n#///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n# ----- User inputs --------------------------------------------------------------------------------------------\n#///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\nif(is_verbose) { \n  message(glue::glue(\"Marco\")) \n}\n\nMarco\n\n\n\n\n\nPython\n\nis_verbose = True\n# In line comment\n\n#///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n# ----- User inputs --------------------------------------------------------------------------------------------\n#///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\nif is_verbose:\n    print(\"Polo\")\n\nPolo\n\n\n\n\n\n\n\n\n\nFile paths\nSee also: Creating and deleting dirs\n\nRPython\n\n\n\nbad_path &lt;- file.path(\"tmp_path\",\"tempfile\",fsep =.Platform$file.sep)\n\nfile.exists(bad_path)\n\n[1] FALSE\n\nfile.exists(file.path(\"fileio.qmd\"))\n\n[1] TRUE\n\n\n\n\n\nimport os\nbad_path = os.path.join(\"tmp_path\",\"tempfile\")\n\nos.path.exists(bad_path)\n\nFalse\n\nos.path.exists(os.path.join(\"fileio.qmd\"))\n\nTrue\n\n\n\n\n\n\n\nChaining commands\n\nBash and powershell chaning and multilineR chains and multilines\n\n\n```{bash}\n{command 1} {inputs} && \\ \n{command 2}\n```\n```{bash}\n{command 1} {inputs} && {command 2}\n```\n\n\n\n i &lt;- 2; message(i); i &lt;- 6; message(i)\n\n2\n\n\n6\n\n\n\n\n\n\n\nTime & Timing\n\nSee: https://www.epochconverter.com/\n\n\nFrom epochTo epochTimezonesWall timeTimestamp\n\n\n\nRPython\n\n\n\nunix_timestamp = 1598403600\nhuman_readable_date &lt;- as.POSIXct(unix_timestamp, origin=\"1970-01-01\")\nmessage(human_readable_date)\n\n2020-08-26 01:00:00\n\n\n\n\n\nimport datetime\n\nunix_timestamp = 1598403600\nhuman_readable_date = datetime.datetime.fromtimestamp(unix_timestamp, datetime.UTC)\nprint(human_readable_date)\n\n2020-08-26 01:00:00+00:00\n\n\n\n\n\n\n\n\nRPython\n\n\n\ndata &lt;- \"2021/05/25 12:34:25\"\n\nmy_time &lt;- lubridate::as_datetime(data)\nlubridate::seconds(my_time)\n\n[1] \"1621946065S\"\n\n\n\n\n\n# https://note.nkmk.me/en/python-unix-time-datetime/\n\n\n\n\n\n\n\nRPython\n\n\n```{r}\n#todo\n```\n\n\n```{python}\n#so many todo\n```\n\n\n\n\n\n\nRPythonBash\n\n\n\nis_verbose = TRUE\n\nfn_time_start &lt;- Sys.time()\nSys.sleep(5)\nfn_time_end &lt;- Sys.time()\n\nruntime &lt;- Sys.time() - fn_time_start\n\nif(is_verbose) {\n  units(runtime) &lt;- \"secs\"\n  print(paste(\"Total Compute Time: \",round(runtime, digits = 3),\"seconds\"))\n  units(runtime) &lt;- \"hours\"\n  print(paste(\"Total Compute Time: \",round(runtime, digits = 3),\"hours\"))\n}\n\n[1] \"Total Compute Time:  5.006 seconds\"\n[1] \"Total Compute Time:  0.001 hours\"\n\n\n\n\n\nimport time\n\nis_verbose = True\n\nfn_time_start = time.time()\ntime.sleep(5)\nfn_time_end = time.time()\n\nruntime = fn_time_end - fn_time_start\n\nif is_verbose:\n    print(f\"Total Compute Time: {round(runtime, 3)} seconds\")\n    runtime_hours = runtime / (60 * 60) \n    print(f\"Total Compute Time: {round(runtime_hours, 3)} hours\")\n\nTotal Compute Time: 6.456 seconds\nTotal Compute Time: 0.002 hours\n\n\n\n\n```{bash}\nnow=$(date +%s.%N)\nruntime=$(python -c \"print(${now} - ${last})\")\nlast=$(date +%s.%N)\necho \"Through HEAL HAND:$runtime\"\n```\n\n\n\n\n\n\nRPythonhtml\n\n\n\nmessage(Sys.time())\n\n2025-08-25 21:02:16.010528\n\nmessage(format(Sys.time(), \"%m/%d/%Y %H:%M:%S\"))\n\n08/25/2025 21:02:16\n\n\n\n\n\nimport datetime\n\nprint(datetime.datetime.now())\n\n2025-08-25 21:02:16.038726\n\nprint(datetime.datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\"))\n\n08/25/2025 21:02:16\n\n\n\n\n```{html}\n&lt;div class=\"t-foot\"&gt;\n  &lt;p id=\"date\"&gt;&lt;/p&gt;&lt;script&gt;const currentDate = new Date();const options = { year: 'numeric', month: 'long', day: 'numeric' };const formattedDate = currentDate.toLocaleDateString(undefined, options);document.getElementById(\"date\").innerHTML = `Last updated: ${formattedDate}&lt;br&gt;(A Tentative WIP)`;&lt;/script&gt;&lt;/div&gt;\n```\n\n\n\n\n\n\n\n\n\n\n\nAOIs\n\nR (with AOI package)R (From maps)R (QGIS, PDAL pipeline)R (Google Maps, functionalized)Python\n\n\nUsing Mike Johnosn’s AOI:\n\n\n\nmy_map_window &lt;- AOI::aoi_get(state = \"TX\", county = \"Harris\")\nmapview::mapview(my_map_window)\n\n\n\n\n\n\n\nmy_map_window &lt;- AOI::geocode(\"Tampa\" ,bbox = TRUE) |&gt; sf::st_buffer(dist = 70000)\nmapview::mapview(my_map_window)\n\n\n\n\n\n\n\n\n\n\nMost of the time it’s best to use Mike Johnosn’s AOI, but if you want to do it the hard way…\n\nFrom Google:\n\n# From https://www.google.com/maps/@40.5482907,-105.1219692,15z\n# google_aoi = c(value_1,value_2,value_3)\ngoogle_aoi &lt;- c(40.5482907,-105.1219692,15)\n\ndistance_to_buffer = (156543.03392 * cos(google_aoi[1] * pi / 180) / 2^(google_aoi[3]-2)) * 1000\n# distance_to_buffer = scale_bar_distance * scale_bar_multiple\n\nmy_map_window &lt;- sf::st_bbox(sf::st_buffer(sf::st_sfc(sf::st_point(c(google_aoi[2],google_aoi[1])),crs = sf::st_crs('EPSG: 4326')),units::as_units(distance_to_buffer,'meter'))) |&gt; sf::st_as_sfc() |&gt; sf::st_as_sf()\n\nmapview::mapview(sf::st_sfc(sf::st_point(c(google_aoi[2],google_aoi[1])),crs = sf::st_crs('EPSG: 4326'))) + \n  mapview::mapview(my_map_window) \n\n\n\n\n\nor, from mapview\n\n# From mapview::mapview() (alt tab out of window to perserve AOI)\n# mapview_aoi = c(value_1,value_2,value_3)\nmapview_aoi &lt;- c(-105.1219692, 40.5482907,15)\n\ndistance_to_buffer = (156543.03392 * cos(mapview_aoi[2] * pi / 180) / 2^(mapview_aoi[3]-2)) * 1000\n# distance_to_buffer = scale_bar_distance * scale_bar_multiple\n\nmy_map_window &lt;- sf::st_bbox(sf::st_buffer(sf::st_sfc(sf::st_point(c(mapview_aoi[1],mapview_aoi[2])),crs = sf::st_crs('EPSG: 4326')),units::as_units(distance_to_buffer,'meter'))) |&gt; sf::st_as_sfc() |&gt; sf::st_as_sf()\n\nmapview::mapview(sf::st_sfc(sf::st_point(c(mapview_aoi[1],mapview_aoi[2])),crs = sf::st_crs('EPSG: 4326'))) + \n  mapview::mapview(my_map_window) \n\n\n\n\n\n\n\n\nMouse over corners to get coordinates and then replace and run…\n\n\nmy_aoi &lt;- sf::st_sfc(\n  c(\n    sf::st_point(c(-87724,756748)),\n    sf::st_point(c(-81237,761721))),\n  crs = sf::st_crs('EPSG: 5070')) |&gt; \n  sf::st_bbox() |&gt;\n  sf::st_as_sfc() |&gt;\n  sf::st_transform(sf::st_crs('EPSG: 3857'))\nmapview::mapview(my_aoi)\n\n\n\n\nglue::glue(\"([{sf::st_bbox(my_aoi)$xmin}, {sf::st_bbox(my_aoi)$xmax}], [{sf::st_bbox(my_aoi)$ymin}, {sf::st_bbox(my_aoi)$ymax}])\")\n\n([-10787902.2895066, -10780366.0461397], [3488445.06405308, 3494270.06004247])\n\n\n\n\n\nMost of the time it’s best to use Mike Johnosn’s AOI, but sometimes it’s nice to be able to take a quick guesstimate of map frame off Google Maps\n\n\n#' @title Create AOI from Google maps\n#' @description Create AOI from Google Maps URL\n#' @param lat The Latitude (second value) in the URL\n#' @param lon The Longitude (first value) in the URL\n#' @param zoom The z (third value) in the URL, Default: NULL\n#' @param scale_bar_distance The distance on the scale bar in meters, Default: NULL\n#' @param scale_bar_multiple The number of times you want the resulting frame to be based on the scale bar, Default: 15\n#' @return An SF bounding box rectangle\n#' @details Creates a bounding box recatngle based on a the frame of a Google Maps URL, handy when you want a quick frame based on the accessable tools.  Provide only one of (zoom) or (scale_bar_distance and scale_bar_multiple)\n#' @examples\n#' \\dontrun{\n#' if(interactive()){\n#'  #EXAMPLE1\n#'  # From https://www.google.com/maps/@33.2239037,-83.542512,8.3z\n#'  google_aoi &lt;- c(33.2239037,-83.542512,8.3)\n#'  my_map_window &lt;- LonLatZoom_to_bbox(google_aoi[2],google_aoi[1],zoom=google_aoi[3])\n#'  mapview::mapview(my_map_window)\n#'\n#'  my_map_window &lt;- LonLatZoom_to_bbox(google_aoi[2],google_aoi[1],scale_bar_distance = 20000, scale_bar_multiple = 15)\n#'  mapview::mapview(my_map_window)\n#'  }\n#' }\n#' @seealso\n#'  \\code{\\link[sf]{st_bbox}}, \\code{\\link[sf]{geos_unary}}, \\code{\\link[sf]{sfc}}, \\code{\\link[sf]{st}}, \\code{\\link[sf]{st_crs}}, \\code{\\link[sf]{st_as_sf}}, \\code{\\link[sf]{st_as_sfc}}\n#'  \\code{\\link[units]{units}}\n#' @rdname util_aoi_from_zoom\n#' @export\n#' @importFrom sf st_bbox st_buffer st_sfc st_point st_crs st_as_sf st_as_sfc\n#' @importFrom units as_units\nLonLatZoom_to_bbox &lt;- function(lat, lon, zoom = NULL, scale_bar_distance = NULL, scale_bar_multiple = 15) {\n  # sinew::moga(file.path(getwd(),\"R/util_aoi_from_zoom.R\"),overwrite = TRUE)\n  # devtools::document()\n  # pkgdown::build_site(new_process=TRUE)\n  # devtools::load_all()\n  #\n  # lat = -84.0784306\n  # lon = 33.034732\n  # zoom = 6.9\n\n  if(!is.null(zoom)) {\n    metersPerPx = 156543.03392 * cos(lat * pi / 180) / 2^(zoom-2)\n    distance_to_buffer = metersPerPx * 1000\n    # distance_to_buffer = 156609*exp(-0.693*6-zoom)\n  } else {\n    distance_to_buffer = scale_bar_distance * scale_bar_multiple\n  }\n\n\n  # mapview::mapview(sf::st_sfc(sf::st_point(c(lat,lon)),crs = sf::st_crs('EPSG: 4326')))\n  my_map_window &lt;- sf::st_bbox(sf::st_buffer(sf::st_sfc(sf::st_point(c(lat,lon)),crs = sf::st_crs('EPSG: 4326')),units::as_units(distance_to_buffer,'meter')))\n  return(sf::st_as_sf(sf::st_as_sfc(my_map_window)))\n}\n\ngoogle_aoi &lt;- c(33.2239037,-83.542512,8.3)\nmy_map_window &lt;- LonLatZoom_to_bbox(google_aoi[2],google_aoi[1],zoom=google_aoi[3])\nmapview::mapview(my_map_window)\n\n\n\n\n\n\n\n```{python}\nfrom shapely.geometry import box\n\nbbox = box(-112, 34, -105, 39)\nbbox = gpd.GeoDataFrame(geometry=[bbox], crs ='EPSG:4326')\n```\n\n\n\n\n\nPackaging and Production\n\nPythonR PackagesHTA\n\n\n\nFunctionsPython GUI\n\n\n```{python}\nimport argparse\nimport geopandas\nimport contextily\nimport os\nimport sys\nimport time\nimport datetime\n\ndef main(is_verbose):\n    \"\"\"\n    Subset dam lines from hydrofabric\n    Args:\n        data_path (str): The path to hydrofabric data.\n        hf_id (str): A hydrofabric flowpath id to use as the headwater.\n        is_verbose (bool): Whether to print verbose output.\n        \n    Example usage:\n    python /hydrofabric3D/Python/subset_dam_lines.py \\\n        -data_path \"/media/sf_G_DRIVE/data/\" \\\n        -hf_id 'wb-2414833' \\\n        -v\n\n    flowpaths, transects, xyz = subset\\_dam\\_lines(data\\_path = \"/media/sf\\_G\\_DRIVE/data/\", hf\\_id = 'wb-2414833', is\\_verbose = True)\n\n    \"\"\"\n    start_runtime = time.time()\n\n    if is_verbose:\n        end_runtime = time.time()\n        time_passed = (end_runtime - start_runtime) // 1\n        time_passed = datetime.timedelta(seconds=time_passed)\n        print('Total Compute Time: ' + str(time_passed))\n\n    return True\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Subset dam break lines.\")\n    parser.add_argument(\"data_path\", help=\"The path to hydrofabric data.\")\n    parser.add_argument(\"hf_id\", type=str, help=\"A hydrofabric flowpath id to use as the headwater.\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Enable verbose output.\")\n\n    args = parser.parse_args()\n\n    processed_result = subset_dam_lines(args.data_path, args.hf_id, args.verbose)\n```\n\n\n\nMake a GUI with https://www.pysimplegui.org/en/latest/\nMake the desktop icon to click: https://gist.github.com/nathakits/7efb09812902b533999bda6793c5e872\n\n```{md}\n[Desktop Entry]\nName=Pupil Capture\nVersion=v0.8.5\nIcon=pupil-capture\nX-Icon-Path=/path/to/icon/file/\nExec=python /path/to/py/file/main.py\nTerminal=false\nType=Application\n```\n\nand then chmod u+x /media/sf_G_DRIVE/Dropbox/root/tools/summerizeR/summerizeR/summerizeR.py\n\n\n\n\n\n\n\nFunctionsPackages\n\n\n```{r}\nfunfunc &lt;- function(path_to_inputs,path_to_outputs,overwrite=FALSE,is_verbose=FALSE) {\n  # sinew::moga(file.path(getwd(),\"R/funfunc.R\"),overwrite = TRUE)\n  # devtools::document()\n  # pkgdown::build_site(new_process=TRUE)\n  #\n  # devtools::load_all()\n  \n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  # ----- User inputs --------------------------------------------------------------------------------------------\n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  fn_time_start &lt;- Sys.time()\n  \n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  # ----- Step function ------------------------------------------------------------------------------------------\n  # More verbose description as needed\n  # Can be multiple lines\n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  if(is_verbose) {\n    runtime &lt;- Sys.time() - fn_time_start\n    units(runtime) &lt;- \"hours\"\n    print(paste(\"Total Compute Time: \",round(runtime, digits = 3),\"hours\"))\n  }\n  \n  return(TRUE)\n}\n```\n\n\n\nsee https://r-pkgs.org/whole-game.html#write-the-first-function, https://yonicd.github.io/sinew/articles/motivation.html, https://github.com/jthomasmock/pkg-building, and https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/\n\nI have no intention of taking a deep dive into this (docs | cheatshet) but in general you should:\n\nMake a new package in a new folder, separate functions out.\nBuild your README.rmd using this template:\n\n```{md}\n---\noutput: github_document\n---\n\n&lt;!-- README.md is generated from README.Rmd. Please edit that file --&gt;\n\n#`#`#`{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  eval = FALSE,\n  comment = \"#&gt;\",\n  fig.path = \"man/figures/README-\",\n  out.width = \"100%\"\n)\n#`#`#`  &lt;!-- remove the # --&gt;\n\nmarkdown here...\n```\n\nAttribute with: withr::with_dir(getwd(), usethis::use_mit_license(name = \"Cornholio\"))\nMake HTML with: usethis::use_pkgdown()\nAppend missing namespace with: sinew::pretty_namespace(getwd(),overwrite = TRUE)\nMake first cut headers with:\n\n```{r}\nusethis::use_pkgdown()\nusethis::use_pkgdown_github_pages()\nsinew::sinew_opts$set(markdown_links = TRUE)\nsinew::makeOxyFile(input = getwd(), overwrite = TRUE, verbose = FALSE)\n```\n\nYou may need to manually add imports such as these like so:\n\n```{r}\n#' @import magrittr\n#' @import data.table\n#' @importFrom foreach `%do%`\n#' @importFrom foreach `%dopar%`\n\nmarco &lt;- function(in_value=TRUE) {\n  # sinew::moga(file.path(getwd(),\"R/hello.R\"),overwrite = TRUE)\n  # devtools::document()\n  # pkgdown::build_site(new_process=TRUE)\n  # devtools::load_all()\n  # \n  # marco(in_value=TRUE)\n  \n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  # -- start -----------------------------------------------------------------------------------------------------\n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  \n  print(\" ⚠ WARNING WIZARD ⚠  \")\n  print(\"                     \")\n  print(\"    ⚠              \")\n  print(\" (∩｀-´)⊃━☆ﾟ.*･｡ﾟ  \" )\n\n  return(TRUE)\n}\n```\n\nAs we iterate we:\n\nTest functions with: devtools::load_all()\nMake new headers with: sinew::moga(file.path(getwd(),\"R/hello.R\"),overwrite = TRUE) and copy output into the file.\nRecreate .Rd files with: devtools::document()\nDelete the markdown version of the readme and run pkgdown::build_site(new_process = TRUE) or press the “knit” button.\n\nremove the docs line from .gitignore and push. Publish that as the github pages. If you are pushing to a newly created empty repo that will look something like:\n\n```{bash}\ngit init\ngit remote add origin https://github.com/JimColl/RRASSLER.git\ngit commit -m \"first commit\"\ngit branch --move master main\ngit push --set-upstream origin main\n```\n\n\n\n\n\nSee FOSSFlood\n\n\n\n\n\nSend an email\n\nProbably don’t do this, it’s questionable from a security standpoint…\n\n```{r}\n# gmailr::gm_auth_configure(path = \"./wreak_my_day.json\")\n# gmailr::gm_auth(email = TRUE, cache=\".secret\")\nbad_email &lt;-\n  gmailr::gm_mime() %&gt;%\n  gmailr::gm_to(\"me@gmail.com\") %&gt;%\n  gmailr::gm_from(\"me@gmail.com\") %&gt;%\n  gmailr::gm_subject(\"RRASSLER's Broken\") %&gt;%\n  gmailr::gm_text_body(\"You suck at coding\")\ntryCatch( { }\n          , error = function(e) { gmailr::gm_send_message(bad_email) })\n```\n\n\nGit\nThere are few things in life that have the ability to immediately anger me, but git is one of them1. I’m all for version control and transparency, but not hidden at the file level and not with this much friction (and the inner perfectionist in me dislikes how exposed a product can be prior to polishing, wrong documentation &lt; bad documentation and not everything needs to happen in a garage). git push --force and github .dev are the sledgehammers I most often reach for, but when it works… Here are a list of commands I hate having to Google every time I have to open a git terminal:\n\nList all branches: git branch -a\nWhat Branch am I on?: git rev-parse --abbrev-ref HEAD\nSwitch branches: git checkout &lt;other_branch&gt;\nCreate and switch: git checkout -b &lt;new_branch&gt;\n\n\nMove existing codebase to newly created git repo:\n```{bash}\ngit remote add origin https://github.com/JimColl/RRASSLER.git\ngit commit -m \"first commit\"\ngit branch --move master main\ngit push --set-upstream origin main\n```\nor\n```{bash}\n&gt; From the hard drive: `git init`\n&gt; add a '.gitignore'\ngit add .gitignore\ngit add .\ngit remote add origin {remote git URL}\ngit push\n```\n\n\nMove Repo B into Repo A:\n```{bash}\ngit remote add origin https://github.com/JimColl/RRASSLER.git\ngit commit -m \"first commit\"\ngit branch --move master main\ngit push --set-upstream origin main\n```\n\n\nMerge & Replace dev with branch without the use of rebase or force:\n```{bash}\ngit clone https://github.com/JimColl/RRASSLER.git\ncd ./RRASSLER/\ngit checkout good_branch\ngit merge --strategy=ours dev\n## In VIM window\n1) Enter \"insert mode\": Type the letter i. You should see -- INSERT -- appear at the bottom of the editor. Now you can type and edit the merge commit message if you want.  The default message is usually fine.\n2) Exit \"insert mode\" and enter \"command mode\": Press the Esc (Escape) key. The -- INSERT -- indicator at the bottom will disappear.\n3) Save and quit: Type :wq (that's a colon, then 'w' for write, and 'q' for quit) and press Enter.\ngit add .\ngit push\n```\n\n\nAre you pushing keys?\n\nBashPowershellwords you shouldn’t push\n\n\ngrep -Rn \"AWS_ACCESS_KEY\" /path/to/repo\n\n\nsls \"AWS_ACCESS_KEY\" ./*\n\n\n\nghp_\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#r",
    "href": "fileio.html#r",
    "title": "FILE I/O",
    "section": "R",
    "text": "R\n\nmy_vector &lt;- c(10, 20, 30, 40, 50)\n# Accessing the first element\nfirst_element &lt;- my_vector[1] \nprint(first_element)\n\n[1] 10\n\nlast_element = my_vector[length(my_vector)] \nprint(last_element)\n\n[1] 50",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#python",
    "href": "fileio.html#python",
    "title": "FILE I/O",
    "section": "Python",
    "text": "Python\n\nmy_list = [10, 20, 30, 40, 50]\n# Accessing the first element\nfirst_element = my_list[0] \nprint(first_element)\n\n10\n\nlast_element = my_list[-1] \nprint(last_element)\n\n50",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#r-1",
    "href": "fileio.html#r-1",
    "title": "FILE I/O",
    "section": "R",
    "text": "R\n```{r}\nutils::remove.packages(\"package\")\n\ninstall.packages(\"package\")\nremotes.install_github(\"NOAA-OWP/RRASSLER@branch_name\")\n\nlibrary(\"package_name\") \n```\nIf you get an error along the lines of cannot open URL 'https://api.github.com/repos/&lt;repo_here&gt;/tarball/HEAD, you can try the tips I outline here.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#python-1",
    "href": "fileio.html#python-1",
    "title": "FILE I/O",
    "section": "Python",
    "text": "Python\n```{python}\n# With conda\nconda remove package_name\nconda install package_name\n\n# Or with pip\npip uninstall package_name\npip install package_name\npip install package_name=='#.##'\n\n&gt;&gt;&gt; import package_name \n```",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#r-2",
    "href": "fileio.html#r-2",
    "title": "FILE I/O",
    "section": "R",
    "text": "R\n\n# Define variables with different data types\nname &lt;- \"Bob\"  # String\nage &lt;- 25  # Integer\nheight &lt;- 3.14159  # Double (float)\nis_student &lt;- TRUE  # Boolean\n\n# Use the class() function to check data types\nclass(name)  # Character\n\n[1] \"character\"\n\nclass(age)   # Integer\n\n[1] \"numeric\"\n\nclass(height) # Numeric\n\n[1] \"numeric\"\n\nclass(is_student) # Logical\n\n[1] \"logical\"\n\n# Alternatively, use typeof() for more detailed info\ntypeof(name)  # character\n\n[1] \"character\"\n\ntypeof(age)   # integer\n\n[1] \"double\"\n\ntypeof(height) # double\n\n[1] \"double\"\n\ntypeof(is_student) # logical\n\n[1] \"logical\"\n\n# if (inherit(X, \"Spatial\"))\n#   X &lt;- sf::st_as_sf(X)\n# \n# if (inherit(Y, \"Raster\"))\n#   Y &lt;- stars::st_as_stars(Y)\n# \n# if (inherit(Y, \"SpatRaster\"))\n#   Y &lt;- stars::st_as_stars(Y) # that one does not exist do it yourself\n# \n# if(!is(X, \"sf\"))\n#    stop(\"Y has to be a spatial object \")\n# \n# if(!is(Y, \"stars\")\n#    stop(\"Y has to be a raster object\")\n# \n# geom = sf::st_geometry(X)\n# \n# if (!is(geom, \"sfc_POLYGON\") & !is(geom, \"sfc_MULTIPOLYGON\"))\n#    stop(\"Y has to be a spatial polygon object \")",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#python-2",
    "href": "fileio.html#python-2",
    "title": "FILE I/O",
    "section": "Python",
    "text": "Python\n\n# Define variables with different data types\nname = \"Bob\"  # String\nage = 25  # Integer\nheight = 3.14159  # Float\nis_student = True  # Boolean\n\n# Use the type() function to check data types\nprint(f\"Name (string): {type(name)}\")\n\nName (string): &lt;class 'str'&gt;\n\nprint(f\"Age (integer): {type(age)}\")\n\nAge (integer): &lt;class 'int'&gt;\n\nprint(f\"Pi (float): {type(height)}\")\n\nPi (float): &lt;class 'float'&gt;\n\nprint(f\"Is Adult (boolean): {type(is_student)}\")\n\nIs Adult (boolean): &lt;class 'bool'&gt;",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#r-3",
    "href": "fileio.html#r-3",
    "title": "FILE I/O",
    "section": "R",
    "text": "R\n\n# Explicit logical values\nif (TRUE) { message(\"TRUE is true\") }\n\nTRUE is true\n\nif (!FALSE) { message(\"FALSE is false\") }\n\nFALSE is false\n\n# Numeric values in `if` - Generally avoid this\nif (1) { message(\"1 is true\") } # This works but is bad practice\n\n1 is true\n\nif (0) { message(\"0 is true\") } # This block is NOT executed\nif (-1) { message(\"-1 is true\") } # This works but is bad practice\n\n-1 is true\n\n# Using logical vectors (element-wise)\nlogical_vec &lt;- c(TRUE, FALSE, TRUE)\nnumeric_vec &lt;- c(1, 0, -5)\n\nprint(logical_vec & c(TRUE, TRUE, FALSE)) # Element-wise AND -&gt; TRUE FALSE FALSE\n\n[1]  TRUE FALSE FALSE\n\nprint(logical_vec | c(FALSE, FALSE, FALSE)) # Element-wise OR -&gt; TRUE FALSE TRUE\n\n[1]  TRUE FALSE  TRUE\n\nprint(!logical_vec) # Element-wise NOT -&gt; FALSE TRUE FALSE\n\n[1] FALSE  TRUE FALSE\n\n# Be careful with NA\nval &lt;- NA\n# if (val) { message(\"NA is true?\") } # ERROR: argument is of length zero\n# if (!val) { message(\"NA is false?\") } # ERROR: argument is of length zero\nprint(is.na(val)) # TRUE\n\n[1] TRUE\n\n# Use identical() for exact comparison\nprint(identical(TRUE, TRUE)) # TRUE\n\n[1] TRUE\n\nprint(identical(1, TRUE)) # FALSE - Types differ\n\n[1] FALSE\n\n# Testing values that might represent boolean concepts\nvalues &lt;- c(TRUE, FALSE, TRUE, FALSE) # Logical\nprint(values == TRUE)  # TRUE FALSE TRUE FALSE\n\n[1]  TRUE FALSE  TRUE FALSE\n\nprint(values == FALSE) # FALSE TRUE FALSE TRUE\n\n[1] FALSE  TRUE FALSE  TRUE\n\nstrings &lt;- c(\"True\", \"False\", \"TRUE\", \"FALSE\", NA) # Character\nprint(strings == \"True\") # TRUE FALSE FALSE FALSE NA (Case-sensitive)\n\n[1]  TRUE FALSE FALSE FALSE    NA\n\nprint(tolower(strings) == \"true\") # TRUE FALSE TRUE FALSE NA (Case-insensitive)\n\n[1]  TRUE FALSE  TRUE FALSE    NA\n\nnumbers &lt;- c(1, 0, 1, 0, NA) # Numeric\nprint(numbers == 1) # TRUE FALSE TRUE FALSE NA\n\n[1]  TRUE FALSE  TRUE FALSE    NA\n\nprint(numbers == 0) # FALSE TRUE FALSE TRUE NA\n\n[1] FALSE  TRUE FALSE  TRUE    NA\n\n# Check for NA specifically\nprint(is.na(strings)) # FALSE FALSE FALSE FALSE TRUE\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nprint(is.na(numbers)) # FALSE FALSE FALSE FALSE TRUE\n\n[1] FALSE FALSE FALSE FALSE  TRUE",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#python-3",
    "href": "fileio.html#python-3",
    "title": "FILE I/O",
    "section": "Python",
    "text": "Python\n\n#import pandas as pd\n#import numpy as np\n\n# Basic truthiness\nif True: print(\"True is truthy\")\n\nTrue is truthy\n\nif \"hello\": print(\"'hello' is truthy\")\n\n'hello' is truthy\n\nif [1, 2]: print(\"[1, 2] is truthy\")\n\n[1, 2] is truthy\n\nif {\"a\": 1}: print(\"{'a': 1} is truthy\")\n\n{'a': 1} is truthy\n\nif 1: print(\"1 is truthy\")\n\n1 is truthy\n\nif -1: print(\"-1 is truthy\")\n\n-1 is truthy\n\nif not False: print(\"False is falsy\")\n\nFalse is falsy\n\nif not None: print(\"None is falsy\")\n\nNone is falsy\n\nif not \"\": print(\"'' is falsy\")\n\n'' is falsy\n\nif not []: print(\"[] is falsy\")\n\n[] is falsy\n\nif not {}: print(\"{} is falsy\")\n\n{} is falsy\n\nif not 0: print(\"0 is falsy\")\n\n0 is falsy\n\nif not 0.0: print(\"0.0 is falsy\")\n\n0.0 is falsy\n\n\n# Pandas/Numpy context (similar to your example)\n# data = {'col_bool': [True, False, True, False, None], \n#         'col_str': ['True', 'False', 'TRUE', 'FALSE', None],\n#         'col_num': [1, 0, 1, 0, np.nan],\n#         'col_nan': [np.nan, False, True, np.nan, None]}\n# df = pd.DataFrame(data)\n# \n# # Testing boolean type values (handling potential None)\n# print(\"\\nTesting boolean type values:\")\n# print(df['col_bool'] == True)   # Compares element-wise, None -&gt; False\n# print(df['col_bool'].eq(True))  # Another way, same result\n# print(df['col_bool'].astype(bool)) # Converts None -&gt; False (often not desired!)\n# print(df['col_bool'].fillna(False).astype(bool)) # Explicit NA handling\n# \n# # Testing string type values (case-sensitive, None becomes NaN -&gt; False)\n# print(\"\\nTesting string type values (case-sensitive):\")\n# print(df['col_str'] == 'True') \n# print(df['col_str'].str.lower() == 'true') # Handles case, NA -&gt; False\n# \n# # Testing numeric type values (NaN -&gt; False)\n# print(\"\\nTesting numeric type values:\")\n# print(df['col_num'] == 1) \n# print(df['col_num'] == 0) \n# \n# # Test for NaN/None values specifically\n# print(\"\\nTesting for NaN/None values:\")\n# print(df['col_nan'].isna()) # Correct way to check for NaN or None in Pandas\n# print(df['col_nan'].isnull()) # Alias for isna()",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#r-4",
    "href": "fileio.html#r-4",
    "title": "FILE I/O",
    "section": "R",
    "text": "R\n\n# NULL\nnull_obj &lt;- NULL\nmessage(\"Is null_obj NULL? \", is.null(null_obj)) # TRUE\n\nIs null_obj NULL? TRUE\n\nmessage(\"Length of null_obj: \", length(null_obj)) # 0\n\nLength of null_obj: 0\n\nvec_with_null &lt;- c(1, 2, NULL, 4) # NULL is dropped!\nmessage(\"Vector with NULL: \", paste(vec_with_null, collapse=\", \")) # 1, 2, 4\n\nVector with NULL: 1, 2, 4\n\n# NA (Missing Value)\nna_val &lt;- NA\nmessage(\"Is na_val NA? \", is.na(na_val)) # TRUE\n\nIs na_val NA? TRUE\n\nvec_with_na &lt;- c(1, 2, NA, 4)\nmessage(\"Vector with NA: \", paste(vec_with_na, collapse=\", \")) # 1, 2, NA, 4\n\nVector with NA: 1, 2, NA, 4\n\nmessage(\"Length of vec_with_na: \", length(vec_with_na)) # 4\n\nLength of vec_with_na: 4\n\nmessage(\"Sum of vec_with_na: \", sum(vec_with_na)) # NA\n\nSum of vec_with_na: NA\n\nmessage(\"Sum excluding NA: \", sum(vec_with_na, na.rm = TRUE)) # 7\n\nSum excluding NA: 7\n\n# NaN (Not a Number)\nnan_val &lt;- 0/0\nmessage(\"Is nan_val NaN? \", is.nan(nan_val)) # TRUE\n\nIs nan_val NaN? TRUE\n\nmessage(\"Is nan_val NA? \", is.na(nan_val))   # TRUE (NaN is a type of NA)\n\nIs nan_val NA? TRUE\n\nvec_with_nan &lt;- c(1, 2, NaN, 4)\nmessage(\"Vector with NaN: \", paste(vec_with_nan, collapse=\", \")) # 1, 2, NaN, 4\n\nVector with NaN: 1, 2, NaN, 4\n\nmessage(\"Sum of vec_with_nan: \", sum(vec_with_nan)) # NaN\n\nSum of vec_with_nan: NaN\n\nmessage(\"Sum excluding NaN/NA: \", sum(vec_with_nan, na.rm = TRUE)) # 7\n\nSum excluding NaN/NA: 7\n\n# Distinguishing NA and NULL in lists\nmy_list &lt;- list(a = 1, b = NULL, c = NA, d = \"hello\")\nprint(my_list)\n\n$a\n[1] 1\n\n$b\nNULL\n\n$c\n[1] NA\n\n$d\n[1] \"hello\"\n\nmessage(\"Is element 'b' NULL? \", is.null(my_list$b)) # TRUE\n\nIs element 'b' NULL? TRUE\n\nmessage(\"Is element 'c' NA? \", is.na(my_list$c))     # TRUE\n\nIs element 'c' NA? TRUE",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#python-4",
    "href": "fileio.html#python-4",
    "title": "FILE I/O",
    "section": "Python",
    "text": "Python\n```{python}\nimport math\nimport numpy as np\nimport pandas as pd\n\n# None\nnone_obj = None\nprint(f\"Is none_obj None? {none_obj is None}\") # True (use 'is' for None)\nprint(f\"Type of None: {type(none_obj)}\") # &lt;class 'NoneType'&gt;\n# list_with_none = [1, 2, None, 4] # None is kept\n# print(f\"List with None: {list_with_none}\") # [1, 2, None, 4]\n# print(f\"Length of list_with_none: {len(list_with_none)}\") # 4\n\n# NaN (Not a Number)\nnan_val = float('nan') # Or math.nan, np.nan\nprint(f\"Is nan_val NaN? {math.isnan(nan_val)}\") # True\nprint(f\"Is nan_val == nan_val? {nan_val == nan_val}\") # False! NaN compares unequal to everything, including itself.\nprint(f\"Is nan_val None? {nan_val is None}\") # False\n\n# numpy array with NaN\narr_with_nan = np.array([1.0, 2.0, np.nan, 4.0])\nprint(f\"Array with NaN: {arr_with_nan}\") # [ 1.  2. nan  4.]\nprint(f\"Is NaN in array? {np.isnan(arr_with_nan)}\") # [False False  True False]\nprint(f\"Sum of array: {np.sum(arr_with_nan)}\") # nan\nprint(f\"Sum excluding NaN: {np.nansum(arr_with_nan)}\") # 7.0\n\n# Pandas Series/DataFrame (handles missing data gracefully)\ns_mixed = pd.Series([1, 2, None, np.nan, \"hello\", None])\nprint(\"\\nPandas Series with mixed missing types:\")\nprint(s_mixed)\nprint(\"\\nChecking for missing with isna():\")\nprint(s_mixed.isna()) # True for None and np.nan\n# print(s_mixed.isnull()) # Alias for isna()\n\ns_numeric = pd.Series([1.0, 2.0, np.nan, 4.0])\nprint(f\"\\nSum of numeric series: {s_numeric.sum()}\") # 7.0 (default skipna=True)\nprint(f\"Sum including NA: {s_numeric.sum(skipna=False)}\") # nan\n\n# Distinguishing None and NaN\nprint(f\"\\nIs the first NaN None? {arr_with_nan[2] is None}\") # False\nprint(f\"Is the first None in Series None? {s_mixed[2] is None}\") # True\n```",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#r-5",
    "href": "fileio.html#r-5",
    "title": "FILE I/O",
    "section": "R",
    "text": "R\n\nis_verbose &lt;- TRUE\n# In line comment\n\n#///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n# ----- User inputs --------------------------------------------------------------------------------------------\n#///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\nif(is_verbose) { \n  message(glue::glue(\"Marco\")) \n}\n\nMarco",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#python-5",
    "href": "fileio.html#python-5",
    "title": "FILE I/O",
    "section": "Python",
    "text": "Python\n\nis_verbose = True\n# In line comment\n\n#///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n# ----- User inputs --------------------------------------------------------------------------------------------\n#///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\nif is_verbose:\n    print(\"Polo\")\n\nPolo",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#file-differencing",
    "href": "fileio.html#file-differencing",
    "title": "FILE I/O",
    "section": "File differencing",
    "text": "File differencing\n\nNotepad++R\n\n\n```{r}\n\n```\n\n\n```{r}\ndiffr::diffr(file.path(tmp_path,fsep=.Platform$file.sep), \n             file.path(tmp_path,fsep=.Platform$file.sep), \n             before = \"f1\", after = \"f2\")\n```",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#directory-manipulation",
    "href": "fileio.html#directory-manipulation",
    "title": "FILE I/O",
    "section": "Directory manipulation",
    "text": "Directory manipulation\n\nPaths\n\nRPython\n\n\n```{r}\nfile.path(tmp_path,fsep=.Platform$file.sep)\n```\n\n\n```{python}\nimport os\nos.path.join()\n```\n\n\n\n\n\nSetting up the cloud\n\nAWSMinio\n\n\n\ncommand linepythonbashR\n\n\n\nInstall AWS CLI\n\n```{bash}\n$ aws configure\nAWS Access Key ID [None]: mykey\nAWS Secret Access Key [None]: mysecretkey\nDefault region name [None]: us-west-2\nDefault output format [None]: json\n```\n\n\n```{python}\necho \"Hello World!\"\n```\n\n\n```{bash}\necho \"Hello World!\"\n```\n\n\n```{r}\nSys.setenv(\"AWS_ACCESS_KEY_ID\" = \"mykey\", \"AWS_SECRET_ACCESS_KEY\" = \"mysecretkey\", \"AWS_DEFAULT_REGION\" = \"us-east-1\")\n```\n\n\n\n\n\n\nlinuxPython\n\n\n```{bash}\naws configure\n```\n\n\n```{python}\necho \"Hello World!\"\n```\n\n\n\n\n\n\n\n\nCreate and delete dirs\n\nLocalCloud (AWS)\n\n\n\nRPython\n\n\n```{r}\ndir.create(rstudio, showWarnings = FALSE, recursive = TRUE)\nunlink(file.path(path_to_ras_dbase,\"models\",\"_unprocessed\",fsep = .Platform$file.sep), recursive = TRUE)\n```\n\n\n```{python}\nimport os\nif not os.path.exists(directory_name):\n    os.mkdir(directory_name)\nif os.path.exists(directory_name):\n    os.rmdir(directory_name)\n```\n\n\n\n\n\n\nRPythoncommand line\n\n\n```{python}\naws.s3::bucket_exists(\"s3://herbariumnsw-pds/\")\n```\n\n\n```{python}\naws.s3::bucket_exists(\"s3://herbariumnsw-pds/\")\n```\n\n\n```{python}\naws\n```\n\n\n\n\n\n\n\n\nFile listing\n\nLocalCloud (AWS)\n\n\n\nRPython\n\n\n```{r}\nlist.files(path_to_ras_dbase, pattern = utils::glob2rx(\"*RRASSLER_metadata.csv$\"), full.names=TRUE, ignore.case=TRUE, recursive=TRUE)\n```\n\n\n```{r}\nlist.files(path_to_ras_dbase, pattern = utils::glob2rx(\"*RRASSLER_metadata.csv$\"), full.names=TRUE, ignore.case=TRUE, recursive=TRUE)\n```\n\n\n\n\n\n\nRPythoncommand line\n\n\n```{r}\nlist.files(path_to_ras_dbase, pattern = utils::glob2rx(\"*RRASSLER_metadata.csv$\"), full.names=TRUE, ignore.case=TRUE, recursive=TRUE)\n```\n\n\n```{r}\nlist.files(path_to_ras_dbase, pattern = utils::glob2rx(\"*RRASSLER_metadata.csv$\"), full.names=TRUE, ignore.case=TRUE, recursive=TRUE)\n```\n\n\n```{r}\nlist.files(path_to_ras_dbase, pattern = utils::glob2rx(\"*RRASSLER_metadata.csv$\"), full.names=TRUE, ignore.case=TRUE, recursive=TRUE)\n```\n\n\n\n\n\n\n\n\nCommon file transfer operations\n\nDisk to Disk transfer\n\nFileFolders\n\n\n\nRPythonPowershell (stock - copy)Powershell (stock - move)\n\n\n```{r}\nfile.copy(file.path(target_filw,fsep = .Platform$file.sep), new_folder)\n```\n\n\n```{python}\nimport shutil\nshutil.copyfile('source.txt', 'destination.txt')\n```\n\n\n```{powershell}\nCopy-Item -Path \"C:\\Source\\File.txt\" -Destination \"D:\\Destination\\File.txt\"\n```\n\n\n```{powershell}\nMove-Item -Path \"C:\\Source\\File.txt\" -Destination \"D:\\Destination\\File.txt\"\n```\n\n\n\n\n\n\nRPythonPowershell (stock - copy)Powershell (sync)\n\n\n```{r}\nfile.copy(file.path(current_folder,list_of_files,fsep = .Platform$file.sep), new_folder)\n```\n\n\n```{python}\nimport shutil\n\nsource_folder = '/path/to/source/folder'\ndestination_folder = '/path/to/destination/folder'\n\nshutil.copytree(source_folder, destination_folder)\n```\n\n\n```{powershell}\nCopy-Item -Path \"C:\\SourceFolder\" -Destination \"D:\\DestinationFolder\" -Recurse\n```\n\n\nFrom https://github.com/BornToBeRoot/PowerShell_SyncFolder\n```{powershell}\npowershell -ExecutionPolicy Bypass -File .\\SyncFolder.ps1 -Source G:\\path\\source_samename -Destination G:\\newpath\\dest_samename -Verbose\n```\n\n\n\n\n\n\nNote\n\n\n\nNote that you may receive an error about running scripts being disabled when activating within PowerShell. If you get this error then execute the following command:\n```{powershell}\nSet-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser\n```\n\n\n\n\n\n\n\n\n\n\nDisk to cloud Transfer\n\nFileFolders\n\n\n\nRPythonAWS CLI\n\n\n```{r}\naws.s3::save_object(\n  object = \".../HUC8.fgb\",\n  bucket = \"s3://.../.../HUC8.fgb\",\n  file = \"&lt;path to your catalog&gt;/HUC8.fgb\"\n)\n```\n\n\n```{python}\ncopy\n```\n\n\n```{bash}\naws s3 cp {SRC} {path}{--dryrun}\n```\n\n\n\n\n\n\nRPythonAWS CLI\n\n\n```{r}\nls_srt_folders_file &lt;- strsplit(to, \"/\")[[1]]\nls_srt_folders_file &lt;- ls_srt_folders_file[ls_srt_folders_file != \"\"]\npath_to_root_bucket &lt;- paste0(\"s3://\", ls_srt_folders_file[2], \"/\")\nrest_of_bucket_prefix &lt;- stringr::str_sub(to, nchar(path_to_root_bucket)+1, nchar(to)-1)\n\nall_rrassled_files &lt;- list.files(from, full.names=TRUE, ignore.case=TRUE, recursive=TRUE)\nall_relevent_rrassled_files &lt;- Filter(function(x) !any(grepl(\"_temp\", x)), all_rrassled_files)\nfor(file_to_copy in all_relevent_rrassled_files) {\n  if (is_verbose) { message(glue::glue(\"Trying to move:{file_to_copy}\")) }\n  aws.s3::put_object(\n    file = file_to_copy,\n    object = glue::glue(\"{rest_of_bucket_prefix}{stringr::str_sub(file_to_copy, nchar(from)+1, nchar(file_to_copy))}\"),\n    bucket = path_to_root_bucket\n  )\n}\n```\n\n\n```{python}\n\n```\n\n\n```{bash}\naws s3 cp --recursive C:/ras2fim_data/ras_models s3://ras2fim/OWP_ras_models --dryrun \n```\n```{bash}\naws s3 sync C:/ras2fim_data/ras_models s3://ras2fim/OWP_ras_models --dryrun \n```\n\n\n\n\n\n\n\n\nCloud to Disk Transfer\n\nFileFolders\n\n\n\nRPythonAWS CLI\n\n\n```{r}\n\n```\n\n\n```{python}\n\n```\n\n\n```{bash}\naws s3 cp {SRC} {path} {--dryrun}\n```\n```{bash}\naws s3 sync {SRC} {path} {--dryrun}\n```\n\n\n\n\n\n\nRPythonAWS CLI\n\n\n```{r}\nbucket_path &lt;- \"s3://ras-models/ras_catalog///models/\"\nlocal_path &lt;- \"C:/Users/scraped_models\"\n\nls_srt_folders_file &lt;- strsplit(bucket_path, \"/\")[[1]]\nls_srt_folders_file &lt;- ls_srt_folders_file[ls_srt_folders_file != \"\"]\npath_to_root_bucket &lt;- paste0(\"s3://\", ls_srt_folders_file[2], \"/\")\nrest_of_bucket_prefix &lt;- stringr::str_sub(bucket_path, nchar(path_to_root_bucket)+1, nchar(bucket_path)-1)\n\nmessage(glue::glue(\"Gathering bucket contents\"))\ndf_bucket_data &lt;- aws.s3::get_bucket(bucket = path_to_root_bucket, prefix = \"ras_catalog//models/\",max = Inf)\nlist_bucket_data &lt;- c()\nfor(i in 1:length(df_bucket_data)) {\n  list_bucket_data &lt;- c(\n    list_bucket_data,\n    df_bucket_data[[i]]$Key)\n}\nlist_bucket_data_dt &lt;- data.table::as.data.table(list_bucket_data)\n\nmessage(glue::glue(\"Moving requisite files\")) \nfor(index in 1:nrow(list_bucket_data_dt)) {\n  file_to_move &lt;- list_bucket_data_dt[index][[1]]\n  aws.s3::save_object(\n    object = file_to_move,\n    bucket = path_to_root_bucket,\n    file = file.path(local_path,substring(file_to_move, 14),fsep = .Platform$file.sep)\n  )\n}\n```\n\n\n```{python}\n# todo\n```\n\n\n```{bash}\naws s3 cp --recursive \"s3://ras2fim/inputs\" \"C:/Users/ras2fim/X-National_Datasets\"\n```\n\n\n\n\n\n\n\n\n\nTMP files\n\nR\n\n\n\nFrom cderv’s excellent answer at https://stackoverflow.com/questions/45894133/deleting-tmp-files\n\nYou can get the temp directory for the current R session. This will not change unless you load a new session.\n\ntmp_dir &lt;- tempdir()\ntmp_dir\n\n[1] \"/tmp/RtmpZgm8Uw\"\n\n\nThe temp directory contains the temp files and directory for the current R session\n\nlist.files(tmp_dir)\n\n[1] \"file1d8c146692b1\" \"file1d8c1c2e242e\" \"file1d8c1d4fd5b8\" \"file1d8c4cd02657\"\n[5] \"file1d8c6c6891ed\" \"file1d8c7528ba41\" \"file1d8c91d899d\"  \"file1d8ca184188\" \n\n\nThe session temp directory is in the temp directory of the system. You can use this path if you want to delete all in the temp directory of the system (not recommended though because it is for all the system, not just R temp files)\n\ndirname(tmp_dir)\n\n[1] \"/tmp\"\n\n\nThis path is also contains in an environmental variable for the OS. (Obviously, I am on windows)\n\nSys.getenv(\"TEMP\")\n\n[1] \"\"\n\n\ntempfile() gives the path of a possible temporary file, in the tempdir() directory by default, with no file extension. The file is not created and tempfile gives different values when called several times\n\ntmp_file &lt;- tempfile()\ntmp_file\n\n[1] \"/tmp/RtmpZgm8Uw/file1d8c3b21a68c\"\n\nfile.exists(tmp_file)\n\n[1] FALSE\n\ntempfile() # new file path when called again\n\n[1] \"/tmp/RtmpZgm8Uw/file1d8c1c1b71d7\"\n\n\nWe can write something to tmp_file.\n\n# file is created by writeLines if it does not exist (this is the case here)\nwriteLines(\"This is a temp file\", con = tmp_file)\nfile.exists(tmp_file)\n\n[1] TRUE\n\n\nWe can read from this file:\n\nreadLines(tmp_file)\n\n[1] \"This is a temp file\"\n\n\nNow if you want to delete this file:\n\nfile.remove(tmp_file)\n\n[1] TRUE\n\nfile.exists(tmp_file)\n\n[1] FALSE",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#file-io-1",
    "href": "fileio.html#file-io-1",
    "title": "FILE I/O",
    "section": "File I/O",
    "text": "File I/O\n\n\n\n\n\n\nPartial loads cheatsheet\n\n\n\n\n\nR and the SF package make partial loading pretty easy, exemplified in the following:\n```{r}\n### From a bounding box\nx = AOI::aoi_ext(\"Fort Collins\", wh = 10000, bbox = TRUE) \nlyr = \"flowpaths\"\ngpkg = ...\n\nl = sf::st_layers(gpkg)\nind = which(l$name == lyr)\n\nsf::read_sf(gpkg, lyr,wkt_filter = sf::st_as_text(sf::st_transform(x, l$crs[[ind]])))\n\n### Query a table to partially read\nmy_poi &lt;- sf::read_sf(\"/vsitar/{/vsicurl/https://water.weather.gov/ahps2/shapefiles/national_shapefile_obs.tgz}/national_shapefile_obs.shp\",\n                      query = \"SELECT * FROM national_shapefile_obs WHERE GaugeLID IN ('SHLT2')\"\n                      query = \"SELECT * FROM national_shapefile_fcst_f024 WHERE Status NOT IN ('no_forecast', 'fcst_not_current', 'not_defined', 'out_of_service')\"\n                     )\nmapview::mapview(my_poi)\n```\n\n\n\n\nCompressing files\n\nUnzipZip\n\n\n\nRPythonPython TAR, partial extract\n\n\n```{r}\nutils::unzip(zipfile, files = NULL, list = FALSE, overwrite = TRUE, junkpaths = FALSE, exdir = \".\", unzip = \"internal\", setTimes = FALSE)\n```\n\n\n```{python}\ndf = pd.read_csv ('file_name.csv')\n```\n\n\n```{python}\nfor file in list_of_files:    \n    huc = file[0:8]\n    print('processing huc ' + huc)\n    str_out_dir = 'out_dir'+(huc)\n \n    if not os.path.exists(str_tar_file):\n        print('File does not exist?')\n        print(str_tar_file)\n        continue\n        \n    if os.path.exists(str_out_dir):\n        print('File already unpacked')\n        continue\n    \n    os.mkdir(str_out_dir)\n\n    # read the tar.gz file... this takes some time\n    t = tarfile.open(file, \"r:gz\")\n\n    for member in t.getmembers():\n        if member.name[-3:-2] == 'g' or member.name[-3:-2] == 'p' or member.name[-3:-2] == 'f' or member.name[-3:-2] == 'h' or member.name[-3:-2] == 'v':\n            #print(member.name)\n            t.extract(member, str_out_dir)\n\nprint('finished')\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\n# TODO\n```\n\n\n```{python}\n# TODO\n```\n\n\n\n\n\n\n\n\nCSV\n\nReadMergeWrite\n\n\n\nRPython\n\n\n```{r}\ndata.table::fread()\n```\n\n\n```{python}\nimport pandas as pd\ndf = pd.read_csv('file_name.csv')\n```\n\n\n\n\n\n\nRPythonPowerShell\n\n\n```{r}\nrrassler_records &lt;- list.files(path_to_ras_dbase, pattern = utils::glob2rx(\"*RRASSLER_metadata.csv$\"), full.names=TRUE, ignore.case=TRUE, recursive=TRUE)\nfull_accounting &lt;- rbindlist(lapply(rrassler_records, function(x) data.table::fread(x, colClasses = c(\"nhdplus_comid\" = \"character\",\"model_name\" = \"character\",\"units\" = \"character\",\"crs\" = \"character\",\"final_name_key\" = \"character\"))))\n```\nor\n```{r}\nsetwd(\"/tmp\")\nfilenames &lt;- list.files(pattern =  \"*.csv\", full.names=TRUE)\ncsvdata &lt;- lapply(filenames,function(i){\n    read.csv(i, header=TRUE)\n})\ndf &lt;- do.call(rbind.data.frame, csvdata)\nwrite.csv(df,\"combinedcsvs.csv\", row.names=FALSE)\n```\n\n\n```{python}\n# ToDo\n```\n\n\n```{powershell}\n$getFirstLine = $true\n\nget-childItem *.csv | foreach {\n    $filePath = $_\n    \n    $lines = Get-Content $filePath  \n    $linesToWrite = switch($getFirstLine) {\n        $true  {$lines}\n        $false {$lines | Select -Skip 1}\n    \n    }\n    \n    $getFirstLine = $false\n    Add-Content \"combinedcsvs.csv\" $linesToWrite\n    }\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\ndata.table::fwrite(frame,path)\n```\n\n\n```{python}\nimport pandas as pd\ndf = pd.write_csv(frame,path)\n```\n\n\n\n\n\n\n\n\nFGB\n\nReadMergeWrite\n\n\n\nRPython\n\n\n```{r}\nsf::st_read()\n```\n\n\n```{python}\nimport geopandas\nfeature_collection = geopandas.read_file(file_path)\n```\nor\n```{python}\nimport flatgeobuf\nfeature_collection = flatgeobuf.read(file_path)\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\nflow_points &lt;- list.files(out_path,pattern = \"nbi_poi_VPU*\",full.names = T)\nall_flowpoints &lt;- lapply(1:length(flow_points), function(x) {\nsf::read_sf(flow_points[x])\n}) |&gt;\ndplyr::bind_rows()\nall_flowpoints\nsf::st_write(all_flowpoints,file.path(out_path,glue::glue(\"CONUS_nbi_POI_reconcile.gpkg\"),fsep = .Platform$file.sep), \"NBI\")\n```\n\n\n```{python}\nimport geopandas\nfeature_collection = geopandas.read_file(file_path)\n# todo\n```\n\n\n\n\n\n\nRPython\n\n\n```{python}\nprint(\"Hello World!\")\n```\n\n\n```{bash}\nimport flatgeobuf\nflatgeobuf.write(file_path)\n```\n\n\n\n\n\n\n\n\nParquet\n\nReadMosaicWrite\n\n\n\nRPython\n\n\n```{r}\ndat &lt;- arrow::read_parquet()\n```\n\n\n```{python}\nimport pyarrow.parquet as pq\nconus_transects_xyz = pq.read_table(conus_transects_xyz_path, filters=[('vpuid', '==', '12')]).to_pandas()\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\n\n```\n\n\n```{python}\n\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\n\n```\n\n\n```{python}\n\n```\n\n\n\n\n\n\n\n\nLAZ (entwine)\nSee also: Data form: Point clouds\n\nReadWrite\n\n\n\nFilterWriteUnzip\n\n\n```{r}\n\n```\n\n\n```{python}\n`entwine build -i ~/data/chicago.laz -o ~/entwine/chicago`\n```\n\n\nUse lastools, from the install directory…\n```{cmd}\n\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\n\n```\n\n\n\nXYZ to las\n\nFrom https://pointly.medium.com/how-to-convert-your-point-cloud-data-into-las-laz-434ada0f1e99\n\nPointly will work with the data format LAS or LAZ. The preferred upload format is LAZ because of its compression, followed by LAS.\nExcurse: A LAS (LiDAR Aerial Survey) is an open and binary file format which stores LiDAR point cloud data and a point classification system. It is specified by the American Society for Photogrammetry and Remote Sensing (ASPRS) and its latest Version is LAS 1.4. The purpose of LAS is to function as a standard format between LiDAR tools, including hardware and software. It is seen by the majority as an industry standard for LiDAR data.\nWhereas LAS is probably the most used data format for the work with point clouds, there are as well a couple of different formats, like ASCII or PLY. If you have your point cloud data in a format different to LAS, there are some easy ways to convert your specific point cloud format to LAS or LAZ. For example open source solutions like CloudCompare, LASzip or PDAL help you to achieve this within very few clicks.\nHow to convert benchmark point cloud data sets with Python Apart from the solutions mentioned above, there is always the possibility to quickly convert any kind of file format using Python and the accompanying LAS packages laspy and Open3D. This approach is very useful, when dealing with custom formats you frequently encounter with benchmark point cloud datasets, such as Semantic3D. In this example, point cloud data is stored in various text files and point classifications are likewise stored separately. To have LAS-formatted files including the classification layer, the “pythonic” way of doing this would be:\n\nRead-in the text files using e.g. pandas\n\n```{python}\nraw = pd.read_csv(PATH_TO_DATA_FILE, sep=’ ‘, header=None).values\nlabels = pd.read_csv(PATH_TO_LABEL_FILE, sep=’ ‘, header=None).values[:,0]\n```\n\nExtract the coordinate, the color and intensity information (in this case xyz is stored in the first 3 columns, rgb in columns 5 to 7 and intensity in the 4th column)\n\n```{python}\nxyz = np.ascontiguousarray(raw[:, 0:3], dtype=’float32')\nrgb = np.ascontiguousarray(raw[:, 4:7], dtype=’uint8') * 256\ni = np.ascontiguousarray(raw[:, 3], dtype=’float32') * 256\n```\n\nCreate an empty LAS file with the desired LAS-version and format. Define the necessary header attributes offset and scale to fit the data.\n\n```{python}\nhdr = laspy.header.Header(file_version=1.4, point_format=7)\nmins = np.floor(np.min(xyz, axis=1))\nhdr.offset = mins\nhdr.scale = [0.01,0.01,0.01]\noutfile = laspy.file.File(PATH_TO_OUTPUT_LAS, mode=”w”, header=hdr)\n```\n\nWrite the desired dimensions to the LAS-file — DONE!\n\n```{python}\noutfile.x = xyz[:,0]\noutfile.y = xyz[:,1]\noutfile.z = xyz[:,2]\noutfile.Red = rgb[:,0]\noutfile.Green = rgb[:,1]\noutfile.Blue = rgb[:,2]\noutfile.Intensity = i\noutfile.classification = labels\noutfile.close()\n```\nNow that you know how to convert your data types into LAS, last but not least let’s have a look on how to compress LAS into LAZ. PDAL is one tool for an easy compression. Here (https://pdal.io/workshop/exercises/translation/compression.html) you’ll find a detailed description on how to use PDAL and get that LAZ format.\n\n\n\n\n\n\n\n\n\nGeopackage\n\nReadWrite\n\n\n\nRPython\n\n\n```{r}\nsf::st_read(,quiet=TRUE)\n\n# or, to partially read:\nx = AOI::aoi_ext(\"Fort Collins\", wh = 10000, bbox = TRUE) \nlyr = \"flowpaths\"\ngpkg = ...\n\nl = sf::st_layers(gpkg)\nind = which(l$name == lyr)\n\nsf::read_sf(gpkg, lyr,wkt_filter = sf::st_as_text(sf::st_transform(x, l$crs[[ind]])))\n```\n\n\n```{python}\nimport geopandas\nimport fiona\n\nfiona.listlayers(gpkg)\ncountries_gdf = geopandas.read_file(\"package.gpkg\", layer='countries')\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\n# todo\n```\n\n\n```{python}\n# todo\n```\n\n\n\n\n\n\n\n\nShapefile\n\nReadMergeWrite\n\n\n\nRPython\n\n\n```{r}\nsf::st_read(,quiet=TRUE)\n```\n\n\n```{python}\ngeopandas.read_file()\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\ntop_of_USFIMR &lt;- file.path(\"~/data/raw/USFIMR/USFIMR_all/\",fsep = .Platform$file.sep) \nlist_of_shapes &lt;- list.files(top_of_USFIMR,pattern = \".shp$\",recursive = TRUE,full.names = TRUE)\n\n# fixed_crs &lt;- sf::st_crs(sf::st_read(list_of_shapes[1]))\n\nall_shapes &lt;- list_of_shapes |&gt; \n  purrr::map(sf::st_read) |&gt; \n  # purrr::map(sf::st_transform, crs = fixed_crs) |&gt;\n  dplyr::bind_rows()\n```\n\n\n```{python}\ngeopandas.read_file()\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\nsf::st_write(,quiet=TRUE)\n```\n\n\n```{python}\nimport geopandas\ndata.to_file()\n```\n\n\n\n\n\n\n\n\nGeodatabase\n\nReadWrite\n\n\n\nRPythonGDAL\n\n\n```{r}\nsf::st_layers(file.path(library_path,\"data\",\"WBD_National_GDB.gdb\",fsep = .Platform$file.sep))\n```\nor\n```{r}\nutils::download.file('https://prd-tnm.s3.amazonaws.com/index.html?prefix=StagedProducts/Hydrography/NHD/National/HighResolution/GDB/',\n                     'I:/Dropbox/root/database/raw/water/WBD_National_GDB/WBD_National_GDB.gdb')\n\n# Read and explore database  \nfgdb &lt;- 'I:/Dropbox/root/database/raw/water/WBD_National_GDB/WBD_National_GDB.gdb'\nfc_list &lt;-rgdal::ogrListLayers(fgdb)\nprint(fc_list)\n\n# Read the feature class, transform and clean, and write out\nfc &lt;- rgdal::readOGR(dsn=fgdb,layer=\"WBDHU10\") \nfc_tf &lt;- sf::st_transform(sf::st_as_sf(fc), sf::st_crs(sf::st_read('I:/Dropbox/root/database/hosted/water/HUC8.fgb')))\nsf::st_write(dplyr::select(fc_tf,c(areasqkm,huc10,name)),'I:/Dropbox/root/database/hosted/water/HUC10.fgb')\n```\n\n\n```{python}\nimport fiona\nimport shapely\nimport geopandas as gpd\n\nfiona.listlayers(hydrofabric_gpkg_path)\n\ndriver = ogr.GetDriverByName(\"FileGDB\")\ndriver = ogr.GetDriverByName(\"OpenFileGDB\")\nds = driver.Open(\"NWM_v2_1_hydrofabric.gdb\", 0)\ndat = ds.GetLayer(\"nwm_waterbodies_puertorico\")\n\ndef check_valid(feature):\n    return feature.geometry.is_valid\nmap(check_valid, dat)\n\ngpd.read_file(\"NYCFutureHighTideWithSLR.gdb\", driver='FileGDB', layer='layer_b')\n```\n\n\nFrom a shapefile:\n```{bash}\nogr2ogr -f FlatGeobuf -nln waterbody -t\\_srs EPSG:4326 -nlt PROMOTE\\_TO_MULTI -makevalid waterbody.fgb waterbody.shp\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\nsf::st_layers(file.path(library_path,\"data\",\"WBD_National_GDB.gdb\",fsep = .Platform$file.sep))\n```\n\n\n```{python}\n#todo\n```\n\n\n\n\n\n\n\n\nTIF\n\nReadWriteTransform to COG\n\n\n\nRPython\n\n\n```{r}\nsf::st_layers(file.path(library_path,\"data\",\"WBD_National_GDB.gdb\",fsep = .Platform$file.sep))\n```\n\n\n```{python}\nimport fe\nfe.read(path)\n```\n\n\n\n\n\n\nRPythonGDAL\n\n\n```{r}\nterra::writeRaster(nlcd,overwrite=TRUE,filename=file.path(feature_path,'target_grid.tif', fsep=.Platform$file.sep),wopt=list(gdal=c(\"COMPRESS=LZW\", \"TFW=YES\",\"of=COG\")))\n```\n\n\n```{python}\nimport shapely\nimport geopandas as gpd\n```\n\n\nFrom a shapefile: {bash,eval=FALSE} ogr2ogr -f FlatGeobuf -nln waterbody -t\\_srs EPSG:4326 -nlt PROMOTE\\_TO_MULTI -makevalid waterbody.fgb waterbody.shp\n\n\n\n\n\n\nRGDAL\n\n\n```{r}\nterra::writeRaster(terra::rast(file.path(tmp_path', fsep=.Platform$file.sep)),\n                   overwrite=TRUE,\n                   filename=file.path(tmp_path,file_cog.ext,.Platform$file.sep),\n                   wopt=list(gdal=c(\"COMPRESS=LZW\", \"TFW=YES\",\"of=COG\")))\n```\n\n\n```{bash}\ngdalwarp -f COG -co TILING\\_SCHEME=GoogleMapsCompatible -co COMPRESS=DEFLATE -t\\_srs EPSG:4326 nlcd.tif nlcd_cog.tif\n```\n\n\n\n\n\n\n\n\nGeoservers\n\nRead\n\n\n\nRPythonManually\n\n\n```{r}\n#you guessed it, todo\n```\n\n\n\nUsing https://github.com/openaddresses/pyesridump\n\n```{python}\nesri2geojson http://cookviewer1.cookcountyil.gov/ArcGIS/rest/services/cookVwrDynmc/MapServer/11 cookcounty.geojson\n```\n\n\nWebsite helper: https://geodatadownloader.com/\n\n\n\n\n\n\n\n\nKML/KMZ\n\nReadWrite\n\n\n\nRPython\n\n\n```{r}\n# todo\n```\n\n\n```{python}\n# todo\n```\n\n\n\n\n\n\nRPython\n\n\n```{r}\n# todo\n```\n\n\n```{python}\n# todo\n```\n\n\n\n\n\n\n\n\nNETCDF\n\nReadWrite\n\n\n\nRR (old)Python\n\n\n```{r}\nfull_nc &lt;- ncdf4::nc_open(\"C:/Users/jimcoll/Desktop/nwm.t00z.analysis_assim.channel_rt.tm01.conus.nc\")\nfull_nc\nncdf4::nc_close(full_nc)\n```\n\n\n```{r}\ntest_nc &lt;- RNetCDF::open.nc(\"C:/Users/jimcoll/Desktop/nwm.t23z.short_range_coastal.total_water.f001.atlgulf.nc\")\nRNetCDF::print.nc(test_nc)\ndat &lt;- RNetCDF::var.get.nc(test_nc,'elevation') %&gt;% as.data.frame()\ncolnames(dat) &lt;- 'elevation'\nggplot2::ggplot(na.omit(dat),ggplot2::aes(x=elevation)) +\n  ggplot2::geom_histogram(binwidth=10)\nRNetCDF::close.nc(test_nc)\n```\n\n\n```{python}\n# todo\n```\n\n\n\n\n\n\nR\n\n\n```{r}\nsuppressMessages(suppressWarnings(library(tidyverse)))\nsuppressMessages(suppressWarnings(library(geosphere)))\nsuppressMessages(suppressWarnings(library(sf)))\nsuppressMessages(suppressWarnings(library(sp)))\nsuppressMessages(suppressWarnings(library(httr)))\nsuppressMessages(suppressWarnings(library(gdalUtilities)))\nsuppressMessages(suppressWarnings(library(rgdal)))\nsuppressMessages(suppressWarnings(library(lubridate)))\nsuppressMessages(suppressWarnings(library(elevatr)))\nsuppressMessages(suppressWarnings(library(interp)))\nsuppressMessages(suppressWarnings(library(raster)))\n\nx_section_dbase_name &lt;- \"Lower_Colorado_20220127.nc\" \nRouteLink_name &lt;- 'RouteLink_CONUS.nc'\nx_sections &lt;- ncdf4::nc_open(x_section_dbase_name) \nRouteLink &lt;- ncdf4::nc_open(RouteLink_name) \n\nx_sections_dbase &lt;- data.frame(\n  \"xid\" = ncdf4::ncvar_get(x_sections,\"xid\"),\n  \"xid_length\" = ncdf4::ncvar_get(x_sections,\"xid_length\"),\n  \"comid\" = ncdf4::ncvar_get(x_sections,\"comid\"),\n  \"comid_rel_dist_ds\" = ncdf4::ncvar_get(x_sections,\"comid_rel_dist_ds\"),\n  \"xid_d\" = ncdf4::ncvar_get(x_sections,\"xid_d\"),\n  \"x\" = ncdf4::ncvar_get(x_sections,\"x\"),\n  \"y\" = ncdf4::ncvar_get(x_sections,\"y\"),\n  \"z\" = ncdf4::ncvar_get(x_sections,\"z\"),\n  \"n\" = ncdf4::ncvar_get(x_sections,\"n\"),\n  \"source\" = ncdf4::ncvar_get(x_sections,\"source\"))\n\nRouteLink_dbase &lt;- data.frame(\n  \"link\" = ncdf4::ncvar_get(RouteLink,\"link\"),\n  \"from\" = ncdf4::ncvar_get(RouteLink,\"from\"),\n  \"to\" = ncdf4::ncvar_get(RouteLink,\"to\"),\n  \"lon\" = ncdf4::ncvar_get(RouteLink,\"lon\"),\n  \"lat\" = ncdf4::ncvar_get(RouteLink,\"lat\"),\n  \"alt\" = ncdf4::ncvar_get(RouteLink,\"alt\"),\n  \"order\" = ncdf4::ncvar_get(RouteLink,\"order\"),\n  \"Qi\" = ncdf4::ncvar_get(RouteLink,\"Qi\"),\n  \"MusK\" = ncdf4::ncvar_get(RouteLink,\"MusK\"),\n  \"MusX\" = ncdf4::ncvar_get(RouteLink,\"MusX\"),\n  \"Length\" = ncdf4::ncvar_get(RouteLink,\"Length\"),\n  \"n\" = ncdf4::ncvar_get(RouteLink,\"n\"),\n  \"So\" = ncdf4::ncvar_get(RouteLink,\"So\"),\n  \"ChSlp\" = ncdf4::ncvar_get(RouteLink,\"ChSlp\"),\n  \"BtmWdth\" = ncdf4::ncvar_get(RouteLink,\"BtmWdth\"),\n  \"Kchan\" = ncdf4::ncvar_get(RouteLink,\"Kchan\"),\n  \"ascendingIndex\" = ncdf4::ncvar_get(RouteLink,\"ascendingIndex\"),\n  \"nCC\" = ncdf4::ncvar_get(RouteLink,\"nCC\"),\n  \"TopWdthCC\" = ncdf4::ncvar_get(RouteLink,\"TopWdthCC\"),\n  \"TopWdth\" = ncdf4::ncvar_get(RouteLink,\"TopWdth\"))\n```\nor the newest and shiniest in RNetCDF\n```{r}\n# database export example\nfile &lt;- file.path(snapshot_dir,paste0('eHydro_ned_cross_sections_',gsub(\"-\",\"_\",Sys.Date()),'.nc'))\ncross_section_points &lt;- cross_section_points %&gt;% sf::st_drop_geometry()\ncross_section_points &lt;- export_dbase\nnc &lt;- RNetCDF::create.nc(file)\nRNetCDF::dim.def.nc(nc, \"xid\", unlim=TRUE)\n\nRNetCDF::var.def.nc(nc, \"xid\", \"NC_INT\", \"xid\")\nRNetCDF::var.def.nc(nc, \"xid_length\", \"NC_DOUBLE\", 0)\nRNetCDF::var.def.nc(nc, \"comid\", \"NC_INT\", 0)\nRNetCDF::var.def.nc(nc, \"comid_rel_dist_ds\", \"NC_DOUBLE\", 0)\nRNetCDF::var.def.nc(nc, \"xid_d\", \"NC_DOUBLE\", 0)\nRNetCDF::var.def.nc(nc, \"x\", \"NC_DOUBLE\", 0)\nRNetCDF::var.def.nc(nc, \"y\", \"NC_DOUBLE\", 0)\nRNetCDF::var.def.nc(nc, \"z\", \"NC_DOUBLE\", 0)\nRNetCDF::var.def.nc(nc, \"source\", \"NC_INT\", 0)\n\n##  Put some _FillValue attributes\nRNetCDF::att.put.nc(nc, \"xid\", \"_FillValue\", \"NC_INT\", -99999)\nRNetCDF::att.put.nc(nc, \"xid_length\", \"_FillValue\", \"NC_DOUBLE\", -99999)\nRNetCDF::att.put.nc(nc, \"comid\", \"_FillValue\", \"NC_INT\", -99999)\nRNetCDF::att.put.nc(nc, \"comid_rel_dist_ds\", \"_FillValue\", \"NC_DOUBLE\", -99999.9)\nRNetCDF::att.put.nc(nc, \"xid_d\", \"_FillValue\", \"NC_DOUBLE\", -99999.9)\nRNetCDF::att.put.nc(nc, \"x\", \"_FillValue\", \"NC_DOUBLE\", -99999.9)\nRNetCDF::att.put.nc(nc, \"y\", \"_FillValue\", \"NC_DOUBLE\", -99999.9)\nRNetCDF::att.put.nc(nc, \"z\", \"_FillValue\", \"NC_DOUBLE\", -99999.9)\nRNetCDF::att.put.nc(nc, \"source\", \"_FillValue\", \"NC_INT\", -99999.9)\n\n##  Put all  the data:\nRNetCDF::var.put.nc(nc, \"xid\", cross_section_points$xid)\nRNetCDF::var.put.nc(nc, \"xid_length\", cross_section_points$xid_length)\nRNetCDF::var.put.nc(nc, \"comid\", cross_section_points$comid)\nRNetCDF::var.put.nc(nc, \"comid_rel_dist_ds\", cross_section_points$comid_rel_dist_ds)\nRNetCDF::var.put.nc(nc, \"xid_d\", cross_section_points$xid_d)\nRNetCDF::var.put.nc(nc, \"x\", cross_section_points$x)\nRNetCDF::var.put.nc(nc, \"y\", cross_section_points$y)\nRNetCDF::var.put.nc(nc, \"z\", cross_section_points$z)\nRNetCDF::var.put.nc(nc, \"source\", cross_section_points$source)\n\nRNetCDF::att.put.nc(nc, \"NC_GLOBAL\", \"title\", \"NC_CHAR\", \"Natural cross section data for inland routing task\")\nRNetCDF::att.put.nc(nc, \"NC_GLOBAL\", \"date_scraped\", \"NC_CHAR\", \"eHydro data scraped on 2021-11-01\")\nRNetCDF::att.put.nc(nc, \"NC_GLOBAL\", \"date_generated\", \"NC_CHAR\", paste(\"database generated on \",gsub(\"-\",\"_\",Sys.Date())))\nRNetCDF::att.put.nc(nc, \"NC_GLOBAL\", \"code_repo\", \"NC_CHAR\", 'https://github.com/jimcoll/repo')\nRNetCDF::att.put.nc(nc, \"NC_GLOBAL\", \"contact\", \"NC_CHAR\", \"jim coll\")\nRNetCDF::att.put.nc(nc, \"NC_GLOBAL\", \"projection\", \"NC_CHAR\", \"epsg:6349\")\nRNetCDF::att.put.nc(nc, \"xid\", \"title\", \"NC_CHAR\", \"cross section ID\")\nRNetCDF::att.put.nc(nc, \"xid\", \"interpretation\", \"NC_CHAR\", \"a unique cross section id (fid)\")\nRNetCDF::att.put.nc(nc, \"xid_length\", \"title\", \"NC_CHAR\", \"cross section ID length\")\nRNetCDF::att.put.nc(nc, \"xid_length\", \"interpretation\", \"NC_CHAR\", \"the total length (in meters) of a cross section\")\nRNetCDF::att.put.nc(nc, \"xid_length\", \"unit\", \"NC_CHAR\", \"meters\")\nRNetCDF::att.put.nc(nc, \"comid\", \"title\", \"NC_CHAR\", \"Spatially assosiated COMID\")\nRNetCDF::att.put.nc(nc, \"comid\", \"interpretation\", \"NC_CHAR\", \"the comid from the NHD that the cross section intersects: should join with the routelink link field\")\nRNetCDF::att.put.nc(nc, \"comid_rel_dist_ds\", \"title\", \"NC_CHAR\", \"the relative (0-1) distance from the start of the comid that the cross section crosses at\")\nRNetCDF::att.put.nc(nc, \"comid_rel_dist_ds\", \"interpretation\", \"NC_CHAR\", \"the relative (0-1) distance from the start of the comid that the cross section crosses at\")\nRNetCDF::att.put.nc(nc, \"comid_rel_dist_ds\", \"unit\", \"NC_CHAR\", \"percentage (0-1)\")\nRNetCDF::att.put.nc(nc, \"xid_d\", \"title\", \"NC_CHAR\", \"Cross section distance\")\nRNetCDF::att.put.nc(nc, \"xid_d\", \"interpretation\", \"NC_CHAR\", \"The distance (meters from the left-most point on the cross section) of the observation.  0 is centered on the intersection of the cross section and the NHD reach\")\nRNetCDF::att.put.nc(nc, \"xid_d\", \"unit\", \"NC_CHAR\", \"meters\")\nRNetCDF::att.put.nc(nc, \"x\", \"title\", \"NC_CHAR\", \"x\")\nRNetCDF::att.put.nc(nc, \"x\", \"interpretation\", \"NC_CHAR\", \"x coordinate (Longitude)\")\nRNetCDF::att.put.nc(nc, \"x\", \"unit\", \"NC_CHAR\", \"meters\")\nRNetCDF::att.put.nc(nc, \"x\", \"projection\", \"NC_CHAR\", \"epsg:6349\")\nRNetCDF::att.put.nc(nc, \"y\", \"title\", \"NC_CHAR\", \"y\")\nRNetCDF::att.put.nc(nc, \"y\", \"interpretation\", \"NC_CHAR\", \"y coordinate (Latitude)\")\nRNetCDF::att.put.nc(nc, \"y\", \"unit\", \"NC_CHAR\", \"degree\")\nRNetCDF::att.put.nc(nc, \"y\", \"projection\", \"NC_CHAR\", \"epsg:6349\")\nRNetCDF::att.put.nc(nc, \"z\", \"title\", \"NC_CHAR\", \"z\")\nRNetCDF::att.put.nc(nc, \"z\", \"interpretation\", \"NC_CHAR\", \"vertical elevation (meters)\")\nRNetCDF::att.put.nc(nc, \"z\", \"unit\", \"NC_CHAR\", \"meters\")\nRNetCDF::att.put.nc(nc, \"z\", \"projection\", \"NC_CHAR\", \"epsg:6349\")\nRNetCDF::att.put.nc(nc, \"source\", \"title\", \"NC_CHAR\", \"source\")\nRNetCDF::att.put.nc(nc, \"source\", \"interpretation\", \"NC_CHAR\", \"The source of the data: 1=eHydro, 2=CWMS\")\n\nRNetCDF::close.nc(nc)\nunlink(nc)\n```\n\n\n\n\n\n\n\n\nHDF\n\nReadWrite\n\n\n\npythonR\n\n\nhttps://github.com/ICESat2-SlideRule/h5coro/\n```{bash}\necho \"Hello World!\"\n```\n\n\n```{bash}\necho \"Hello World!\"\n```\n\n\n\n\n\n\npythonR\n\n\n```{bash}\necho \"Hello World!\"\n```\n\n\n```{bash}\necho \"Hello World!\"\n```\n\n\n\n\n\n\n\n\nVideo\n\nTransform to webm with ffmpg\n\n\n\nA two pass approach\n```{bash}\n// You can do this in two passes\nffmpeg\n  -ss 00:00:14 -to 00:00:24 //time stamp of what you want to cut in your video in the format: hh:mm:ss.ss\n  -i \"input.mp4\" //input file location and name\n  -vf scale=-1:720 //scale of the video, -1 for variable, exact value for what you want, format: width:height\n  -c:v libvpx-vp9 //selects vp9 as the renderer\n  -b:v 2500k //your bitrate, to keep the file under 4 MB multiple the value you write before k by the length of your webm in seconds and keep that under 32384\n  -pass 1 //pass 1 of the two pass encoding which will greatly increase the quality of your wemb, ffmpeg will scout out the webm you want to render during pass 1 and apply many compression tricks to sharpen the moving parts and blur the background for example\n  -an // audio:null, for no audio. You can use \"-c:a libopus\" to render audio if you wish.\n  -f null NUL && ^ //ends strings\n\n//Repeat the same things but change pass1 to pass2 and add your output.webm filename at the end instead of \"-f null NUL&& ^\".\nffmpeg -ss 00:00:14 -to 00:00:24 -i \"input.mp4\" -vf scale=-1:720 -c:v libvpx-vp9 -b:v 2500k -an -sn -pass 2 \"output.webm\"\n```\n- or -\n\n\nA two pass approach in one command\n\nBashPowerShell\n\n\n```{bash}\nffmpeg -i \"input.mkv\" -vf scale=-1:720 -c:v libvpx-vp9 -b:v 2500k -an -sn -pass 1 -f webm /dev/null && \\ ffmpeg -i \"input.mkv\" -vf scale=-1:720 -c:v libvpx-vp9 -b:v 2500k -an -sn -pass 2 \"output.webm\"\n```\n\n\n```{powershell}\nffmpeg -i \"input.mkv\" -vf scale=-1:720 -c:v libvpx-vp9 -b:v 2500k -an -sn -pass 1 -f webm NUL && ffmpeg -i \"input.mkv\" -vf scale=-1:720 -c:v libvpx-vp9 -b:v 2500k -an -sn -pass 2 \"output.webm\"\n```\n\n\n\n- or -\n\n\nA single transformation\n```{bash}\nffmpeg -i \"input.mkv\" -vf scale=-1:720 -c:v libvpx-vp9 -crf 30 -b:v 0 -an -sn \"output.webm\"\n```\n\n\n\n\n\n\nImages\n\nencode as base64\n\n\n```{Chrome}\n1) Drag the image into chrome\n2) Right-click and open the source\n3) Right-click on the url in the \"Elements\" tab and \"Reveal in Source\"\n4) Right-click the image in the Resources Panel and choose Copy image as Data URL\n```\n\n\n\n_f1ebcd3e3aaea07fd966f8e2b5bcf067.png\n\n\n\n\n\n\n\nPDF\n\nConvert to markdownConvert to png\n\n\nAlso elaborated in how I install zotero, with markitdown:\n```{python}\nset PYTHONUTF8=1   # Only need to do this once if you see an encoding error\nmarkitdown \".\\ZoteroDBase\\storage\\9JF26F9X\\Coll and Li - 2018 - Comprehensive accuracy assessment of MODIS daily s.pdf\" &gt; .\\text.md\n```\n\n\n\nRGimp\n\n\n\n## With magick\nmagick::image_write(magick::image_read_pdf(\"~/data/temp/templateannn.pdf\",pages = 1,density = 300), \n                    path = \"~/data/temp/templateannn.png\", \n                    format = \"png\",\n                    density = 300)\n\n## With pdftools\npdftools::pdf_convert(\"~/data/temp/templateannn.pdf\")\n\n## With animation\nanimation::im.convert(files = \"~/data/temp/templateannn.pdf\", output = \"~/data/temp/templateannn.png\", extra.opts=\"-density 300\")\n\n\n\nImport and export",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "fileio.html#footnotes",
    "href": "fileio.html#footnotes",
    "title": "FILE I/O",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nmatplotlib syntax being the other↩︎",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "FILE I/O"
    ]
  },
  {
    "objectID": "elementsofGIST.html",
    "href": "elementsofGIST.html",
    "title": "Elements of GIS&T",
    "section": "",
    "text": "The bread and butter tasks of a geographer, hopefully sorted and organized in an atomic, functional, and logical manner. As mentioned, this form is only partially my own, but is more a reflection, distillation, and grateful pilfering of several far more authoritative and comprehensive references and thinkers. I heartily recommend checking them out before you waste your time here. While that full list can be found on my Atlas homepage, most this section in particular draws inspiration and content from Spatial Data Science by Edzer Pebesma and Roger Bivand, the periodic table of spatial analysis from https://gisgeography.com and the framing of Diataxis.\n\n\n\nMy own factoring of the Elements of GIS&T. Again. heavily inspired by the far more fleshed out pages of gisgeography.com’s Periodic Table\n\n\nAs I’ve pointed out before, I hate semantics but they seem like a necessary evil in interdisciplinary teams. While it’s usually seen as fine to let semantic slipups slide by, I’d argue that in the long run it’s best to catch the important ones in the moment. Not only does doing so ensure that you and your audience are aiming at the same concepts, but when that audience goes on to share the knowledge they’ve gained they’ll (hopefully) be more attentive to those nuances and ensure that they are communicating the message you would like to convey as clearly as possible. This comes from someone who would (and still does) conceptually call half of the following operations “joins”…\n\nI’ve scaffolded out the entire table, but expect to find only a few of these to be populated for the time being; I’ll release the others as I feel they matured into real resources.",
    "crumbs": [
      "Atlas",
      "Geoprocessing"
    ]
  },
  {
    "objectID": "ePydRo.html#tutorials",
    "href": "ePydRo.html#tutorials",
    "title": "ePydRo",
    "section": "Tutorials",
    "text": "Tutorials"
  },
  {
    "objectID": "ePydRo.html#explainers",
    "href": "ePydRo.html#explainers",
    "title": "ePydRo",
    "section": "Explainers",
    "text": "Explainers\n\nWhat’s ePydRo?\nShort answer?:\nThis is a suite of tools and scripts written in both Python and R that are used to manipulate the eHydro database into a form that can be used for intimidate integration into the NOAA master modeling surface.\nSlightly longer version:\nMeasuring the landscape is a complex science with several critical methodological inflection points that bake assumptions into the data that is read off the survey pole, recorded in the computer, pushed through the internal processing pipeline, and delivered to a client for hosting to the end users. At that point those end users (us) come along and discover that the data is not in a form immediately integrable into our workflow. For some applications, addressing this can simply be a means of harmonizing the naming schema between the two databases but in the case of terrain and elevations, the process of “harmonizing that naming schema” involves complex datum transformations and re-projections that place that measured point in the correct spot for the intended space.\nThis application transforms the eHydro data from their recorded datum to a normalized universal datum and reshapes the data structure to a more efficient form amenable to cloud access patterns and partial reads as an analysis ready value used to plug into the living master modeling surface constructed by the hydrofabric team.\n\n\nHow does ePydRo data flow?\n ## HowTo’s\nThe general workflow in cookbook form:"
  },
  {
    "objectID": "ePydRo.html#credits-and-references",
    "href": "ePydRo.html#credits-and-references",
    "title": "ePydRo",
    "section": "Credits and references",
    "text": "Credits and references\nCredit to the packages used in the development, testing, and deployment of this package not exclusive of the ones found in the DESCRIPTION file. We are incredibly grateful to the USACE eHydro team for the phenomenal job they do orchestrating the nations hydrographic survey collection and the open dissemination pattern they use.\n\nFor questions\nJim Coll (FIM Developer), Fernando Salas (Director, OWP Geospatial Intelligence Division)"
  },
  {
    "objectID": "diataxis.html",
    "href": "diataxis.html",
    "title": "Diataxis",
    "section": "",
    "text": "feb8644da2b3b04e59f3ab6bc997ed00.png\nTutorials are lessons that take the reader by the hand through a series of steps to complete a project of some kind.\nHow-to guides are directions that take the reader through the steps required to solve a real-world problem.\nReference guides are technical descriptions of the machinery and how to operate it.\nExplanation is discussion that clarifies and illuminates a particular topic."
  },
  {
    "objectID": "diataxis.html#what-am-i-writing",
    "href": "diataxis.html#what-am-i-writing",
    "title": "Diataxis",
    "section": "What am I writing?",
    "text": "What am I writing?\n\nTutorials vs guidesReferences vs Explanations\n\n\nIn important respects, tutorials and how-to guides are indeed similar. They are both practical guides: they contain directions for the user to follow. They’re not there to explain or convey information. They exist to guide the user in what to do rather than what there is to know or understand. They both set out steps for the reader to follow, and they both promise that if the reader follows those steps, they’ll arrive at a successful conclusion. Neither of them make much sense except for the user who has their hands on the machinery, ready to do things. They both describe ordered sequences of actions. You can’t expect success unless you perform the actions in the right order. They are closely related, and like many close relations, can be mistaken for one another at first glance.\nWhat matters is what the user needs. A tutorial serves the needs of the user who is at study. Its obligation is to provide a successful learning experience. A how-to guide serves the needs of the user who is at work. Its obligation is to help the user accomplish a task. These are completely different needs and obligations, and they are why the distinction between tutorials and how-to guides matters: tutorials are learning-oriented, and how-to guides are task-oriented.\n\n\n\n\n\n\n\nTutorial\nHow to\n\n\n\n\nA tutorial’s purpose is to help the pupil acquire basic competence.\nA how-to guide’s purpose is to help the already-competent user perform a particular task correctly.\n\n\nA tutorial provides a learning experience. People learn skills through practical, hands-on experience. What matters in a tutorial is what the learner does, and what they experience while doing it.\nA how-to guide directs the user’s work.\n\n\nThe tutorial follows a carefully-managed path, starting at a given point and working to a conclusion. Along that path, the learner must have the encounters that the lesson requires.\nThe how-to guide aims for a successful result, and guides the user along the safest, surest way to the goal, but the path can’t be managed: it’s the real world, and anything could appear to disrupt the journey.\n\n\nA tutorial familiarises the learner with the work: with the tools, the language, the processes and the way that what they’re working with behaves and responds, and so on. Its job is to introduce them, manufacturing a structured, repeatable encounter with them.\nThe how-to guide can and should assume familiarity with them all.\n\n\nThe tutorial takes place in a contrived setting, a learning environment where as much as possible is set out in advance to ensure a successful experience.\nA how-to guide applies to the real world, where you have to deal with what it throws at you.\n\n\nThe tutorial eliminates the unexpected.\nThe how-to guide must prepare for the unexpected, alerting the user to its possibility and providing guidance on how to deal with it\n\n\nA tutorial’s path follows a single line. It doesn’t offer choices or alternatives.\nA how-to guide will typically fork and branch, describing different routes to the same destination: If this, then that. In the case of …, an alternative approach is to…\n\n\nA tutorial must be safe. No harm should come to the learner; it must always be possible to go back to the beginning and start again.\nA how-to guide cannot promise safety; often there’s only one chance to get it right.\n\n\nIn a tutorial, responsibility lies with the teacher. If the learner gets into trouble, that’s the teacher’s problem to put right.\nIn a how-to guide, the user has responsibility for getting themselves in and out of trouble.\n\n\nThe learner may not even have sufficient competence to ask the questions that a tutorial answers.\nA how-to guide can assume that the user is asking the right questions in the first place.\n\n\nThe tutorial is explicit about basic things - where to do things, where to put them, how to manipulate objects. It addresses the embodied experience - in our medical example, how hard to press, how to hold an implement; in a software tutorial, it could be where to type a command, or how long to wait for a response.\nA how-to guide relies on this as implicit knowledge - even bodily knowledge.\n\n\nA tutorial is concrete and particular in its approach. It refers to the specific, known, defined tools, materials, processes and conditions that we have carefully set before the learner.\nThe how-to guide has to take a general approach: many of these things will be unknowable in advance, or different in each real-world case.\n\n\nThe tutorial teaches general skills and principles that later could be applied to a multitude of cases.\nThe user following a how-to guide is doing so in order to complete a particular task.\n\n\n\n\n\nMostly it’s fairly straightforward to recognize whether you’re dealing with one or the other. Reference, as a form of writing, is well understood; it’s used in distinctions we make about writing from an early age. In addition, examples of writing are themselves often clearly one or the other. A tidal chart, with its tables of figures, is clearly reference material. An article that explains why there are tides and how they behave is self-evidently explanation. There are good rules of thumb if you get confused. If it’s boring and unmemorable it’s probably reference. Lists of things (such as classes or methods or attributes), and tables of information, will generally turn out to belong in reference.\nOn the other hand, if you can imagine reading something in the bath, probably, it’s explanation (though really there is no accounting for what people might read in the bath). Imagine asking a friend, while out for a walk or over a drink, Can you tell me more about topic? - the answer or discussion that follows is most likely going to be an explanation of it.\nMostly we can rely safely on intuition to manage the distinction between reference and explanations. But only mostly - because it’s also quite easy to slip between one form and the other. It usually happens while writing reference material that starts to become expansive. For example, it’s perfectly reasonable to include illustrative examples in reference (just as an encyclopedia might contain illustrations) - but examples are fun things to develop, and it can be tempting to develop them into explanation (using them to say why, or show what if, or how it came to be). As a result one often finds explanatory material sprinkled into reference. This is bad for the reference, interrupted and obscured by digressions. But it’s bad for the explanation too, because it’s not allowed to develop appropriately and do its own work. The real test though if we’re in doubt about whether we’re something is or is supposed to be reference or explanation is: is this something someone would turn to while working, that is, while actually getting something done, executing a task? Or is it something they’d need once they have stepped away from the work, and want to think about it? These are two very fundamentally different needs of the reader, that reflect how, at that moment, the reader stands in relation to the craft in question, in a relationship of work or study.\nReference is what a user needs in order help apply knowledge and skill, while they are working. Explanation is what someone will turn to to help them acquire knowledge and skill - “study”. Understanding those two relationships and responding to the needs in them is the key to creating effective reference and explanation.\n\n\n\n\nIn short:\nWhen in doubt as to where to place something, ask yourself:\n\nAre we dealing with practical steps (action, something someone will do), or are we dealing with theoretical knowledge (propositional knowledge, information, something someone will only think)?\n\nand:\n\nAre we dealing with study (the acquisition of skills and knowledge) or work (the application of skills and knowledge)?\n\n\n\n\n\n\n\n\n\nIf the content describes…\n…and serves the user’s…\n…then it must belong to…\n\n\n\n\npractical steps\nstudy\na tutorial\n\n\npractical steps\nwork\na how-to guide\n\n\ntheoretical knowledge\nwork\nreference\n\n\ntheoretical knowledge\nstudy\nexplanation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAgain, all credit to the original authors for the above content at https://diataxis.fr/. You really ought to go read this in their words. (“Diátaxis” n.d.)"
  },
  {
    "objectID": "datasources.html",
    "href": "datasources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Figure 1: Prompt: Can you draw me an anime style-d picture of a computer wielding a machete?\nThere are two primary lenses I reach for when you start talking to me about data:\nTo address the first, I’ve spent a great deal of time learning, observing, teaching, relearning, and documenting my pathway towards reproducible and more durable knowledge and useful notes. Addressing the second requires a bit of a shift in perspective. Though it’s a common quote that “All models are wrong, some are useful” - George Box Box (1976), this doesn’t usually suffice when the ask-r has that favorite dataset or model they want to see used in an analysis, and while annoying there is (usually) less friction for all involved if you’ve done all the heavy lifting of functionalizing your analysis properly. In theory that should be as easy as swapping inputs…\nThat being said, there are diminishing returns on all of that effort. Some datasets are more authoritative or more widely used, and so there is value added in quantifying skill using that more consistent benchmark. More tangibly, some datasets are formatted and distributed using modern computational patterns and formats, and that ease of accessibility tips the scales heavily in favor of using that data, since the gravity and friction of that data access has been at least partially mitigated by the provider, meaning the limiting factor to scaling your domain is the size of your PC (or the credit limit on the card tied to your cloud budget).\nAn additional and very frustrating piece of friction I encounter almost daily is that the chasm between theory and practice has grown so large that the “head work” of swapping inputs and the “hand work” of actually implementing that pipeline can be quite significant, particularly when dealing with spatial scale. Below I’ve copied and modified the Google Earth Engine data catalog layout to more closely document and follow the different datasets I encounter and deploy. Starting with what I most frequently encounter…",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#a-preamble-setting-up-access-and-passwords",
    "href": "datasources.html#a-preamble-setting-up-access-and-passwords",
    "title": "Data Sources",
    "section": "A preamble: Setting up access and passwords",
    "text": "A preamble: Setting up access and passwords\nRelated to [[20241009121912]] Setting up ClimateR\nYou can thank Papa Bush for opening up the satellite archives to the general public, but open and free does not mean frictionless and several of the more authoritative data warehouses make you sign up for an account. Additionally, if you’d like to use the climateR package you’ll need to place credentials into a file so that you can hit these resources..\n\nI am a security disaster and the easiest way to make your life simple and ensure that anyone can use your account is to create the same login for all services. An “Ideal” password that works across all services is something shorter with a capital letter and a few numerics at the end.\n\n```{r}\nremotes::install_github(\"mikejohnson51/climateR\",force = TRUE)\nclimateR::writeNetrc(login = \"user\",password = \"Password123\")\nclimateR::checkNetrc()\nsites_stack &lt;- climateR::getGLDAS(AOI=AOI::aoi_get(state = \"OR\"),\n                        varname='rootmoist_inst',\n                        model='NOAH025_M.2.1',\n                        startDate='2006-10-01',\n                        endDate='2007-09-30',\n                        verbose = TRUE)\n```",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#water-specific",
    "href": "datasources.html#water-specific",
    "title": "Data Sources",
    "section": "Water Specific",
    "text": "Water Specific\n\nMany of these datasets are deployed in advancement of FIM workflows. Due to the topic-centric lens, some of these are also listed under their respective thematic subsections as well.\n\n\nMeasurements\n\nAHPS and NWPS\nThe National Weather Service Advanced Hydrologic Prediction Service (AHPS), newly superseded by the National Water Prediction Service (NWPS), is a series of gages and measurements of river stage and discharge collected by the USGS and NOAA, and are our primary means of accounting and managing our water resources and scale.\n\n\nBathymetery\n\n\nAltimitry\n\nICSAT\n\n\nSWOT\nhttps://storymaps.arcgis.com/stories/4a9184e813e540248040069580f6a54c\n\n\n\n\nModel data (inputs and outputs)\n\nHydrogaphy\nA term that technically translates to study of water, this is most commonly found in the context of the applied science of measurements of the ocean floor but is often conflated with the study and representation of the flow of water over the earths surface and its’ representation. When you encounter someone talking about hydrography, they likely actually mean…\n\n\nHydrofabric\nThe most technically appropriate term for the conceptual standard as defined by the OGC. (See my Hydrofabric data model Hydrofabric page] for more details. As a concept though, I like to think of “hydrofabric” as the critical differentiator that separates planetary hydrology from earth hydrology. In planetary hydrology, we know nothing about the system that we are trying to model beyond coarse remote measurements, a high quality DEM if we’re lucky. On earth however, we have the advantage of long durations of meaningful measurements and an implied understanding of the system beyond what “just physics” might tell us, so when we try to model our system, it’s useful to know critical areas that we want to represent, the standard flow paths and channels that usually flow, and how basins delineate themselves. The hydrofabric product does this for us. As I like to say, particularly at the national scale, “water flows down channel, not water flows downhill”.\nQuick links: Links to the official documentation, publication: (“OGC WaterML 2: Part 3 - Surface Hydrology Features (HY_Features) - Conceptual Model” n.d.), more information and the data bucket\n\n\n3D Hydrofabric\nIf hydrofabric is the line/polygon representation of the hydrography of a system, 3D Hydrography is intended to walk all the way back to a point cloud. This is a long running program that is still finding it’s footing, and is still an active area for community to debate and come to a concensus. Our take is here: [[20241016192617]] Hydrofabric3D Data Model\n\n\nBase Level Engineering\nSee the sample RRASSLER catalog here; and a STAC’d version of a generated catalog from publicly available RAS models (primarily sourced from FEMA Region 6) here. You can find Dewberries version of a HEC-RAS catalog based on efforts in (Lawler et al. 2025) here.\n\n\nThe National Water Model (NWM)\n [[NWM.qmd]] NWM.qmd The National Water Model is a framing and system designed to take atmospheric forcings and landscape states and produce a forecast of discharge for every stream reach in the system. It accomplishes all of this at continental scales and in operational time intervals (Operational for weather prediction at least, hourly predictions using 15 minute timestep-d inputs).\n\nBase data can be found at:\nOperational data can be found at:. FIM specific\n\nand accessed with:\n\nGeoserver: https://opengeo.ncep.noaa.gov/geoserver/www/index.html\n\n\nNWM retro data can be found at:\nNWM hydrofabric can be found at: https://lynker-spatial.s3.amazonaws.com/index.html\nTraining can be found at: https://www.meted.ucar.edu/education_training/lessons?query=National%20Water%20Model&page=1\n\nhttps://coast.noaa.gov/digitalcoast/\nhttps://www.weather.gov/media/wrn/calendar/NationalWaterModel.pdf\n\nNational Water Model retroactive forecasts\nhttps://github.com/NOAA-OWP/hydrotools/issues/157\n\n\nNexGen Water Model\nNextGen is a means of coupling modeling efforts and allowing for different representations of the hydrologic cycle to be able to talk to each other in different spatial extents. This framework allows many of the different types of model formulations to talk to one another while hiding the complexity associated with making those models talk to each other explicitly.\n[[20240507073634]] NWM forecast steps [[20240817132222]] Time within the NWM\n\n\n\n\nAuxillary and adjacent\nFIM adjacent data, in my mind, includes inundation maps whose objectives, uses, or derivation include forces outside the accurate prediction of inundation (since those values obfuscate and are unaccountable in the empirical or physical processes we are attempting to describe) ancillary asset data can help add context to the mapped outputs of a model.\n\nThe U.S. Flood Inundation Mapping Repository (USFIMR)\n\n\nThe USFIMR project commenced in August 2016 with funding from NOAA. The project’s main goal is to provide high-resolution inundation extent maps of past U.S. flood events to be used by scientists and practitioners for model calibration and flood susceptibility evaluation.\n\nBy hand digitizing and performing much of the requisite pre-processing, the US Flood Inundation Map Repository (USFIMR) database represents one of the most consolidated and published “ground truth” databases suitable as a means of performing a more consistent benchmarking of FIM results. To derive these, image classification techniques and a number of satellite sensors (e.g., Landsat, Sentinel-1, Sentinel-2) with some ground truthing based on secondary sources (e.g., news reports, social media) were combined with hand digitized water classifications to create bounds and ancillary data for an event. These maps have been widely used to evaluate FIM skill, and so even though the strict application of hydraulics was not used in the process, these maps have a high degree of skill and utility in that they have been used as the means of demonstrating process improvements from version to version of FIM libraries, and because a human (well-trained or in training graduate student) was used in the generation process the results are akin to a deeper dive into the area than an unsupervised classifier might apply.\n\n\nFlood Insurance Rate Maps\n\n\n\n_8d7e6bd7f9b0f52d74b007823461aa49.png\n\n\n\nBackground and interpretation\nAs a regulatory product, the use and goals of these maps and data are no longer parallel to those of the hydraulic modeler, but they can help add some context to the underlying values and objectives of the communities they govern. Many of these maps are the result of taking the model outputs for a 1:100 year storm and mapping the elevation as the Base Flood Elevation. Further refinement “based on stakeholder input” are taken into account and the resultant map is accepted as the defacto standard off which flood insurance rates are set and enforced. These maps can display many different areas, but the ones of most common consequence include:\n\nHigh-Risk Zones (A & V): These areas are delineate as having the potential to inundate at least once every 100 years, or a 1% annual chance of experiencing a flood event (Sometimes also termed as Annual Exceedance Probability). These maps are the result of the elevations calculated from hydraulic models and the “base flood” that defines it are the zones under which federally backed mortgages must have some form of flood insurance. Zone V is used in coastal zones to indicate the area is impacted by wave action in addition to fluvial flooding.\nModerate and Low-Risk Zones (X, B, C): These areas define locations which flood less frequently, and specific categories are called out on the FIRM legends. While not all are present for a given area, some of the more commonly found include Zone X (a 0.2% annual chance of flooding, once every 500 years), and medium to low risk zones such as B and C.\n\nA table of zone definitions aggregated from “FEMA Directory” (n.d.). See the glossary for more definitions.\n\n\n\n\n\n\n\nZone\nInterpretation\n\n\n\n\nA\nThe flood insurance rate zone that corresponds to the 100-year floodplains that are determined in the FIS by approximate methods. Because detailed hydraulic analyses are not performed for such areas, no base flood elevations or depths are shown within this zone.\n\n\nAE\nThe flood insurance rate zone that corresponds to the 100-year floodplains that are determined in the FIS by detailed methods. In most instances, whole-foot base flood elevations derived from the detailed hydraulic analyses are shown at selected intervals within this zone.\n\n\nAH\nThe flood insurance rate zone that corresponds to the 100-year shallow flooding (usually areas of ponding) where average depths are between 1 and 3 feet. Whole-foot base flood elevations are derived from detailed hydraulic analyses are shown at selected intervals within this zone.\n\n\nAR\nThe flood insurance rate zone used to depict areas protected from flood hazards by flood control structures, such as a levee, that are being restored. FEMA will consider using the Zone AR designation for a community if the flood protection system has been deemed restorable by a Federal agency in consultation with a local project sponsor; a minimum level of flood protection is still provided to the community by the system; and restoration of the flood protection system is scheduled to begin within a designated time period and in accordance with a progress plan negotiated between the community and FEMA. Mandatory purchase requirements for flood insurance will apply in Zone AR, but the rate will not exceed the rate for unnumbered A zones if the structure is built in compliance with Zone AR floodplain management regulations. For floodplain management in Zone AR areas, elevation is not required for improvements to existing structures. However, for new construction, the structure must be elevated (or floodproofed for non-residential structures) such that the lowest floor, including basement, is a maximum of 3 feet above the highest adjacent existing grade if the depth of the base flood elevation (BFE) does not exceed 5 feet at the proposed development site. For infill sites, rehabilitation of existing structures, or redevelopment of previously developed areas, there is a 3 foot elevation requirement regardless of the depth of the BFE at the project site. The Zone AR designation will be removed and the restored flood control system shown as providing protection from the 1% annual chance flood on the NFIP map upon completion of the restoration project and submittal of all the necessary data to FEMA.\n\n\nAO\nThe flood insurance rate zone that corresponds to the 100-year shallow flooding ( usually sheet flow on sloping terrain) where average depths are between 1 and 3 feet. Average whole-foot depths derived from the detailed hydraulic analyses. The highest top of curb elevation adjacent to the lowest adjacent grade (LAG) must be submitted if the request lies within this zone.\n\n\nA99\nThe flood insurance rate zone that corresponds to areas of the 100-year floodplain what will be protected by a Federal flood protection system where construction has reached specified statutory milestones. No base flood elevations or depths are shown within this zone.\n\n\nD\nThe flood insurance rate zone that corresponds to unstudied areas where flood hazards are undetermined but possible.\n\n\nGutter\nBoundary, shown on a Flood Insurance Rate Map, dividing Special Flood Hazard Areas of different Base Flood Elevations, base flood depths, flood velocities, or flood insurance risk zone designations.\n\n\nV\nThe flood insurance rate zone that corresponds to the 100-year coastal floodplains that have additional hazards associated with storm waves. Because approximate hydraulic analyses are performed for such areas, no base flood elevations (BFEs) are shown within this zone. Mandatory flood insurance purchase requirements apply.\n\n\nVE\nThe flood insurance rate zone that corresponds to the 100-year coastal floodplains that have additional hazards associated with storm waves. BFEs derived from the detailed hydraulic analyses are shown at selected intervals within this zone. Mandatory flood insurance purchase requirements apply.\n\n\nX\nThe flood insurance rate zone that corresponds to areas outside the 500-year floodplain, areas within the 500- year floodplain, and areas of 100-year flooding where average depths are less than 1 foot, areas of 100-year flooding where the contributing drainage area is less than 1 square mile, and areas protected from 100-year flood by levees. No base flood elevations or depths are shown within this zone.\n\n\nX (shaded and unshaded) B, and C\nThe flood insurance rate zones that corresponds to areas outside the 500-year floodplain, areas within the 500- year floodplain, and areas of 100-year flooding where average depths are less than 1 foot, areas of 100-year flooding where the contributing drainage area is less than 1 square mile, and areas protected from 100-year flood by levees. No base flood elevations or depths are shown within these zones.\n\n\n\n\n\n\n\n\n\nAcronym\n\n\nLongform\n\n\nDefinition\n\n\n\n\n\n\n\n\n500-Year Flood\n\n\nSee 0.2-Percent-Annual-Chance Flood\n\n\n\n\n\n\n50-Year Flood\n\n\nSee 2-Percent-Annual-Chance Flood.\n\n\n\n\n\n\nOrdinance Level A\n\n\nPer 44 CFR 60.3 (a), this ordinance level occurs when the Administrator has not defined the special flood hazard areas within a community, has not provided water surface elevation data, and has not provided sufficient data to identify the floodway or coastal high hazard are, but the community has indicated the presence of such hazards by submitting an application to participate in the Program.\n\n\n\n\n\n\nOrdinance Level B\n\n\nPer 44 CFR 60.3 (b), this ordinance level occurs when the Administrator has designated areas of special flood hazards (A zones) by the publication of a community’s FHBM or FIRM, but has neither produced water suface elevation data nor identified a floodway or coastal high hazard area.\n\n\n\n\n\n\nOrdinance Level C\n\n\nPer 44 CFR 60.3 (c), this ordinance level occurs when the Administrator has provided a notice of final flood elevations for one or more special flood hazard areas on the community’s FIRM and, if appropriate, has designated other special flood hazard areas without base flood elevations on the community’s FIRM, but has not identified a regulatory floodway or coastal high hazard area.\n\n\n\n\n\n\nOrdinance Level D\n\n\nPer 44 CFR 60.3 (d), this ordinance level occurs when the Administrator has provided a notice of final flood elevations within Zones A1-30 and/or AE on the community’s FIRM and, if appropriate, has designated AO zones, AH zones, A99 zones, and A zones on the community’s FIRM, and has provided data from which the community shall designate its regulatory floodway.\n\n\n\n\n\n\nOrdinance Level E\n\n\nPer 44 CFR 60.3 (e), this ordinance level occurs when the Administrator has provided a notice of final base flood elevations within Znes A1-30 and/or AE on the community’s FIRM and, if appropriate, has designated AH zones, AO zones, A99 zones, and A zones on the community’s FIRM, and has identified on the community’s FIRM coastal high hazard areas by designating Zones V1-30, and/or V.\n\n\n\n\n\n\nUnnumbered A Zone\n\n\nFlood insurance rate zones, designated “Zone A” on a FIRM, that are based on approximate studies.\n\n\n\n\n\n\nzonal classification\n\n\nClassification based data set value (ranges) and/or material properties or other attributes specified for polygonal regions as is common with GIS products.\n\n\n\n\nSFHA\n\n\nSpecial Flood Hazard Area\n\n\nThe area delineated on a National Flood Insurance Program map as being subject to inundation by the base flood. SFHAs are determined using statistical analyses of records of riverflow, storm tides, and rainfall; information obtained through consultation with a community; floodplain topographic surveys; and hydrologic and hydraulic analyses.\n\n\n\n\nSOMA\n\n\nSummary of Map Actions\n\n\nA list, generated by FEMA and delivered to the community, that summarizes the LOMAs, LOMR-Fs, and LOMRs that are or will be affected by a physical update to a FIRM.\n\n\n\n\nSOMA Category 1\n\n\nSOMA Category 1 - LOMRs and LOMAs Incorporated\n\n\nThe modifications effected by the LOMRs and LOMAs have been reflected on the Preliminary copies of the revised FIRM panels. However, these LOMRs and LOMAs will remain in effect until the revised FIRM becomes effective.\n\n\n\n\nSOMA Category 2\n\n\nSOMA Category 2 - LOMRs and LOMAs Not Incorporated (Revalidated)\n\n\nThe modifications effected by the LOMRs and LOMAs have not been reflected on the Preliminary copies of the revised FIRM panels because of scale limitations or because the LOMR or LOMA issued had determined that the lot(s) or structure(s) involved were outside the Special Flood Hazard Area, as shown on the FIRM. These LOMRs and LOMAs will be revalidated free of charge 1 day after the revised FIRM becomes effective through a single letter that reaffirms the validity of the previous LOMC.\n\n\n\n\nSOMA Category 3\n\n\nSOMA Category 3 - LOMRs and LOMAs Superseded\n\n\nThe modifications effected by the LOMRs and LOMAs have not been reflected on the Preliminary copies of the revised FIRM panels because they are being superseded by new detailed flood hazard information or the information available was not sufficient to make a determination. These LOMRs and LOMAs will no longer be in effect when the revised FIRM becomes effective.\n\n\n\n\nSOMA Category 4\n\n\nSOMA Category 4 - LOMRs and LOMAs To Be Redetermined\n\n\nThe LOMCs in Category 2 will be revalidated through a single letter that reaffirms the validity of the determination in the previously issued LOMC. For LOMCs issued for multiple lots or structures where the determination for one or more of the lots or structures has changed, the LOMC cannot be revalidated through this administrative process. The NSP will review the data previously submitted for the LOMR or LOMA request and issue a new determination for the affected properties after the effective date of the revised FIRM.\n\n\n\n\nZone A\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year floodplains that are determined in the FIS by approximate methods. Because detailed hydraulic analyses are not performed for such areas, no base flood elevations or depths are shown within this zone.\n\n\n\n\nZone A99\n\n\n\n\nThe flood insurance rate zone that corresponds to areas of the 100-year floodplain what will be protected by a Federal flood protection system where construction has reached specified statutory milestones. No base flood elevations or depths are shown within this zone.\n\n\n\n\nZone AE\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year floodplains that are determined in the FIS by detailed methods. In most instances, whole-foot base flood elevations derived from the detailed hydraulic analyses are shown at selected intervals within this zone.\n\n\n\n\nZone AH\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year shallow flooding (usually areas of ponding) where average depths are between 1 and 3 feet. Whole-foot base flood elevations are derived from detailed hydraulic analyses are shown at selected intervals within this zone.\n\n\n\n\nZone AO\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year shallow flooding ( usually sheet flow on sloping terrain) where average depths are between 1 and 3 feet. Average whole-foot depths derived from the detailed hydraulic analyses. The highest top of curb elevation adjacent to the lowest adjacent grade (LAG) must be submitted if the request lies within this zone.\n\n\n\n\nZone AR\n\n\n\n\nZone AR is the flood insurance rate zone used to depict areas protected from flood hazards by flood control structures, such as a levee, that are being restored. FEMA will consider using the Zone AR designation for a community if the flood protection system has been deemed restorable by a Federal agency in consultation with a local project sponsor; a minimum level of flood protection is still provided to the community by the system; and restoration of the flood protection system is scheduled to begin within a designated time period and in accordance with a progress plan negotiated between the community and FEMA. Mandatory purchase requirements for flood insurance will apply in Zone AR, but the rate will not exceed the rate for unnumbered A zones if the structure is built in compliance with Zone AR floodplain management regulations. For floodplain management in Zone AR areas, elevation is not required for improvements to existing structures. However, for new construction, the structure must be elevated (or floodproofed for non-residential structures) such that the lowest floor, including basement, is a maximum of 3 feet above the highest adjacent existing grade if the depth of the base flood elevation (BFE) does not exceed 5 feet at the proposed development site. For infill sites, rehabilitation of existing structures, or redevelopment of previously developed areas, there is a 3 foot elevation requirement regardless of the depth of the BFE at the project site. The Zone AR designation will be removed and the restored flood control system shown as providing protection from the 1% annual chance flood on the NFIP map upon completion of the restoration project and submittal of all the necessary data to FEMA.\n\n\n\n\nZone D\n\n\n\n\nThe flood insurance rate zone that corresponds to unstudied areas where flood hazards are undetermined but possible.\n\n\n\n\nZone Gutter\n\n\n\n\nBoundary, shown on a Flood Insurance Rate Map, dividing Special Flood Hazard Areas of different Base Flood Elevations, base flood depths, flood velocities, or flood insurance risk zone designations.\n\n\n\n\nZone V\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year costal floodplains that have additional hazards associated with storm waves. Because approximate hydraulic analyses are performed for such areas, no base flood elevations (BFEs) are shown within this zone. Mandatory flood insurance purchase requirements apply.\n\n\n\n\nZone VE\n\n\n\n\nZone VE is the flood insurance rate zone that corresponds to the 100-year coastal floodplains that have additional hazards associated with storm waves. BFEs derived from the detailed hydraulic analyses are shown at selected intervals within this zone. Mandatory flood insurance purchase requirements apply.\n\n\n\n\nZone X\n\n\n\n\nThe flood insurance rate zone that corresponds to areas outside the 500-year floodplain, areas within the 500- year floodplain, and areas of 100-year flooding where average depths are less than 1 foot, areas of 100-year flooding where the contributing drainage area is less than 1 square mile, and areas protected from 100-year flood by levees. No base flood elevations or depths are shown within this zone.\n\n\n\n\nZone X (shaded and unshaded) B, and C\n\n\n\n\nAre the flood insurance rate zones that corresponds to areas outside the 500-year floodplain, areas within the 500- year floodplain, and areas of 100-year flooding where average depths are less than 1 foot, areas of 100-year flooding where the contributing drainage area is less than 1 square mile, and areas protected from 100-year flood by levees. No base flood elevations or depths are shown within these zones.\n\n\n\n\n\n\n\n\nAside: Modifications to these maps\nIf you want to build in these zones or have a property reevaluated for flood insurance purposes, there are two primary ways to go about accomplishing that. You can either receive a Letter of Map amendment, or a Letter of map Revision. LOMA’s “A Letter of Map Amendment (LOMA) is an official amendment, by letter, to an effective NFIP map. A LOMA establishes a property’s location in relation to the SFHA.” A Letter of Map Revision (LOMR) “is an official revision, by letter, to an effective NFIP map. A LOMR may change flood insurance risk zones, floodplain and/or floodway boundary delineations, planimetric features, and/or BFE.”\n\n\nAside: Blending use cases\nAs I allude to, I do not classify FIRM’s as a FIM product. This is a function of both my relative “power user” status as a hydrologist and how these maps are used across the country. While FIRMS represent the “best available information” for a large swath of the country, there are locations which have neither product, there are locations which have both standard BLE models and FIRMS, and there are locations which have heavy invested in custom hydraulic modeling. None of these different data products will align perfectly. Additionally, much like all “federal” regulations, these set that minimum baseline, but states and counties may opt to adopt more rigorous standards. The standard “better practice” as used now is to compare both and use whichever has the most extent as an informed recommendation. It’s important to note however, that the FIRM extent takes legal precedent in enforcement, even if an alternative model shows greater extents.\n\n\nAside: Disaster recovery and payouts\nOne of the commonly used metrics in the use of these flood insurance programs and the National Flood Insurance Program is the 50% rule. If a building is flooded, insurance payouts can occur in a few different ways. If a building is historic, the renovations may take place but for every dollar spent, a match in flood prevention measures must take place. If the building is not historic and is more than 50% destroyed, renovations must also include bringing the structure back up to code. Enforcement and interpretation is up to the states, FEMA, flood insurance providers, and the lawyers. I just like making the maps and designing the structures.\nSources: https://sarasota.wateratlas.usf.edu/upload/documents/FloodplainFacts.pdf https://www.fema.gov/pdf/nfip/manual201205/content/16_maps.pdf https://adeca.alabama.gov/faq/what-are-the-different-types-of-letters-of-map-change-lomcs/#:~:text=LOMA%20%E2%80%93%20A%20Letter%20of%20Map,to%20an%20effective%20NFIP%20map.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#climate-and-weather",
    "href": "datasources.html#climate-and-weather",
    "title": "Data Sources",
    "section": "Climate and Weather",
    "text": "Climate and Weather\n\nSurface Temperature\nThermal satellite sensors can provide surface temperature and emissivity information. The Earth Engine data catalog includes both land and sea surface temperature products derived from several spacecraft sensors, including MODIS, ASTER, and AVHRR, in addition to raw Landsat thermal data.\n\n\nClimate\nClimate models generate both long-term climate predictions and historical interpolations of surface variables. The Earth Engine catalog includes historical reanalysis data from NCEP/NCAR, gridded meteorological datasets like NLDAS-2, and GridMET, and climate model outputs like the University of Idaho MACAv2-METDATA and the NASA Earth Exchange’s Downscaled Climate Projections.\n\n\nAtmospheric\nYou can use atmospheric data to help correct image data from other sensors, or you can study it in its own right. The Earth Engine catalog includes atmospheric datasets such as ozone data from NASA’s TOMS and OMI instruments and the MODIS Monthly Gridded Atmospheric Product.\n\n\nPMP\nFrom NOAA https://hdsc.nws.noaa.gov/pfds/pfds_map_cont.html: https://hdsc.nws.noaa.gov/cgi-bin/new/fe_text.csv?lat=38.0000&lon=-95.0000&data=depth&units=english&series=pds&selAddr=Bronson,%20Kansas,%20USA&selElevNum=1039&selElevSym=ft&selStaName=-\n\n\nWeather\nWeather datasets describe forecasted and measured conditions over short periods of time, including precipitation, temperature, humidity, and wind, and other variables. Earth Engine includes forecast data from NOAA’s Global Forecast System (GFS) and the NCEP Climate Forecast System (CFSv2), as well as sensor data from sources like the Tropical Rainfall Measuring Mission (TRMM).",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#imagery",
    "href": "datasources.html#imagery",
    "title": "Data Sources",
    "section": "Imagery",
    "text": "Imagery\n\nLandsat\nLandsat, a joint program of the USGS and NASA, has been observing the Earth continuously from 1972 through the present day. Today the Landsat satellites image the entire Earth’s surface at a 30-meter resolution about once every two weeks, including multispectral and thermal data.\n\n\nSentinel\nThe Copernicus Program is an ambitious initiative headed by the European Commission in partnership with the European Space Agency (ESA). The Sentinels include all-weather radar images from Sentinel-1A and -1B, high-resolution optical images from Sentinel 2A and 2B, as well as ocean and land data suitable for environmental and climate monitoring from Sentinel 3.\n\n\nMODIS\nThe Moderate Resolution Imaging Spectroradiometer (MODIS) sensors on NASA’s Terra and Aqua satellites have been acquiring images of the Earth daily since 1999, including daily imagery, 16-day BRDF-adjusted surface reflectance, and derived products such as vegetation indices and snow cover.\n\n\nHigh-Resolution Imagery\nHigh-resolution imagery captures the finer details of landscapes and urban environments. The US National Agriculture Imagery Program (NAIP) offers aerial image data of the US at one-meter resolution, including nearly complete coverage every several years since 2003.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#geophysical",
    "href": "datasources.html#geophysical",
    "title": "Data Sources",
    "section": "Geophysical",
    "text": "Geophysical\n\nTerrain\nDigital Elevation Models (DEMs) describe the shape of Earth’s terrain. The Earth Engine data catalog contains several global DEMs such as Shuttle Radar Topography Mission (SRTM) data at 30-meter resolution, regional DEMs at higher resolutions, and derived products such as the WWF’s HydroSHEDS hydrology database.\n\n3DEP\nhttps://usgs.entwine.io/, the footprints live here.\nhttp(s)://&lt;bucket&gt;.s3.amazonaws.com/&lt;object&gt;\nhttp(s)://s3.amazonaws.com/&lt;bucket&gt;/&lt;object&gt;\nFrom the URL in /aws s3 sync s3://usgs-lidar-public/USGS_LPC_NY_Sandy_Ul_Du_Or_2013_LAS_2016/\nRelated: [[CA_Brushes.qmd]] CA_Brushes.qmd\n\n\n\nLand Cover\nLand cover maps describe the physical landscape in terms of land cover classes such as forest, grassland, and water. Earth Engine includes a wide variety of land cover datasets, from near real-time Dynamic World to global products such as ESA World Cover.\n\n\nCoastal Land Cover\n\n\nC-CAP High Resolution Land Cover\nhttps://coast.noaa.gov/digitalcoast/data/ccaphighres.html A 1 to 2.4 meter land cover classification dataset that has coverage of the coasts and includes\nThe new 1-meter products are useful at the local level in ways not previously possible with national-level data. Advanced artificial intelligence combined with expert human analysis, review, and editing are used to produce these high-quality, standardized, raster-based map products.\n\n\nCropland\nCropland data is key to understanding global water consumption and agricultural production. Earth Engine includes a number of cropland data products such as the USDA NASS Cropland Data Layers, as well as layers from the Global Food Security-Support Analysis Data (GFSAD) including cropland extent, crop dominance, and watering sources.\n\n\nVector data\nData from other satellite image sensors is available in Earth Engine as well, including night-time imagery from the Defense Meteorological Satellite Program’s Operational Linescan System (DMSP-OLS), which has collected imagery of night-time lights at approximately 1-kilometer resolution continuously since 1992.\n\n\nOther Geophysical Data\nData from other satellite image sensors is available in Earth Engine as well, including night-time imagery from the Defense Meteorological Satellite Program’s Operational Linescan System (DMSP-OLS), which has collected imagery of night-time lights at approximately 1-kilometer resolution continuously since 1992.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#administrative-and",
    "href": "datasources.html#administrative-and",
    "title": "Data Sources",
    "section": "Administrative and",
    "text": "Administrative and\nftp://ftp2.census.gov/geo/tiger/\n\nOther\nOpen Source GIS Data, specifically the state data index based on efforts from MappingSupport Cartographic line data: http://www.projectlinework.org/\nBase imagery: https://www.naturalearthdata.com/downloads/10m-raster-data/\nEasy SRTM tiles: http://dwtkns.com/srtm30m/ Topo resources: http://www.earthpoint.us/TopoMap.aspx\nShaded relief data: http://www.shadedreliefarchive.com/ Water resources: https://www.hydroshare.org/landingPage/\nEarth as art: https://eros.usgs.gov/image-gallery/earth-as-art https://stn.wim.usgs.gov/FEV/#2022Ian Weather shmorgasbord: * Forecasts: * https://weather.cod.edu/forecast/ * https://www.weather.gov/gis/PublicZones https://www.usgs.gov/mission-areas/water-resources/science/usgs-flood-information#web-tools’; https://datasetsearch.research.google.com/ https://www.google.com/publicdata/directory https://communitycrimemap.com/\n\nPolaris\nhttp://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/Readme\n\n\n\n\n\n\n\n\nVariable\nDescription\nUnits\n\n\n\n\nsilt\nSilt percentage\n%\n\n\nsand\nSand Percentage\n%\n\n\nclay\nClay Percentage\n%\n\n\nbd\nBulk Density\ng/cm3\n\n\ntheta_s\nSaturated Soil Water Content\nm3/m3\n\n\ntheta_r\nResidual Soil Water Content\nm3/m3\n\n\nksat\nSaturated Hydraulic Conductivity\nlog10(cm/hr)\n\n\nph\nSoil pH in H2O\n\n\n\nom\nOrganic Matter\nlog10(%)\n\n\nlambda\nPore Size Distribution Index (Brooks-Corey)\n\n\n\nhb\nBubbling Pressure (Brooks-Corey)\nlog10(kPa)\n\n\nn\nMeasure of the pore size distribution (Van Genuchten)\n\n\n\nalpha\nScale parameter inversely proportional to mean pore diameter (Van Genuchten)\nlog10(kPa-1)\n\n\n\nAcross 6 depth ranges (0-5 cm, 5-15 cm, 15-30 cm, 30-60 cm, 60-100 cm, and 100-200 cm) and with (5th, Arithmetic mean, Mode, Median, and 95th percentiles) statistical reporting.\n**\n\n\n\nClimateR\nSee [[20241009121912]] Setting up ClimateR\n\n\nGage data",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#nwis",
    "href": "datasources.html#nwis",
    "title": "Data Sources",
    "section": "NWIS",
    "text": "NWIS",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#ahps",
    "href": "datasources.html#ahps",
    "title": "Data Sources",
    "section": "AHPS",
    "text": "AHPS",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#imerg",
    "href": "datasources.html#imerg",
    "title": "Data Sources",
    "section": "IMERG",
    "text": "IMERG\nIMERG Known anomalies include:\n* IMERG overestimates light precip, but at monthly scales this washes out, that added “fake drizzle” accommodates for accumulation values measured at the monthly scale\n* Coastal boundaries are blocky and artifact-d\n* Processing errors reduce input scenes * “Flashy” precip from two sources: * The GPROF retrievals, and the subsequent IMERG calibrations for them, are not entirely consistent between sensors. Accordingly, users will observe “flashing” in the precipitation fields as successive overpasses in a particular location are populated by different sensors. * The IR estimates sometimes exhibit “flashing” because successive images in a given region are successively populated with data from different geo-IR satellites, usually with one having a near-nadir view, but then dropping out and being replaced by high-zenith-angle data from an adjacent geo-IR satellite. * Sensor drift: The mix of satellites has changed over time, which affects the overall performance of the algorithm in two ways. First, the relative weighting of conical-scan imagers versus crosstrack-scan sounders shifts, and second, the relative proportion of IR-based estimates changes. The PMW sensor inventory is shown in “sensors contributing to IMERG”. See Behrangi et al. (2014) for more discussion of sensor performance for legacy algorithms. * and a whole host of outages as outlined below:",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "datasources.html#infrastructure",
    "href": "datasources.html#infrastructure",
    "title": "Data Sources",
    "section": "Infrastructure",
    "text": "Infrastructure\n\nNational Bridge Inventory\nThe National Bridge Inventory (NBI) serves as the authoritative data source for the location, description, classification, and condition of the critical bridge infrastructure used for inventory and legislative purposes. Maintained by the Federal Highway Administration (FHWA), this extensive dataset encompasses information on over 735,000 bridges along public roads, including interstate highways, U.S. highways, state and county roads, and publicly accessible bridges on federal and tribal lands. This data can be accessed (in ASCII form) here. See how these are being transformed to hydrolocations for the hydrofabric here.\n\n\n\n\n\n\n\n\n\n\nRelated: Historic Bridges .org\n“HistoricBridges.org offers professional detail-oriented photo-documentation, information, and preservation advocacy for all types of historic bridges except covered bridges. HistoricBridges.org is authored by Nathan Holth.”\n\n\n\nElectricity Use\nhttps://github.com/mdbartos/RIPS - Electric",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Sources"
    ]
  },
  {
    "objectID": "data_realizations.html",
    "href": "data_realizations.html",
    "title": "Data Realizations",
    "section": "",
    "text": "_33bf4331091f232313c631a9a493f818.png",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows"
    ]
  },
  {
    "objectID": "data_realizations.html#the-analog-world-is-difficult-to-express-in-digital-terms-but-well-try-to-anyways",
    "href": "data_realizations.html#the-analog-world-is-difficult-to-express-in-digital-terms-but-well-try-to-anyways",
    "title": "Data Realizations",
    "section": "The analog world is difficult to express in digital terms, but we’ll try to anyways",
    "text": "The analog world is difficult to express in digital terms, but we’ll try to anyways\nPart of data science is representing the real world with 1’s and 0’s. This is a challenging proposition as representing “reality” using two bits of information and then rendering that on what is typically a two-dimensional screen is a process fraught with theoretical simplifications, generalizations, and assumptions. While it might be common to reduce the types of data into two classes: vector and raster, I find it useful to pull vector apart a bit more. First and most tangibly, point clouds and points are distribution in different formats and represent different ends of the data size spectrum. While you might just have a handful of points represented in a csv, it is not uncommon to need to represent millions of points with an X,Y,Z, time, reflectance values, and a host of other metadata that are needed to efficiently interact with large scale measurements of the world coming from LIDAR. Meshes such as TIN’s area also a special case of vector data in that the connections of the different points are more easily represented in a TIN structure vs loose polygons, and the way in which software and the underlying mathematical models interact with the data make meshes a more appropriate form to convert the computational grid over their more generic vector alternatives. Therefore, when I talk about data types, I am typically referencing or conceptualizing the following forms. Don’t forget to check out FILE I/O if you are looking for tips on how to open and manipulate these.\n\nPoint Clouds: How we measure much of the world.\nMeshes: How we (typically) model the world.\nVectors: (small scale) points, lines, and polygons.\nRasters: irregular and regularly gridded representations.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows"
    ]
  },
  {
    "objectID": "critical_think.html",
    "href": "critical_think.html",
    "title": "Critical spatial thinking",
    "section": "",
    "text": "Map of the several nations of Indians to the Northwest of South Carolina, from https://www.loc.gov/resource/g3860.ct000734/, Satellite imagery from Google Earth, and radar from NOAA precipitation Map\n\n\n\nDefining “critical spatial thinking”\n\nThe following thoughts were funded in part by the Haskell Environmental Research Studies (HERS) 2020 program\n\nThe ability to think critically is widely considered to be a key facet of intelligence and a desirable cognitive capability. However, critical thinking, and by extension, critical spatial thinking, eludes a strict definition and often harkens back to the colloquial “I know it when I see it” phrase. Furthermore, efforts to create a standardized means of measuring one’s ability to think critically are extremely limited. This vagueness creates substantial friction when attempting to design, deliver, and assess the efficacy of a curriculum centered around the cultivation of these skills. Therefore, with the goal of improving spatial critical thinking skills of incoming graduate students in preparation to succeed in fields related to Science, Technology, Engineering, and Math, it is first worthwhile to define what precisely critical spatial thinking is.\nThere have been innumerable attempts to define what critical thinking is; one need only look to Wikipedia to find no less than 10 proffered definitions in use (“Critical Thinking” 2023). Most succinctly, critical thinking is the process of analyzing facts to form a judgment. Most definitions of critical thinking also encompass communication in some form; a recognition that it is often not enough to be able to reach a conclusion, one must be able to articulate how and why they arrive at that result. Finally, this process is ideally a self-directed, self-disciplined, self-monitored, and self-correcting one; critical thinking is ultimately something that occurs at the scale of an individual and is an ongoing process which updates as new information is gained (Clarke 2019).\nIt would be a logical step to then say that critical spatial thinking is a subset of critical thinking with special attention paid to space. In perhaps the most authoritative and often cited body of work, the National Academics Press “Learning to Think Spatially” defines spatial thinking thusly (Learning to Think Spatially: GIS as a Support System in the K-12 Curriculum 2006):\n\nA constructive amalgam of three elements: concepts of space, tools of representation, and processes of reasoning. Space provides the conceptual and analytical framework within which data can be integrated, related, and structured into a whole. Representations—either internal and cognitive or external and graphic, linguistic, physical, and so forth—provide the forms within which structured information can be stored, analyzed, comprehended, and communicated to others. Reasoning processes provide the means of manipulating, interpreting, and explaining the structured information.\n\nThis definition is purposely broad in a recognition that, although spatial thinking is a universal mode of thought, its manifestation often takes a distinct form depending on the practitioner’s discipline or the perspective with which they approach the problem. For instance, mind maps or organizational charts are a manifestation of spatial thinking not immediately obvious, or perhaps pertinent to a spatial curriculum in its modern academic form but are nonetheless a mode of thought dominated with concerns for space.\nA more appropriate definition of spatial thinking to our original inquiry, presented by Penn State’s “The Learner’s Guide to Geospatial Analysis” defines spatial thinking in terms of attributes a practitioner exhibit (“The Learner’s Guide to Geospatial Analysis (V1.1)  The Learner’s Guide to Geospatial Analysis” n.d.):\n\nAn expert spatial thinker visualizes relations, imagines transformations from one scale to another, mentally rotates an object to look at its other sides, creates a new viewing angle or perspective, and remembers images in places and spaces.\n\nThis definition provides some insight into the desirable characteristics of a spatial thinker but lacks any structure of thought. It also woefully underrepresents the depth spatial skills may take. In the pursuit of summarizing these into a concise table, Lee and Bednarz performed a longitudinal literature review and identified common concepts of spatial thinkers from some of the most prominent spatial thinkers of the time (Lee and Bednarz 2012). Their findings, reproduced below, show several commonalities. More comprehensive efforts to identify these core components (sometimes referred to as modes, or competencies), was undertaken by Zwartjes et al in 2017 (Zwartjes, n.d.).\n\n\n\n\n\n\n\n\nGersmehl and Gersmehl (2007)\nGolledge et al. (2008)\nJanelle and Goodchild (2009)\n\n\n\n\nCondition\nIdentity\nObjects and Fields\n\n\nLocation\nLocation\nLocation\n\n\nConnection\nConnectivity\nNetwork\n\n\n\nDistance\nDistance\n\n\n\nScale\nScale\n\n\nComparison\nPattern Matching\n\n\n\nAura\nBuffer\n\n\n\nRegion\nAdjacency, Classification\nNeighborhood and Region\n\n\nHierarchy\n\n\n\n\nTransition\nGradient, Profile\n\n\n\nAnalogy\n\n\n\n\n\nCoordinate\n\n\n\nPattern\nPattern, Arrangement, Distribution, Order, Sequence\n\n\n\nSpatial Association\nSpatial Association, Overlay/Dissolve, Interpolation\nSpatial Dependence, Spatial Heterogeneity\n\n\n\nProjection, Transformation\n\n\n\n\nA reproduction of Table 1 from (Lee and Bednarz 2012), demonstrating the formulation and overlap between three prominent realizations of the core concepts of spatial thinking.\n\n\nRedefining Critical Spatial Thinking\nThese are all far greater thinkers than myself, and it seems a bit presumptuous that anyone should care to read my thoughts on the topic, but here we are. These incomplete definitions of spatial critical thinking and the various iterations of core concepts that lack a standardized set of competencies to refer to are the result of common shortfalls. The first is the rigid nature of classification itself. As with many anthropogenic and natural phenomenon, it is not immediately apparent that the nature of spatial thinking is one that can be distilled into finite, functional, and independent categories. Often these classifications become a semantical exercise far removed from any practical application. The second is the fluid nature of the discipline and its place in academia. The department of geography – derived from the Greek word geographia and literally translated as “to describe or write about the world” – has undergone several major restructurings and can these days be found alongside environmental science, computer science, geoscience, urban planning, and anthropology, just to name a few (AAG 2019).\nThe most recent of these trends has followed the rise of the computer, and with it the appearance of a new subdivision of geography: geographic information science. I’ll touch on the role of this branch in the next section, but regardless of the role it plays, its introduction has been as paradigm shifting to the discipline as the computer itself has been to humanity. This digital arm of geography wreaked havoc on the strict categorization of the geographic discipline, sometimes lumping what would traditionally fall under computer science into geography or fracturing these concepts into standalone geoinformatics departments. This digital revolution also facilitated time-space compression, allowing a quantitative treatment of a wide range of spatio-temporal phenomenon and more prominently accentuating the intertwined nature of space and time (Warf 2013). Given these trends and the most recent categorization and organization of GIScience within the academic domain, the time appears ripe to offer up my own take on a functional definition of critical spatial thinking:\n\nCritical spatial thinking is the ability to observe and form a query; place that query in the context of the core spatial concepts; devise an appropriate means of testing a hypothesis; executing that test accurately; communicate those results in textual, verbal or visual form; and objectively reflect on that process.\n\nThis definition blends portions of the previously cited critical spatial thinking definitions and the scientific method as a means of structuring the process of thinking and injecting pathways of self-reflection and correction into the process.\nAlongside this new definition, I propose a new set of spatio-temporal skills, based on the modern state of the geographic discipline (newly bound to computation and with explicit consideration of the temporal) and a refactoring of the above skills into distinct categories and hierarchies, the existence of which was originally proposed by Perdue et al. as shown in Figure 2A (“Dr. Amy Lobben  S3C” n.d.). These hierarchies are a recognition that some of these “fundamental” spatial skills are assemblages of other spatial skills (e.g. In identifying the change of an area over time, individuals start with a perception of the phenomenon and identifying boundaries and the proximity and classification of that space. They then need to cluster that space into distinct regions, and place that region along the continuum of its history, Figure 2B.)\n\n\n\n\n\n\n\n\na8621ec34b08925794b2e91ec4b24082.png\n\n\n\n\n\n\n\n0f92fe1d47d9692cc2a89749ac863368.png\n\n\n\n\n\n\nFigure 1: (left): a conceptual diagram of spatial skills arranged based on primitive hierarchical order and separated by the treatment of the phenomenon as either space as the primary attribute or as a distinguishable element in space, with the temporal dimension capping that array in red to distinguish the difference between dimensionality (right) an example of how these skills build to complex skills and recognition of phenomena.\n\n\n\nWhile there are many concepts which appear self-evident, a few of these more notable ones include:\n\nIdentity: One of the core concepts consistent across the three reviews, this has to do with the nature of the entity in question. Goodchild frames this in terms of GIS, calling them object and fields, or the attributes that that object possesses (Goodchild and Janelle 2010). These attributes are typically consistent across a set but can also take on a parent-child relationship as with object-oriented programming.\nLocation: Another one of the core concepts consistent across the three reviews, this concept pertains to how one might attempt to define a location. Although at first glance one might assume that location is an attribute of identity, there is no single frame off which to determine how one might locate a point in space. See Figure 3, a reproduction of Figure 1 from (Gersmehl and Gersmehl 2007), for an excellent example of this concept.\nNetwork: A concept identical in two of the three realizations above, network refers to the topological relationship and connectivity (or lack thereof) between objects and is dependent on the “mode” of transportation (e.g. via car or by foot).\nDistance: Another concept identical in two of the three realizations above, distance refers to the quantization of space, and is always relative with respect to location and origin.\nScale: Another concept identical in two of the three realizations above, here scale generally refers to the scale of thought. As quoted from Golledge, Marsh, and Battersby (2008):\n\n\nMontello (1993) (Montello (1993) argued that four scales of spatial thinking can be determined. The first (micro or body scale) covers the scale of nanotechnology, brain cell analysis, and spatial examination from the microscopic to the arrangement of parts of the body. Figural scale covers the immediate vicinity of the body, extending to the edge of the tactile (reach) domain. Environmental scale includes the environment that can be visually perceived, usually from single viewpoints, and represents the space of everyday bodily activity. Geographic scale includes areas and places that cannot be perceived from a single viewpoint, including occluded and distant areas.\n\nAlthough we could certainly debate the precise definition of these breaks in scale, the concept remains essential to spatial thought more than 25 years later.\n\nPattern: A concept spread across both lists, this concept is the process of identifying order, repetition, distribution, and hierarchy of space and objects.\nScope: A concept rephrased across the three representations, and further refined here, I take neighborhood, region, adjacency, and spatial association to refer to the same concept, the scope of the operation. Most famously ascribed by Tomlin (Tomlin 2013), within a Geographic Information System operations can be local, focal, zonal, or global and are all used to define the nature of the window of operation.\nTransformation: Here I refer to as the concept of manipulating, rotating, and warping an object in 2 or three dimensions.\nRepresentation: Here I refer to the ability to visualize and represent these objects in mental, visual, or digital space. Adding representation as a core concept neatly categorizes map reading and making, as well as categorization and classification, which were omitted or under-represented in the previous iterations of core concepts.\nLineage: A new concept not included in the above, and not in the strictest sense relevant to spatial, it would be a missed opportunity to reformulate the core spatial concepts without expanding them to include time. Indeed, this temporal aspect was alluded to in Zwatjes et al.’s treatment of the core spatial concepts (Zwartjes et al., n.d.). Spatio-temporal phenomena (think ENSO oscillations, land use history, ect.) have recently seen an explosion of interest due in part to the increased length of earth observation systems. Here the word lineage is used both to encapsulate the ideas of vector travel path (cars have a location, direction, and momentum), and the use of space (this used to be a mall and is now a distribution facility).\n\nWhat is notable but not explicitly stated in these definitions is that they all hinge on the assumption that the thinker has some foundational knowledge and is spatially literate. While not strict prerequisite, it is difficult to be able to think critically without having some base with which to compare new information to; and the ability to think critically without being able to ingest spatial information (spatial literacy), is an exceedingly rare occurrence (Patterson, Reeve, and Page 2003). Therefore, if the goal is to cultivate critical spatial thinking skills, individuals must develop some foundation in spatial knowledge and spatial literacy. In that regard, GIS has a recognized capacity to facilitate that acquisition (Abbott, n.d.).\n\n\nDefining GIS in the Context of Spatial Thinking\nBefore attempting to describe how GIS can be used to teach spatial thinking however, it is worth dissecting what the acronym GIS actually stands for. There are generally two interpretations for this acronym: Geographic Information Systems, and Geographic Information Science. Although rarely taught as such, Geographic Information Science – the scientific discipline that studies the techniques to capture, represent, process, and analyze geographic information – has quite a lot in common with data science, which itself is a subset of computer science. GISystems are a realization of that science in the form of a tool. This spectrum between tool, toolmaking, and science was first vocalized by Goodchild, who notes that these borders are inherently fuzzy. In conceptualizing it this way, we also imply that there is an endpoint in the spectrum; a final destination after which there is no real purpose in advancing critical spatial thinking skills. While it is true that there are diminishing effects to advancement, learning is a lifelong journey and not simply a destination, and thinking is an activity which should be performed on a daily basis. Hereon in, I’ll differentiate between the two when applicable, but the scope of this report is largely concerned with discussing the use of GISystems Wright, Goodchild, and Proctor (1997).\n\n\nThe Role of GIS in Developing Spatial Literacy and Critical Thinking Skills\nSpatial literacy has received comparably less interest within the educational domain than its reading, writing, and mathematical counterparts. As Goodchild and Janelle point out, this may be the result of the instinctive nature of spatial intelligence; spatial awareness was key to survival from an evolutionary standpoint and is presumed to be acquired at such an early age that it requires no attention from the education system (Goodchild and Janelle 2010). This is contrasted however, by the increasing lack of spatially literate citizens. To address these deficiencies, and to meet marketplace demands, many educational programs have turned to teaching GISystems in their curricula. But how GIS can best be used to teach critical spatial thinking skills is still debatable. Given the scope, time constraints, vagueness, and lack of research addressing this question, particularly at the collegiate level, I draw much of this section from anecdotal evidence and experience.\nWhile there is no standardized curriculum design to teach GIScience, there are two primary means of introducing GIS to students. One can take a graphical user interface (GUI) based approach to introducing GIS or take a more programmatic approach. GUI GIS includes programs such as Google Maps and Earth, QGIS and ESRI Arc products, whereas programmatic approaches to GIS include Python, R, JavaScript, and Matlab. There are tradeoffs to each, and it’s worth peeling them apart.\nAnything built in a digital system is inherently less free and less flexible than we can conceptualize internally. These limitations are not necessarily even imposed by hardware, they can often simply be too unwieldy to implement in a digital environment. Likewise, a GUI driven implementation will always have fewer degrees of freedom than a programmatic implementation. This loss of flexibility is further compounded by the nature of a GUI driven system in general; one cannot expose too many of the underlying parameters without overwhelming the end user. Finally, although not limited to GUI programs, many of the more advanced GUI driven systems must be paid for, creating additional barriers of access and raising ethical and moral issues related to academia teaching a private company’s software and building dependence on computational crutches.\nProgrammatic approaches, in contrast, require the ability to program which takes quite a while to build competency in and progress is not particularly obvious when first starting. Furthermore, it is often harder to find motivation in programming, whereas GUI driven systems provide immediate feedback and a tangible goal to visualize. I find it far easier to inspire the desire to program when individuals have a self-developed goal in mind; the learning will follow. However, as a skilled GIScientist must learn to program eventually, and in that respect, a programmatic approach to learning GIS will take time in the long run. A strictly programmatic approach also foregoes the process of learning common interface accesses paths of a GIS GUI, but such things are trivially straightforward to learn and by the time one has acquired the ability to think critically, they also likely possess the capabilities for self-directed learning.\nThese differences in approach are summarized in the generalized conceptual diagram shown in Figure 4. Users who start on the GUI driven track rapidly acquire skills in spatial literacy, but progress quickly plateaus while acquiring computational skills. This is due in parts to the abstraction GUI driven programs provide over the complexities of the computational domain, and the obfuscation these platforms impose by obscuring parameters and limitations of GUI platforms to design custom tools. Therefore, these users need to spend additional time learning how to program, and in some cases unlearning poor habits that GUI driven tools can create. Eventually, users overcome this learning curve, and move on to become critically spatially literate. In contrast, users who start learning GIS programmatically have a much slower rise to spatial literacy as they overcome the early hurdles associated with programming and visualization, and consequently acquire spatial literacy later than their GUI taught counterparts. However, they have none of the later learning curve, and rapidly transition from spatial literacy and using GIS as a tool to toolmaking. Consequently, they acquire critical spatial and data literacy somewhat sooner.\n\n\n\nGISConceptDiagram.png\n\n\nOne more critical aspect left unaddressed between these two approaches to teaching GISystems is the retention rates and success (in terms of the number of students who matriculate and go on to practice “good” GIScience). To provide the most accessible experience and archive possible, this site and the classes within will include approaches to labs using multiple different approaches ranging from proprietary, closed source industry standards like ESRI (ArcMap and ArcPro) and cloud native solutions (Google Earth Engine) as well as Free and Open Source alternative including QGIS, Python, R, and other toolings from across the FOSS community. Although I obviously hope you become a seasoned GIScience practitioner, my goal is that you leave with a deeper appreciation and understanding for the nuances of spatial analysis and phenomenon, and that you gain practical problem solving skills you can deploy in your own careers.\n\n\n\n\n\nReferences\n\nAAG. 2019. “Guide to Geography Programs in the Americas.”\n\n\nAbbott, Thomas Diamond. n.d. “An Evaluation of a Geographic Information System Software and Its Utility in Promoting the Use of Integrated Process Skills in Secondary Students,” 149.\n\n\nClarke, John. 2019. Critical Dialogues: Thinking Together in Turbulent Times. 1st ed. Policy Press. https://doi.org/10.2307/j.ctvk3gkvq.\n\n\n“Critical Thinking.” 2023. Wikipedia, October.\n\n\n“Dr. Amy Lobben  S3C.” n.d. https://blogs.uoregon.edu/s3clab/2018/11/16/amy-lobben/. Accessed October 9, 2023.\n\n\nGersmehl, Philip J., and Carol A. Gersmehl. 2007. “Spatial Thinking by Young Children: Neurologic Evidence for Early Development and ‘Educability’.” Journal of Geography 106 (5): 181–91. https://doi.org/10.1080/00221340701809108.\n\n\nGolledge, Reginald G., Meredith Marsh, and Sarah Battersby. 2008. “Matching Geospatial Concepts with Geographic Educational Needs.” Geographical Research 46 (1): 85–98. https://doi.org/10.1111/j.1745-5871.2007.00494.x.\n\n\nGoodchild, Michael F., and Donald G. Janelle. 2010. “Toward Critical Spatial Thinking in the Social Sciences and Humanities.” GeoJournal 75 (1): 3–13. https://doi.org/10.1007/s10708-010-9340-3.\n\n\nLearning to Think Spatially: GIS as a Support System in the K-12 Curriculum. 2006. Washington, D.C.: National Academies Press. https://doi.org/10.17226/11019.\n\n\nLee, Jongwon, and Robert Bednarz. 2012. “Components of Spatial Thinking: Evidence from a Spatial Thinking Ability Test.” Journal of Geography 111 (1): 15–26. https://doi.org/10.1080/00221341.2011.583262.\n\n\nMontello, Daniel R. 1993. “Scale and Multiple Psychologies of Space.” In Spatial Information Theory A Theoretical Basis for GIS, edited by Gerhard Goos, Juris Hartmanis, Andrew U. Frank, and Irene Campari, 716:312–21. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-57207-4_21.\n\n\nPatterson, Mark W., Kay Reeve, and Dan Page. 2003. “Integrating Geographic Information Systems into the Secondary Curricula.” Journal of Geography 102 (6): 275–81. https://doi.org/10.1080/00221340308978559.\n\n\n“The Learner’s Guide to Geospatial Analysis (V1.1)  The Learner’s Guide to Geospatial Analysis.” n.d. https://www.e-education.psu.edu/sgam/node/25. Accessed October 9, 2023.\n\n\nTomlin, C. Dana. 2013. GIS and Cartographic Modeling. Redlands, Calif: Esri Press.\n\n\nWarf, Barney. 2013. “Time-Space Compression.” Oxford University Press. https://doi.org/10.1093/obo/9780199874002-0025.\n\n\nWright, Dawn J., Michael F. Goodchild, and James D. Proctor. 1997. “Demystifying the Persistent Ambiguity of GIS as ‘Tool’ Versus ‘Science’.” Annals of the Association of American Geographers 87 (2): 346–62. https://doi.org/10.1111/0004-5608.872057.\n\n\nZwartjes, Luc. n.d. “GI Learner Competencies,” 3.\n\n\nZwartjes, Luc, María Luisa de Lázaro, Karl Donert, Isaac Buzo Sánchez, Rafael De Miguel González, and Elżbieta Wołoszyńska-Wiśniewska. n.d. “Literature Review on Spatial Thinking,” 58."
  },
  {
    "objectID": "compute.html",
    "href": "compute.html",
    "title": "Compute",
    "section": "",
    "text": "While I am a patient man and am happy to wait, many objective of data science care about speed. If that’s your jam, check out https://duckdblabs.github.io/db-benchmark/, but as quoted from the duckdblabs page: “We also include the syntax being timed alongside the timing. This way you can immediately see whether you are doing these tasks or not, and if the timing differences matter to you or not.” (“Database-Like Ops Benchmark” n.d.)",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Compute"
    ]
  },
  {
    "objectID": "compute.html#the-complications-of-scaling-and-compute",
    "href": "compute.html#the-complications-of-scaling-and-compute",
    "title": "Compute",
    "section": "The complications of scaling and compute",
    "text": "The complications of scaling and compute\nMany user and use cases desire high resolution data, but that use can become a significant cost burden the finer in resolution you move. Furthermore, this cost increase does not correspond with the changes in resolution (i.g. a process at 10 meters will not take twice as long as a 20 meter one). What’s often missing in this conversation is some geographic context and some bounds around the scales of the process you’re modeling.\n```{r}\nfort_collins_lat &lt;- 40.5853\nfort_collins_lon &lt;- -105.0844\n\n# Create a bounding box as an sf object\n# bbox &lt;- sf::st_bbox(sf::st_buffer(\n#   sf::st_sf(geometry = sf::st_sfc(sf::st_point(c(fort_collins_lon, fort_collins_lat))),crs = sf::st_crs(\"EPSG:4326\")),\n#   units::as_units(500,'m')))\n# projected_bbox &lt;- sf::st_transform(sf::st_as_sfc(bbox), crs = sf::st_crs(\"EPSG:5070\"))\n# cell_sizes &lt;- c(500, 100, 10, 5, 3, 2, 1)\n\nbbox &lt;- sf::st_bbox(sf::st_buffer(\n  sf::st_sf(geometry = sf::st_sfc(sf::st_point(c(fort_collins_lon, fort_collins_lat))),crs = sf::st_crs(\"EPSG:4326\")),\n  units::as_units(50,'m')))\nprojected_bbox &lt;- sf::st_transform(sf::st_as_sfc(bbox), crs = sf::st_crs(\"EPSG:5070\"))\ncell_sizes &lt;- c(10, 5, 3, 2, 1)\n\n# Create a list to store the grids\ngrids &lt;- list()\nfor (size in cell_sizes) {\n  grid &lt;- sf::st_make_grid(projected_bbox, cellsize = c(size, size))\n  grids[[paste0(\"grid_\", size, \"m\")]] &lt;- grid\n}\ngrids_wgs84 &lt;- lapply(grids, sf::st_transform, crs = sf::st_crs(\"EPSG:4326\"))\n\nggplot2::ggplot() +\n  ggplot2::geom_sf(data = sf::st_transform(sf::st_as_sfc(bbox), crs = 4326), fill = \"lightblue\", alpha = 0.3) +\n  # ggplot2::geom_sf(data = grids_wgs84$grid_500m, fill = NA, color = \"red\", linewidth = 1) +\n  # ggplot2::geom_sf(data = grids_wgs84$grid_100m, fill = NA, color = \"green\", linewidth = 0.8) +\n  ggplot2::geom_sf(data = grids_wgs84$grid_10m, fill = NA, color = \"blue\", linewidth = 0.6) +\n  ggplot2::geom_sf(data = grids_wgs84$grid_5m, fill = NA, color = \"orange\", linewidth = 0.5) +\n  ggplot2::geom_sf(data = grids_wgs84$grid_3m, fill = NA, color = \"purple\", linewidth = 0.4) +\n  ggplot2::geom_sf(data = grids_wgs84$grid_2m, fill = NA, color = \"brown\", linewidth = 0.3) +\n  ggplot2::geom_sf(data = grids_wgs84$grid_1m, fill = NA, color = \"black\", linewidth = 0.2) +\n  ggplot2::labs(title = \"Overlapping Grids Centered on Fort Collins\",\n                subtitle = \"Cell Sizes: 500m (red), 100m (green), 10m (blue), 5m (orange), 3m (purple), 2m (brown), 1m (black)\",\n                x = \"Longitude\", y = \"Latitude\") +\n  ggplot2::theme_minimal()\n\n# Calculate the number of cells for each grid\nnum_cells &lt;- sapply(grids, length)\ncomputation_table &lt;- data.frame(\n  Cell_Size_m = as.numeric(gsub(\"grid_(\\\\d+)m\", \"\\\\1\", names(num_cells))),\n  Number_of_Cells = num_cells\n)\ncomputation_table &lt;- computation_table[order(computation_table$Cell_Size_m, decreasing = TRUE), ]\ncomputation_table$Relative_Increase &lt;- computation_table$Number_of_Cells / computation_table$Number_of_Cells[1]\nprint(computation_table)\n```",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Compute"
    ]
  },
  {
    "objectID": "compute.html#local",
    "href": "compute.html#local",
    "title": "Compute",
    "section": "Local",
    "text": "Local\nThe most accessible way to run calculations is on the computer right in front of your face. However, how you set up and run your computer (environment) might vary wildly from someone else, and so issues like reproducibility and replicability creep in as you advance. One of the most direct ways to overcome some of those hurdles is to create a “virtual environment” within which you develop and test your process. While there are all sorts of different virtual environments that you can use that provide all the bells and whistle you might want, those can also be very “heavy” both in terms of the amount of disk space the environment takes up and how ineffectively the environment uses your hardware. When performance and ease of scaleability matter, Docker is a great way to keep a task reproducible which makes porting and scaling your workflow almost frictionless. Per the Docker landing page: “Docker helps developers build, share, run, and verify applications anywhere — without tedious environment configuration or management.” See how I install and use Docker for tips and cheatsheets.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Compute"
    ]
  },
  {
    "objectID": "compute.html#cloud",
    "href": "compute.html#cloud",
    "title": "Compute",
    "section": "Cloud",
    "text": "Cloud\n\nARCO resource\nhttps://radiantearth.github.io/stac-browser/#/",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Compute"
    ]
  },
  {
    "objectID": "classindex.html",
    "href": "classindex.html",
    "title": "Class index",
    "section": "",
    "text": "From https://imgflip.com/memegenerator/247756783/patrick-to-do-list-actually-blank\n\n\nDon’t flip out, but this page is in the backlog.\n(╯°□°)╯︵ ┻━┻\nHey! I said don’t flip out. Worry not, I’ll take you back to the homepage.",
    "crumbs": [
      "Physical Geography"
    ]
  },
  {
    "objectID": "array_pliers.html",
    "href": "array_pliers.html",
    "title": "Array pliers",
    "section": "",
    "text": "super_sleepers &lt;- data.frame(rating=1:4,\n                             animal=c('koala', 'hedgehog', 'sloth', 'panda'),\n                             country=c('Australia', 'Italy', 'Peru', 'China'),\n                             avg_sleep_hours=c(21, 18, 17, 10))\nprint(super_sleepers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://stackoverflow.com/questions/77434/how-to-access-the-last-value-in-a-vector"
  },
  {
    "objectID": "array_pliers.html#r",
    "href": "array_pliers.html#r",
    "title": "Array pliers",
    "section": "",
    "text": "super_sleepers &lt;- data.frame(rating=1:4,\n                             animal=c('koala', 'hedgehog', 'sloth', 'panda'),\n                             country=c('Australia', 'Italy', 'Peru', 'China'),\n                             avg_sleep_hours=c(21, 18, 17, 10))\nprint(super_sleepers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://stackoverflow.com/questions/77434/how-to-access-the-last-value-in-a-vector"
  },
  {
    "objectID": "array_pliers.html#python",
    "href": "array_pliers.html#python",
    "title": "Array pliers",
    "section": "Python",
    "text": "Python"
  },
  {
    "objectID": "acc.html",
    "href": "acc.html",
    "title": "Accuracy matrix and metrics",
    "section": "",
    "text": "When communicating the results of an analysis, one of the first questions anyone will ask you is if you calibrated/validated your model. Often, this is a loaded question for a number of reasons, but one of the most addressable ones is to provide them the accuracy metric of your choice. While not always the case, this generally involves comparing two rasters, a reference and a ground truth to create what is known as a “confusion matrix”. This combination of values can be mathematically reduced to a single value, and that singular number is all that’s usually reported.\nThis of course can get the box checked at the end of the day, but leaves quite a lot of room for improvement. One of the “easiest” ways to improve on this baseline though, is to continue to perform that accuracy assessment at regular intervals. The single marker in that time series may not mean much in isolation, but when you can show steady or incremental improvements in that metric over time, that lends confidence to the underlying process representations you are applying to your model.\nThe other game to play here is of course the “which metric do you choose?” game. Much like an extremely accessible form of p-hacking, They take catchment aggregated p-bias and take that as the room temperature measurement. “If water model versions are making improvements in each iteration that is seen as the sign that these efforts are improving. That is misleading not only because they’ve hinged that outcome on a single metric with no spatial significance, but also because the calibration and validation data is treated as a monolitic and stable set of observations instead of a living collection of data points which have a seasonality and even event based alternatives.\n\n\n\n\n\n\n\n\n\n\nPrediction Positive\nPrediction Negative\nPrediction Null\n\n\n\n\nTruth Positive\nTrue Positive (A)\nFalse Negative (B)\nNull Positive (E)\n\n\nTruth Negative\nFalse Positive (C)\nTrue Negative (D)\nNull Negative (F)\n\n\n\n\n\nThe hydrologic and hydraulic communities have a select number of accuracy metrics and traditional workflows that they reach to when they try to quantify the skill or performance of a model.\n\n\n\n\n\n\n\n\nName\nContext\nReflects\n\n\n\n\nBias\nContinuous\nLong term average error (dry/wet) - also forcings\n\n\nRMSE / RSR Nash-sutcliffe\nCont\nA set of standard variance estimatros, sometimes applied to long(Q) to de-emphasize extremes\n\n\nCorrelation\nCont\nLinear corrolation coefficent (Pearson’s r)\n\n\nKling-Gupta efficencit\nCont\nRational decomposition of error commpointents (linear corrolation, variability bias, and mean bias)\n\n\nPeak discharge\nevent\nAbility of model to predict Qp for events\n\n\nStormflow error\nevent\nAbility of model to predict Rp for events\n\n\nConditional\nevent\nOccourance of an event (TP,FP,TN,FN)\n\n\nPeak timing\nevent\nAbility of model to predict Tp for events\n\n\nHydrologic signiture\nmixed\nOther information content in streamflow signal\n\n\n\n\n\n\nHydrologists lean to several common metrics but one of the most common is nash shutcliff.\nCarson plots provide a rich overview of comparisons by tracking 3 of the 4 confusion matrix values alongside an aggragate metric of the designers choice.\nSNOTEL is a series of stations set up primarily along the Rockies that include a series of advanced sensors to measure precipitation.  The king of the show is the snow pillow, which measures the weight of snow on top of it, and can be used to calculate the snow water equivalent.\n\n\n\nI don’t have many pet peeves but one of them is listening to a conversation that poorly deploys or interprets accuracy metrics. Much of that frustration comes because those conversation tend to poorly wield these otherwise powerful aggregate statistics in a way that might be inappropriate, or that the “blame” for the poor performance is pinned to a misunderstanding of how variations in the confusion matrix do or do not impact the resulting calculation. One of the simplest ways I can try and describe this is along the lines of a paper on snow cover accuracy I wrote many moons ago. In this, we are comparing accuracy of two measurements, one, a SNOTEL snow pillow and two, a MODIS snow cover pixel. Taking the data from Table 3 of [@collComprehensiveAccuracyAssessment2018],\n\nframe &lt;- data.frame(\n  header = c('Month','A','B','C','D','E','F','AC','AA','MCC','A  AB','D DC'),\n  jan = c(1,99134,5473,397,1355,163031,1839,0.9448,0.3705,0.3745,0.9477,0.7734),\n  feb = c(2,80492,3779,330,1655,159109,1827,0.9524,0.3323,0.4869,0.9552,0.8338),\n  mar = c(3,83064,6284,502,4407,172667,4366,0.9280,0.3224,0.5797,0.9297,0.8977),\n  apr = c(4,63886,13287,1004,16302,152530,15548,0.8487,0.3054,0.6420,0.8278,0.9420),\n  may = c(5,30313,21423,1937,59356,98668,59702,0.7933,0.3304,0.6116,0.5859,0.9684),\n  jun = c(6,5538,11733,1441,130360,25731,88136,0.9116,0.5168,0.4693,0.3207,0.9891),\n  jly = c(7,171,2268,790,179941,1586,87754,0.9833,0.6609,0.1043,0.0701,0.9956),\n  aug = c(8,8,140,1151,174369,172,97594,0.9927,0.6377,0.0170,0.0541,0.9934),\n  sep = c(9,1237,1033,2136,166161,3415,91506,0.9814,0.6305,0.4381,0.5449,0.9873),\n  oct = c(10,18951,11432,5737,108457,44574,81879,0.8812,0.4701,0.6209,0.6237,0.9498),\n  nov = c(11,64085,16762,3620,25111,125803,27115,0.8140,0.3398,0.6035,0.7927,0.8740),\n  dec = c(12,79714,6627,985,3292,175798,4859,0.9160,0.3060,0.4707,0.9232,0.7697)\n)\n\n\n\n\n\nSee the full paper in remote sensing: Comprehensive accuracy assessment of MODIS daily snow cover products and gap filling methods\n\nOne of my earliest foreys into accuracy reporting was comparing how well the fractional snow cover valve from MODIS compared with SNOTEL sites.  As a new student to the discipline, I had quite a few questions about how best to approach reporting model behavior, and was interested in describing the way we study accuracy.\nThis study was quite novel for it’s time due to the sweeping scale that the use of Google Earth Engine provided and the cutting edge nature of the factional snow cover band in the then new MODIS V6 product contained.  When I have the time and direction to push back into this space, I feel a site specific exploration of these properties would be insightful.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Accuracy matrix and metrics"
    ]
  },
  {
    "objectID": "acc.html#common-metrics",
    "href": "acc.html#common-metrics",
    "title": "Accuracy matrix and metrics",
    "section": "",
    "text": "The hydrologic and hydraulic communities have a select number of accuracy metrics and traditional workflows that they reach to when they try to quantify the skill or performance of a model.\n\n\n\n\n\n\n\n\nName\nContext\nReflects\n\n\n\n\nBias\nContinuous\nLong term average error (dry/wet) - also forcings\n\n\nRMSE / RSR Nash-sutcliffe\nCont\nA set of standard variance estimatros, sometimes applied to long(Q) to de-emphasize extremes\n\n\nCorrelation\nCont\nLinear corrolation coefficent (Pearson’s r)\n\n\nKling-Gupta efficencit\nCont\nRational decomposition of error commpointents (linear corrolation, variability bias, and mean bias)\n\n\nPeak discharge\nevent\nAbility of model to predict Qp for events\n\n\nStormflow error\nevent\nAbility of model to predict Rp for events\n\n\nConditional\nevent\nOccourance of an event (TP,FP,TN,FN)\n\n\nPeak timing\nevent\nAbility of model to predict Tp for events\n\n\nHydrologic signiture\nmixed\nOther information content in streamflow signal",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Accuracy matrix and metrics"
    ]
  },
  {
    "objectID": "acc.html#acc",
    "href": "acc.html#acc",
    "title": "Accuracy matrix and metrics",
    "section": "",
    "text": "Hydrologists lean to several common metrics but one of the most common is nash shutcliff.\nCarson plots provide a rich overview of comparisons by tracking 3 of the 4 confusion matrix values alongside an aggragate metric of the designers choice.\nSNOTEL is a series of stations set up primarily along the Rockies that include a series of advanced sensors to measure precipitation.  The king of the show is the snow pillow, which measures the weight of snow on top of it, and can be used to calculate the snow water equivalent.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Accuracy matrix and metrics"
    ]
  },
  {
    "objectID": "acc.html#metrics-matter",
    "href": "acc.html#metrics-matter",
    "title": "Accuracy matrix and metrics",
    "section": "",
    "text": "I don’t have many pet peeves but one of them is listening to a conversation that poorly deploys or interprets accuracy metrics. Much of that frustration comes because those conversation tend to poorly wield these otherwise powerful aggregate statistics in a way that might be inappropriate, or that the “blame” for the poor performance is pinned to a misunderstanding of how variations in the confusion matrix do or do not impact the resulting calculation. One of the simplest ways I can try and describe this is along the lines of a paper on snow cover accuracy I wrote many moons ago. In this, we are comparing accuracy of two measurements, one, a SNOTEL snow pillow and two, a MODIS snow cover pixel. Taking the data from Table 3 of [@collComprehensiveAccuracyAssessment2018],\n\nframe &lt;- data.frame(\n  header = c('Month','A','B','C','D','E','F','AC','AA','MCC','A  AB','D DC'),\n  jan = c(1,99134,5473,397,1355,163031,1839,0.9448,0.3705,0.3745,0.9477,0.7734),\n  feb = c(2,80492,3779,330,1655,159109,1827,0.9524,0.3323,0.4869,0.9552,0.8338),\n  mar = c(3,83064,6284,502,4407,172667,4366,0.9280,0.3224,0.5797,0.9297,0.8977),\n  apr = c(4,63886,13287,1004,16302,152530,15548,0.8487,0.3054,0.6420,0.8278,0.9420),\n  may = c(5,30313,21423,1937,59356,98668,59702,0.7933,0.3304,0.6116,0.5859,0.9684),\n  jun = c(6,5538,11733,1441,130360,25731,88136,0.9116,0.5168,0.4693,0.3207,0.9891),\n  jly = c(7,171,2268,790,179941,1586,87754,0.9833,0.6609,0.1043,0.0701,0.9956),\n  aug = c(8,8,140,1151,174369,172,97594,0.9927,0.6377,0.0170,0.0541,0.9934),\n  sep = c(9,1237,1033,2136,166161,3415,91506,0.9814,0.6305,0.4381,0.5449,0.9873),\n  oct = c(10,18951,11432,5737,108457,44574,81879,0.8812,0.4701,0.6209,0.6237,0.9498),\n  nov = c(11,64085,16762,3620,25111,125803,27115,0.8140,0.3398,0.6035,0.7927,0.8740),\n  dec = c(12,79714,6627,985,3292,175798,4859,0.9160,0.3060,0.4707,0.9232,0.7697)\n)",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Accuracy matrix and metrics"
    ]
  },
  {
    "objectID": "acc.html#accuracy-in-the-context-of-snow-cover-presence",
    "href": "acc.html#accuracy-in-the-context-of-snow-cover-presence",
    "title": "Accuracy matrix and metrics",
    "section": "",
    "text": "See the full paper in remote sensing: Comprehensive accuracy assessment of MODIS daily snow cover products and gap filling methods\n\nOne of my earliest foreys into accuracy reporting was comparing how well the fractional snow cover valve from MODIS compared with SNOTEL sites.  As a new student to the discipline, I had quite a few questions about how best to approach reporting model behavior, and was interested in describing the way we study accuracy.\nThis study was quite novel for it’s time due to the sweeping scale that the use of Google Earth Engine provided and the cutting edge nature of the factional snow cover band in the then new MODIS V6 product contained.  When I have the time and direction to push back into this space, I feel a site specific exploration of these properties would be insightful.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Accuracy matrix and metrics"
    ]
  },
  {
    "objectID": "XY.html#how-to",
    "href": "XY.html#how-to",
    "title": "Add XY Coordinates",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Math",
      "Add XY Coordinates"
    ]
  },
  {
    "objectID": "VW.html#how-to",
    "href": "VW.html#how-to",
    "title": "Viewshed",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Viewshed"
    ]
  },
  {
    "objectID": "VC.html#how-to",
    "href": "VC.html#how-to",
    "title": "Vector Conversion",
    "section": "How to ",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Vector Conversion"
    ]
  },
  {
    "objectID": "TP.html#how-to",
    "href": "TP.html#how-to",
    "title": "Topology",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Analysis",
      "Network",
      "Topology"
    ]
  },
  {
    "objectID": "TB.html#how-to",
    "href": "TB.html#how-to",
    "title": "Table Tools",
    "section": "How to ",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Table Tools"
    ]
  },
  {
    "objectID": "SU.html#how-to",
    "href": "SU.html#how-to",
    "title": "Suitability",
    "section": "How to",
    "text": "How to\nSS?",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Suitability"
    ]
  },
  {
    "objectID": "SS.html",
    "href": "SS.html",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "title: Site Selection bibliography: ZoteroDBase.json"
  },
  {
    "objectID": "SS.html#example",
    "href": "SS.html#example",
    "title": "So long as you get the GIS&T of it",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "SS.html#how-to",
    "href": "SS.html#how-to",
    "title": "So long as you get the GIS&T of it",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "SL.html#how-to",
    "href": "SL.html#how-to",
    "title": "Skyline",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Skyline"
    ]
  },
  {
    "objectID": "SG.html#how-to",
    "href": "SG.html#how-to",
    "title": "Segmentation",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "SCHISM.html",
    "href": "SCHISM.html",
    "title": "SCHISM",
    "section": "",
    "text": "Probably best encapsulated by the authors: “SCHISM (Semi-implicit Cross-scale Hydroscience Integrated System Model) is an open-source community-supported modeling system based on unstructured grids, designed for seamless simulation of 3D baroclinic circulation across creek-lake-river-estuary-shelf-ocean scales. It uses a highly efficient and accurate semi-implicit finite-element/finite-volume method with Eulerian-Lagrangian algorithm to solve the Navier-Stokes equations (in hydrostatic form)…”. This model forms the core of OWP’s total water level prediction and has been deployed at scale across a number of domains. (“SCHISM Manual at Ccrm-Vims-Edu” n.d.)",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "SCHISM"
    ]
  },
  {
    "objectID": "SCHISM.html#executive-summary",
    "href": "SCHISM.html#executive-summary",
    "title": "SCHISM",
    "section": "",
    "text": "Probably best encapsulated by the authors: “SCHISM (Semi-implicit Cross-scale Hydroscience Integrated System Model) is an open-source community-supported modeling system based on unstructured grids, designed for seamless simulation of 3D baroclinic circulation across creek-lake-river-estuary-shelf-ocean scales. It uses a highly efficient and accurate semi-implicit finite-element/finite-volume method with Eulerian-Lagrangian algorithm to solve the Navier-Stokes equations (in hydrostatic form)…”. This model forms the core of OWP’s total water level prediction and has been deployed at scale across a number of domains. (“SCHISM Manual at Ccrm-Vims-Edu” n.d.)",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "SCHISM"
    ]
  },
  {
    "objectID": "SCHISM.html#tutorials",
    "href": "SCHISM.html#tutorials",
    "title": "SCHISM",
    "section": "Tutorials",
    "text": "Tutorials\n\nLooking at interpolation differences\n\n\nPulling the maximum forecast (the easy way)\n\nNOMADS_from_nwmTools &lt;- nwmTools::get_nomads_urls(\n  output = 'total_water',\n  domain = 'atlgulf',\n  num = 18,\n  version = 'v3.0',\n  outdir = \".\") %&gt;%\n  nwmTools::get_timeseries(\n    index_id = c('SCHISM_hgrid_node_x', 'SCHISM_hgrid_node_y'),\n    varname  = 'elevation'\n  )\n  \nNOMADS_from_nwmTools$max &lt;- do.call(pmax, c(NOMADS_from_nwmTools[,3:ncol(NOMADS_from_nwmTools)], list(na.rm=TRUE)))\nschism_df_sf &lt;- sf::st_as_sf(NOMADS_from_nwmTools, coords=c(\"SCHISM_hgrid_node_x\",\"SCHISM_hgrid_node_y\")) |&gt;\n  sf::st_set_crs(sf::st_crs(\"EPSG:4326\"))\n\n\n\nPulling the maximum forecast (the hard way)\nSometimes nwmTools might not work as expected and so in those cases we have to replace all of the heavy lifting that the package does. This is generally not ideal but, much like you do math by hand for a bit until you’ve reached that level of technical understanding and complexity that the tools really start to shine and the values of hand calculations dwindle. Starting with an understanding of the forecast and releases cycle will help you predict the right timestep to look at, but you can also just try and guess at what you want by clicking around the NOMDAS server. Here we’ll pull in the short range forecast which we know has 18 timesteps, and we’ll brute force our way into the right time with a few handles, and match the format that nwmTools generates because we haven’t already made our lives hard enough.\n\noutpath &lt;- file.path(\"~/data/output/schism_forecast\")\nunlink(outpath)\nis_quiet &lt;- FALSE\n\n## Get the timestamp for this run\nfirst_valid_timestamp &lt;- format(Sys.time(), tz = \"GMT\", format = \"%Y-%m-%d %H:%M:%S\") \nfirst_valid_date &lt;- gsub(\"-\", \"\",format(Sys.time(), tz = \"GMT\", format = \"%Y-%m-%d\"))\nfirst_valid_hour &lt;- format(Sys.time(), tz = \"GMT\", format = \"%H\")\n\n\n# -- Check to make sure I have the right date --------------------------------------------------------------------------- \ntesturl &lt;- paste0('https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/v3.0/nwm.', first_valid_date, '/short_range_coastal_atlgulf')\nif (httr::http_error(testurl)) { \n  first_valid_date &lt;- gsub(\"-\", \"\",format(as.Date(first_valid_timestamp, tz = \"GMT\")-1, format = \"%Y-%m-%d\"))\n}\n\n# -- Guess at the most recent time ---------------------------------------------------------------------------------------\nfirst_valid_hour &lt;- 25\nrepeat {\n  first_valid_hour = first_valid_hour-1\n  run_time &lt;- sprintf(\"%02d\", first_valid_hour)\n  testurl &lt;- paste0('https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/v3.0/nwm.', first_valid_date,'/short_range_coastal_atlgulf/nwm.t', run_time, 'z.short_range_coastal.total_water.f018.atlgulf.nc')\n  if(!httr::http_error(testurl)) {\n    break\n  }\n}\n\nfirst_nomads_timestamp &lt;- strptime(paste0(first_valid_date,\" \",run_time,':00:00'), format =\"%Y%m%d %H:%M:%S\",tz = \"UTC\")\nfor (i in sprintf(\"%03d\",c(1:18))) {\n  # i = sprintf(\"%03d\",c(1:18))[1]\n  url &lt;- glue::glue(\"http://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/v3.0/nwm.{first_valid_date}/short_range_coastal_atlgulf/nwm.t{run_time}z.short_range_coastal.total_water.f{i}.atlgulf.nc\")\n  download.file(url, file.path(outpath,\"forecast\",basename(url),fsep = .Platform$file.sep))\n}\nall_nc_timesteps &lt;- list.files(file.path(outpath,\"forecast\",fsep = .Platform$file.sep),pattern = '*.nc$',full.names = TRUE) |&gt; gtools::mixedsort()\n\nnc_file &lt;- ncdf4::nc_open(all_nc_timesteps[1])\nx &lt;- ncdf4::ncvar_get(nc_file,\"SCHISM_hgrid_node_x\")\ny &lt;- ncdf4::ncvar_get(nc_file,\"SCHISM_hgrid_node_y\")\nz &lt;- ncdf4::ncvar_get(nc_file,\"elevation\")\n\nvarname &lt;- glue::glue('elevation_{sub(\"_\",\" \",ncdf4::ncatt_get(nc_file,0,attname=\"model_output_valid_time\")$value)}')\nNOMADS_from_files &lt;- data.frame(\"SCHISM_hgrid_node_x\" = x,\n                                \"SCHISM_hgrid_node_y\" = y,\n                                varname = z)\nnames(NOMADS_from_files)[names(NOMADS_from_files) == \"varname\"] &lt;- glue::glue('elevation_{sub(\"_\",\" \",ncdf4::ncatt_get(nc_file,0,attname=\"model_output_valid_time\")$value)}')\nncdf4::nc_close(nc_file)\n\nj &lt;- length(all_nc_timesteps)\nfor(i in 2:j) {\n  if(!is_quiet) { message(glue::glue(\"Folding in {i} of {j}\")) }\n  nc_file &lt;- ncdf4::nc_open(all_nc_timesteps[i])\n  NOMADS_from_files$varname &lt;- ncdf4::ncvar_get(nc_file,\"elevation\")\n  names(NOMADS_from_files)[names(NOMADS_from_files) == \"varname\"] &lt;- glue::glue('elevation_{sub(\"_\",\" \",ncdf4::ncatt_get(nc_file,0,attname=\"model_output_valid_time\")$value)}')\n  ncdf4::nc_close(nc_file)\n}\n\nNOMADS_from_files$max &lt;- do.call(pmax, c(NOMADS_from_files[,3:ncol(NOMADS_from_files)], list(na.rm=TRUE)))\n\n## And the spatial object:\nschism_df_sf &lt;- sf::st_as_sf(NOMADS_from_files, coords=c(\"SCHISM_hgrid_node_x\",\"SCHISM_hgrid_node_y\")) |&gt;\n  sf::st_set_crs(sf::st_crs(\"EPSG:4326\"))\n\n\n\nA few examples of mapping meshes\nThere are several places to grab mesh elevations but the most authoritative is here:(https://www.nohrsc.noaa.gov/owp_files/nwm/nwm_parameters/README.v3.0.txt). There are a few files we can hunt down to find any differences.\n\nelev.ic: A .gr3 format file that specifies the initial elevation at each node.\nhgrid.gr3 and hgrid.ll: Horizontal grid file with node centered spatial data and mesh connectivity.\nhgrid.nc: grid file in netcdf format, containing a list of nodes with their locations and elevations along with a list of elements.\nhgrid.vtk: ASCII version of the grid file containing element number, element coordinates, and element original coordinates.\nAnd NOMADS short range outputs.\n\nOf those, elev.ic seems inappropriate as it represents a “hot” start file since the average node elevation across the database is &gt; 5? hgrid.nc is oddly formatted (as virtually every netcdf file seems to be), but more concretely I’m not seeing an elevation field. Regardless, we can still use the XY as a data point to sanity check. hgrid.vtk is aspatial/unscaled, let’s skip that headache for the time being. That gives us the following:\n\n\n[1] \"Are coordiantes identical?\"\n\n\nidentical(hgrid.gr3, hgrid.ll):TRUE\n\n\nidentical(hgrid.gr3, hgrid.nc):FALSE\n\n\n\n\n\n\nFile.testing\n\n\nNumber.of.nodes\n\n\nMean.Elevation\n\n\n\n\n\n\nelev.ic\n\n\n10537609\n\n\n5.735372\n\n\n\n\nhgrid.gr3\n\n\n10537609\n\n\n-4.853219\n\n\n\n\nhgrid.ll\n\n\n10537609\n\n\n-4.853219\n\n\n\n\nhgrid.nc\n\n\n10537609\n\n\nNA\n\n\n\n\nNOMADS\n\n\n10481055\n\n\nNA\n\n\n\n\n\n\nJust to make sure we’re not going insane, let’s take a look at these nodes along the U.S.’s shortest seacoast, New Hampshire!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYup, we’ve lost it.\n\n\n\n\nMapping a mesh\nAs a finite element model, SCHISM’s numerical schema is constructed from nodes and the polygons that those nodes define between them. While it’s not typically needed to regenerate the polygons themselves since the model has given us an explicit forecast at the nodes, having access to those shapes is useful. For SCHISM, these are stored in the hgrid.gr3 file, which encodes the node centered spatial data and mesh connectivity as guessable text we can parse out like so:",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "SCHISM"
    ]
  },
  {
    "objectID": "SCHISM.html#reference",
    "href": "SCHISM.html#reference",
    "title": "SCHISM",
    "section": "Reference",
    "text": "Reference\nSite head: https://ccrm.vims.edu/schismweb/ Online docs: https://schism-dev.github.io/schism/master/index.html",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "SCHISM"
    ]
  },
  {
    "objectID": "SCHISM.html#explanations",
    "href": "SCHISM.html#explanations",
    "title": "SCHISM",
    "section": "Explanations",
    "text": "Explanations\nSTOFS P-Surge: Probabilistic Tropical Storm Surge c3\nRelated: SFINCS (Super-Fast Inundation of CoastS) developed by Deltares",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "SCHISM"
    ]
  },
  {
    "objectID": "SA.html#how-to",
    "href": "SA.html#how-to",
    "title": "Spatial Autocorrelation",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "RS.html#how-to",
    "href": "RS.html#how-to",
    "title": "Resample",
    "section": "How to",
    "text": "How to\nEffects of method on landcover",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Data Management",
      "Resample"
    ]
  },
  {
    "objectID": "RRASSLER_need.html#rrassler-fills-a-need-by-paying-geospatial-technical-debt",
    "href": "RRASSLER_need.html#rrassler-fills-a-need-by-paying-geospatial-technical-debt",
    "title": "So long as you get the GIS&T of it",
    "section": "RRASSLER Fills A Need by Paying (Geospatial) Technical Debt",
    "text": "RRASSLER Fills A Need by Paying (Geospatial) Technical Debt\n\n\n\n\n\n\nGoals:\n\nTo set the background for the state and status of HEC-RAS model data.\n\nOutcomes and Takeaways:\n\nA deeper understanding of the technical friction experienced working with legacy models and the sorts of questions that are hard to answer without something like RRASSLER.\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\nSlide layout:  default     Items linked/bordered in green are cited in the tooltip on hover.\n: interactive elements     Items linked/bordered in blue are hyperlinked to relevant resources.\n\n\nThere are few hydraulic models as prolific as HEC-RAS, and since it’s first named release in 1995 users have created these models using public and private data and countless hours of engineering scrutinization in order to generate the best possible purpose-built representation of the world. Like any model, some level of input massaging is necessary in order to get the data into the specified mathematical format a model requires. Like most domain specific software solutions, that massaging was rather, forceful, to the point of permanently altering the shape of those inputs into something that most geospatial data readers are unable to handle. This creates a great deal of friction both in terms of model accountability and interoperability, particularly when you take the standpoint as a model consumer. The R based HEC-RAS Wrestler (RRASSLER) is here to mediate that. By internally versioning and aligning data and pointers, the resulting structure provides a bottoms up approach amenable to walking continental scale applications back to the specific point, cross section, and HEC-RAS model they were sourced from.\nBackground image from: OWP template"
  },
  {
    "objectID": "RRASSLER_need.html#refresher-the-hec-ras-model-format",
    "href": "RRASSLER_need.html#refresher-the-hec-ras-model-format",
    "title": "So long as you get the GIS&T of it",
    "section": "Refresher: The HEC-RAS model format",
    "text": "Refresher: The HEC-RAS model format\n\n\n\n\n\n\nThe Theoretical Standard\n\n\n\nFiles to Track\n\n\n\n\n\n\n\n\nFile grep Pattern (# denotes single numeric wildcard)\nHEC-RAS Model Use\n\n\n\n\n.g##\nGeometry definitions\n\n\n.prj\nProjection (can be non-standard proj4 string defined file)\n\n\n.prj\nProject (same extension, defines how RAS models are wired)\n\n\n.p##\nPlan file, used to drive the model\n\n\n.f##\nSteady Flow file. Profile information, flow data and boundary conditions\n\n\n.h##\nHydraulic Design data file\n\n\n.v##\nVelocity file\n\n\n.o##\nOutput file\n\n\n.r##\nRun file for steady flow\n\n\n.u##\nunsteady Flow file. Profile information, flow data and boundary conditions\n\n\n.x##\nRun file for unsteady flow\n\n\n.dss\nData files\n\n\n.rasmap\nOutput plan\n\n\n\n\n\n\n\n\n\nThese files, followed by a .hdf, are transformations of their counterparts needed for newer versions of HEC-RAS and are automatically created as needed.\n\n\n\n\n\n\nSidecar files are fun."
  },
  {
    "objectID": "RRASSLER_need.html#refresher-the-hec-ras-model-format-1",
    "href": "RRASSLER_need.html#refresher-the-hec-ras-model-format-1",
    "title": "So long as you get the GIS&T of it",
    "section": "Refresher: The HEC-RAS model format",
    "text": "Refresher: The HEC-RAS model format\n\n\n\n\n\n\nModel Standards\n\n\n\n\n\n\n\nFiles to Track\n\n\n\n\n\n\n\n\nFile grep Pattern (# denotes single numeric wildcard)\nHEC-RAS Model Use\n\n\n\n\n.g##\nGeometry definitions\n\n\n.prj\nProjection (can be non-standard proj4 string defined file)\n\n\n.prj\nProject (same extension, defines how RAS models are wired)\n\n\n.p##\nPlan file, used to drive the model\n\n\n.f##\nSteady Flow file. Profile information, flow data and boundary conditions\n\n\n.h##\nHydraulic Design data file\n\n\n.v##\nVelocity file\n\n\n.o##\nOutput file\n\n\n.r##\nRun file for steady flow\n\n\n.u##\nunsteady Flow file. Profile information, flow data and boundary conditions\n\n\n.x##\nRun file for unsteady flow\n\n\n.dss\nData files\n\n\n.rasmap\nOutput plan\n\n\n\n\n\n\n\n\n\nThese files, followed by a .hdf, are transformations of their counterparts needed for newer versions of HEC-RAS and are automatically created as needed.\n\n\n\n\n\n\nSidecar files are fun."
  },
  {
    "objectID": "RRASSLER_need.html#refresher-the-hec-ras-model-format-2",
    "href": "RRASSLER_need.html#refresher-the-hec-ras-model-format-2",
    "title": "So long as you get the GIS&T of it",
    "section": "Refresher: The HEC-RAS model format",
    "text": "Refresher: The HEC-RAS model format\n\n\n\n\n\n\nA “real-world” example\n\n\nFiles to Track\n\n\n\n\n\n\n\n\nFile grep Pattern (# denotes single numeric wildcard)\nHEC-RAS Model Use\n\n\n\n\n.g##\nGeometry definitions\n\n\n.prj\nProjection (can be non-standard proj4 string defined file)\n\n\n.prj\nProject (same extension, defines how RAS models are wired)\n\n\n.p##\nPlan file, used to drive the model\n\n\n.f##\nSteady Flow file. Profile information, flow data and boundary conditions\n\n\n.h##\nHydraulic Design data file\n\n\n.v##\nVelocity file\n\n\n.o##\nOutput file\n\n\n.r##\nRun file for steady flow\n\n\n.u##\nunsteady Flow file. Profile information, flow data and boundary conditions\n\n\n.x##\nRun file for unsteady flow\n\n\n.dss\nData files\n\n\n.rasmap\nOutput plan\n\n\n\n\n\n\n\n\n\nThese files, followed by a .hdf, are transformations of their counterparts needed for newer versions of HEC-RAS and are automatically created as needed.\n\n\n\n\n\n\n\n\n\n\n\nData from the wild can be a bit unruly."
  },
  {
    "objectID": "RRASSLER_need.html#even-after-overcoming-the-pain-of-reading-in-a-hec-ras-model",
    "href": "RRASSLER_need.html#even-after-overcoming-the-pain-of-reading-in-a-hec-ras-model",
    "title": "So long as you get the GIS&T of it",
    "section": "Even After Overcoming the Pain of Reading in a HEC-RAS model…",
    "text": "Even After Overcoming the Pain of Reading in a HEC-RAS model…\n\n\n\nOther Potential Readers\n\n\nPoint and click through the HEC-RAS interface to export\n\n\n\nCon: Unaccountable and unsustainable\n\n\nPro: I can do it \n\n\n\nMCAT-RAS\n\n\nCon: After 2 days of banging my head against the wall, I was not able to get this to run \n\n\nDocker builds with 13/14 failing tests.\n\n\n\n\n\nExpand for my specific errors\n\n\nThe container builds, and I’m sure the issue is between the keyboard and the chair, but this is very unhelpful and configuration and procedures are very opaque to this simpleton external end user.\n\nuser@Brains:/mnt/c/WINDOWS/system32$ cd /mnt/g/Dropbox/root/projects/mcat-ras/Dewberry/mcat-ras/\nuser@Brains:/mnt/g/Dropbox/root/projects/mcat-ras/Dewberry/mcat-ras$ docker-compose up\n[+] Running 2/1\n ✔ Container mcat-ras-api     Created        0.1s\n ✔ Container mcat-ras-newman  Created        0.0s\nAttaching to mcat-ras-api, mcat-ras-newman\nmcat-ras-api     | /bin/sh: /app/main: not found\nmcat-ras-api exited with code 127\nmcat-ras-newman  | newman\nmcat-ras-newman  |\nmcat-ras-newman  | mcat-ras-testing\nmcat-ras-newman  |\nmcat-ras-newman  |\nmcat-ras-newman  | ❏ Forcing Data / Positvies / 2D Unsteady Flow\nmcat-ras-newman  | ↳ BaldEagleCrkMulti2D\nmcat-ras-newman  |\nmcat-ras-newman  |   GET http://api:5600/forcingdata?definition_file=mcat-ras-testing/Example_Projects/2D Unsteady Flow Hydraulics/BaldEagleCrkMulti2D/BaldEagleDamBrk.prj\nmcat-ras-newman  | [errored]\nmcat-ras-newman  |      getaddrinfo ENOTFOUND api\nmcat-ras-newman  |   2. response must be valid and have a body\nmcat-ras-newman  |   3. Status code is 200\nmcat-ras-newman  |   4. response should have Unsteady\nmcat-ras-newman  |   5⠄ JSONError in test-script\nmcat-ras-newman  |\nmcat-ras-newman  | ❏ Forcing Data / Positvies / General\nmcat-ras-newman  |\nmcat-ras-newman  | ↳ Bare minimum project\nmcat-ras-newman  |\nmcat-ras-newman  |   GET http://api:5600/forcingdata?definition_file=mcat-ras-testing/mocked-data/bare-minimum-project/BaldEagleDamBrk.prj\nmcat-ras-newman  | [errored]\nmcat-ras-newman  |      getaddrinfo ENOTFOUND api\nmcat-ras-newman  |   7. response must be valid and have a body\nmcat-ras-newman  |   8. Status code is 200\nmcat-ras-newman  |   9. response should have Unsteady\nmcat-ras-newman  |\nmcat-ras-newman  | ↳ No flow files\nmcat-ras-newman  |\nmcat-ras-newman  |   GET http://api:5600/forcingdata?definition_file=mcat-ras-testing/mocked-data/no-flow-file/BaldEagleDamBrk.prj\nmcat-ras-newman  | [errored]\nmcat-ras-newman  |      getaddrinfo ENOTFOUND api\nmcat-ras-newman  |  11. response must be valid and have a body\nmcat-ras-newman  |  12. Status code is 200\nmcat-ras-newman  |  13. response should be empty json\nmcat-ras-newman  |\nmcat-ras-newman  | ❏ Forcing Data / Negatives\nmcat-ras-newman  | ↳ File does not exist\nmcat-ras-newman  |\nmcat-ras-newman  |   GET http://api:5600/forcingdata?definition_file=mcat-ras-testing/Example_Projects/not-exist.prj\nmcat-ras-newman  | [errored]\nmcat-ras-newman  |      getaddrinfo ENOTFOUND api\nmcat-ras-newman  |  15. response must be valid and have a body\nmcat-ras-newman  |  16. Status code is equar or greater than 400\nmcat-ras-newman  |  17. Bad Reqest\nmcat-ras-newman  |\nmcat-ras-newman  | ❏ General / Positives\nmcat-ras-newman  | ↳ shapefile not a model\nmcat-ras-newman  |\nmcat-ras-newman  |   GET http://api:5600/isamodel?definition_file=mcat-ras-testing/Example_Projects/2D Unsteady Flow Hydraulics/BaldEagleCrkMulti2D/GISData/MainChannelBanks.prj\nmcat-ras-newman  | [errored]\nmcat-ras-newman  |      getaddrinfo ENOTFOUND api\nmcat-ras-newman  |  19. response must be valid and have a body\nmcat-ras-newman  |   ✓  Shapefile not a model\nmcat-ras-newman  |\nmcat-ras-newman  | ┌─────────────────────────┬──────────┬──────────┐\nmcat-ras-newman  | │                         │ executed │   failed │\nmcat-ras-newman  | ├─────────────────────────┼──────────┼──────────┤\nmcat-ras-newman  | │              iterations │        1 │        0 │\nmcat-ras-newman  | ├─────────────────────────┼──────────┼──────────┤\nmcat-ras-newman  | │                requests │        5 │        5 │\nmcat-ras-newman  | ├─────────────────────────┼──────────┼──────────┤\nmcat-ras-newman  | │            test-scripts │       19 │        1 │\nmcat-ras-newman  | ├─────────────────────────┼──────────┼──────────┤\nmcat-ras-newman  | │      prerequest-scripts │       14 │        0 │\nmcat-ras-newman  | ├─────────────────────────┼──────────┼──────────┤\nmcat-ras-newman  | │              assertions │       14 │       13 │\nmcat-ras-newman  | ├─────────────────────────┴──────────┴──────────┤\nmcat-ras-newman  | │ total run duration: 299ms                     │\nmcat-ras-newman  | ├───────────────────────────────────────────────┤\nmcat-ras-newman  | │ total data received: 0B (approx)              │\nmcat-ras-newman  | └───────────────────────────────────────────────┘\nmcat-ras-newman  |\nmcat-ras-newman  |\nmcat-ras-newman  |    #  failure         detail\nmcat-ras-newman  |\nmcat-ras-newman  |  01.  Error\nmcat-ras-newman  |                       getaddrinfo ENOTFOUND api\nmcat-ras-newman  |                       at request\nmcat-ras-newman  |                       inside \"\"\nmcat-ras-newman  |\nmcat-ras-newman  |  02.  AssertionError  response must be valid and have a body\nmcat-ras-newman  |                       expected response to have content in body\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / 2D Unsteady Flow / BaldEagleCrkMulti2D\"\nmcat-ras-newman  |\nmcat-ras-newman  |  03.  AssertionError  Status code is 200\nmcat-ras-newman  |                       expected PostmanResponse{ …(5) } to have property 'code'\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / 2D Unsteady Flow / BaldEagleCrkMulti2D\"\nmcat-ras-newman  |\nmcat-ras-newman  |  04.  AssertionError  response should have Unsteady\nmcat-ras-newman  |                       expected PostmanResponse{ …(5) } to have property 'code'\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / 2D Unsteady Flow / BaldEagleCrkMulti2D\"\nmcat-ras-newman  |\nmcat-ras-newman  |  05.  JSONError\nmcat-ras-newman  |                       Unexpected token u in JSON at position 0\nmcat-ras-newman  |                       at test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / 2D Unsteady Flow / BaldEagleCrkMulti2D\"\nmcat-ras-newman  |\nmcat-ras-newman  |  06.  Error\nmcat-ras-newman  |                       getaddrinfo ENOTFOUND api\nmcat-ras-newman  |                       at request\nmcat-ras-newman  |                       inside \"\"\nmcat-ras-newman  |\nmcat-ras-newman  |  07.  AssertionError  response must be valid and have a body\nmcat-ras-newman  |                       expected response to have content in body\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / General / Bare minimum project\"\nmcat-ras-newman  |\nmcat-ras-newman  |  08.  AssertionError  Status code is 200\nmcat-ras-newman  |                       expected PostmanResponse{ …(5) } to have property 'code'\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / General / Bare minimum project\"\nmcat-ras-newman  |\nmcat-ras-newman  |  09.  AssertionError  response should have Unsteady\nmcat-ras-newman  |                       expected PostmanResponse{ …(5) } to have property 'code'\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / General / Bare minimum project\"\nmcat-ras-newman  |\nmcat-ras-newman  |  10.  Error\nmcat-ras-newman  |                       getaddrinfo ENOTFOUND api\nmcat-ras-newman  |                       at request\nmcat-ras-newman  |                       inside \"\"\nmcat-ras-newman  |\nmcat-ras-newman  |  11.  AssertionError  response must be valid and have a body\nmcat-ras-newman  |                       expected response to have content in body\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / General / No flow files\"\nmcat-ras-newman  |\nmcat-ras-newman  |  12.  AssertionError  Status code is 200\nmcat-ras-newman  |                       expected PostmanResponse{ …(5) } to have property 'code'\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / General / No flow files\"\nmcat-ras-newman  |\nmcat-ras-newman  |  13.  JSONError       response should be empty json\nmcat-ras-newman  |                       Unexpected token u in JSON at position 0\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Positvies / General / No flow files\"\nmcat-ras-newman  |\nmcat-ras-newman  |  14.  Error\nmcat-ras-newman  |                       getaddrinfo ENOTFOUND api\nmcat-ras-newman  |                       at request\nmcat-ras-newman  |                       inside \"\"\nmcat-ras-newman  |\nmcat-ras-newman  |  15.  AssertionError  response must be valid and have a body\nmcat-ras-newman  |                       expected response to have content in body\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Negatives / File does not exist\"\nmcat-ras-newman  |\nmcat-ras-newman  |  16.  AssertionError  Status code is equar or greater than 400\nmcat-ras-newman  |                       expected undefined to be a number or a date\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Negatives / File does not exist\"\nmcat-ras-newman  |\nmcat-ras-newman  |  17.  AssertionError  Bad Reqest\nmcat-ras-newman  |                       expected response to have status reason 'BAD REQUEST' but got 'UNDEFINED'\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"Forcing Data / Negatives / File does not exist\"\nmcat-ras-newman  |\nmcat-ras-newman  |  18.  Error\nmcat-ras-newman  |                       getaddrinfo ENOTFOUND api\nmcat-ras-newman  |                       at request\nmcat-ras-newman  |                       inside \"\"\nmcat-ras-newman  |\nmcat-ras-newman  |  19.  AssertionError  response must be valid and have a body\nmcat-ras-newman  |                       expected response to have content in body\nmcat-ras-newman  |                       at assertion:0 in test-script\nmcat-ras-newman  |                       inside \"General / Positives / shapefile not a model\"\nmcat-ras-newman  |\nmcat-ras-newman exited with code 1\nuser@Brains:/mnt/g/Dropbox/root/projects/mcat-ras/Dewberry/mcat-ras$\n\n\n\n\n\n\nIt is unclear from documentation what I should do to resolve this, and only loose guesses as to what the next step would be, assuming this was working for me.\n\n\nI am unfamiliar with go, it’s implemented language.\n\n\n\npyHMT2D is primarily oriented towards 2D.\n\n\n\nThis pain is not trivial, I weep weekly.\nOther archival, tangentially related, under-baked, or circuitous readers. See related utils\n\nRAS2FIM has readers, which we will extend as part of RRASSLER\nhttps://github.com/solomonvimal/pyras\nhttps://github.com/larflows/PyRASFile\nhttps://github.com/mikebannis/parserasgeo\nhttps://hatarilabs.com/ih-en/open-modify-run-and-read-output-from-hec-ras-models-with-python-tutorial"
  },
  {
    "objectID": "RRASSLER_need.html#can-you-answer-these-questions",
    "href": "RRASSLER_need.html#can-you-answer-these-questions",
    "title": "So long as you get the GIS&T of it",
    "section": "Can you answer these questions?",
    "text": "Can you answer these questions?\n\nWhere are models?\nWhat cross sections do I have for my network?\nIs this the same model?\n\n\nSimple questions, hard answers"
  },
  {
    "objectID": "RRASSLER_need.html#models-from-different-providers",
    "href": "RRASSLER_need.html#models-from-different-providers",
    "title": "So long as you get the GIS&T of it",
    "section": "Models from Different Providers?",
    "text": "Models from Different Providers?\n\n\n\n\n\n\n\nTexas Water Development Board\n\n\n\n\n\n\nFEMA Region 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nasynchronous update cycles\n\n\n\n\nhttps://www.twdb.texas.gov/flood/science/ble.asp\nhttps://www.arcgis.com/apps/dashboards/45d3284db7e64694bdcece58c6de39fc\n\nAlthough the download url is the same, the status of the maps is not the same and makes determining what model an end user is querying for difficult."
  },
  {
    "objectID": "RRASSLER_need.html#models-from-broad-geographies",
    "href": "RRASSLER_need.html#models-from-broad-geographies",
    "title": "So long as you get the GIS&T of it",
    "section": "Models from Broad Geographies",
    "text": "Models from Broad Geographies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClashing spatial units\n\n\n\n\nNeed to pull the entire watershed to get any data\nTake a minute to acknowledge the geographic pain we all just experienced. Am I worried AI will take my job? A boy can dream…\nHelper function"
  },
  {
    "objectID": "RRASSLER_need.html#models-at-street-scale-example-austin-floodpro",
    "href": "RRASSLER_need.html#models-at-street-scale-example-austin-floodpro",
    "title": "So long as you get the GIS&T of it",
    "section": "Models at street scale: Example Austin FloodPro",
    "text": "Models at street scale: Example Austin FloodPro\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual subsetting\n\n\n\n\nYou have to know your street address (easy), or know your watershed name (harder)\neither way are restricted to the watershed scale download\nhttps://maps.austintexas.gov/FloodPro/"
  },
  {
    "objectID": "RRASSLER_need.html#rrassler-can",
    "href": "RRASSLER_need.html#rrassler-can",
    "title": "So long as you get the GIS&T of it",
    "section": "RRASSLER Can!",
    "text": "RRASSLER Can!\n“Where are models?” – See the mapping vignette!"
  },
  {
    "objectID": "RRASSLER_need.html#thank-you-for-your-time---looking-forward",
    "href": "RRASSLER_need.html#thank-you-for-your-time---looking-forward",
    "title": "So long as you get the GIS&T of it",
    "section": "Thank you for your time - looking forward?",
    "text": "Thank you for your time - looking forward?\n\n\n\nOutcomes and Takeaways:\n\nA deeper understanding of the technical friction experienced working with legacy models and the sorts of questions that are hard to answer without something like RRASSLER.\n\nNext Steps:\n\nBuild RRASSLER!\nVerify that it’s processing surveys correctly.\nCreate a public model catalog.\nSolve Hydrology.\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\nSee more at the Official RRASSLER documentation."
  },
  {
    "objectID": "RP.html#how-to",
    "href": "RP.html#how-to",
    "title": "Raster Processing",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Data Management",
      "Raster Processing"
    ]
  },
  {
    "objectID": "RE.html#how-to",
    "href": "RE.html#how-to",
    "title": "Spatial Regression",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Spatial Regression"
    ]
  },
  {
    "objectID": "RC.html#how-to",
    "href": "RC.html#how-to",
    "title": "Raster Creation",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Data Management",
      "Raster Creation"
    ]
  },
  {
    "objectID": "RAS2FIM.html#executive-summary",
    "href": "RAS2FIM.html#executive-summary",
    "title": "RAS2FIM",
    "section": "Executive summary",
    "text": "Executive summary\nAs one of the foundational hydraulic modeling systems, HEC-RAS needs no introduction. This modeling system is so prolific that it’s one and two dimensional flood modeling capabilities provide the authoritative baseline for flood insurance and the default against which other modeling efforts compare. One of the most critical outputs of a HEC-RAS model is a Flood Inundation Map. However, as a complex piece of engineering and water resource science software, it’s use is difficult for many and not readily scalable. Starting with compliant one-dimensional HEC-RAS floodplain models, the RAS2FIM process can be used to create a library of flood depth inundation rasters for a range of storm water discharges. These libraries can be used with the National Water Model. More details can be found on the official repository at https://github.com/NOAA-OWP/ras2fim",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "RAS2FIM"
    ]
  },
  {
    "objectID": "RAS2FIM.html#tutorials",
    "href": "RAS2FIM.html#tutorials",
    "title": "RAS2FIM",
    "section": "Tutorials",
    "text": "Tutorials\nHow to install and set up RAS2FIM. How to run RAS2FIM over sample data.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "RAS2FIM"
    ]
  },
  {
    "objectID": "RAS2FIM.html#explainers",
    "href": "RAS2FIM.html#explainers",
    "title": "RAS2FIM",
    "section": "Explainers",
    "text": "Explainers\n\nWhat is RAS2FIM?\nRAS2FIM can be thought of as a HEC-RAS mapper which runs RAS models with an eye towards densification of the model boundary conditions and subsequent FIM library creation with an eye towards NWM access patterns. It’s important to note that RAS2FIM, in both versions, hinge on the key inputs of an already constructed HEC-RAS 1D model and the National Water Model stream segments. This toolset was developed, tested, and deployed over BLE sourced HEC-RAS models, and it’s important to bear in mind that not every (and in fact, most) HEC-RAS models may not be compliant with the way the tools modify the model in order to create an inundation library. Additionally, without special modification, some models which are otherwise complaint will not generate a FIM library because of the explicit tie-in with the NWM access patterns or may otherwise be superceeded by a model which better conflates during the inundation process. Despite many of the technical hurdles and strict input data compliance standards (compared to geospatial flood modeling methods), the RAS2FIM method is one of the most widely deployed and deployable, hydraulically informed, means of generating FIM libraries\n\n\nRAS2FIM V1 vs RAS2FIM V2\nRAS2FIM has had two major versions, colloqually titled RAS2FM v1 and RAS2FIM v2. These versions both accomplish the same end goal of generating a FIM library using a compliant 1D HEC-RAS model for use in NWM access patterns, but the way in which they accomplish this has important implications in both the way you should conceptually frame the underlying processes and the ways in which we should interpret the outputs. Despite small differences in the expected form of the inputs, if you have set up your system for RAS2FIM v2 both v1 and v2 will execute.\nTo talk about this, it’s worth defining some terms used. A “parent” model is the “raw” HEC-RAS model as it is pulled from a database. “Child” models are models that the RAS2FIM program modifies in some way. In RAS2FIM v1, these parent models are conflated to the NWM forecast network by identifying the model that has the most streamline points snapped to it, and taking a slice of the parent model + two cross sections in both the up and downstream directions. That subset of the parent model has it’s boundary conditions replaced upstream by a range of discharge values fed into it by the national database, and a normal energy gradeline boundary condition is applied to the downstream boundary condition. That model is then run numerous times to generate a stack of depth rasters that are modified to meet a standard, and then packaged for deployment. The rating curve is simply the depth average of all the cross sections for that model, and because the models were sliced to the NWM network prior to running there’s no need to aggregate the results.\nIn RAS2FIM v2, we take a slightly different approach in that the most appropriate model is still identified for a given NWM segment, but the parent model is used as a single monolith as opposed to being subset out. The differing boundary conditions are respected by the code base, but the boundary conditions are densified to create tighter increments, and then after the entire model is run the desired subset of rating curves are pulled out and used to calculate that reach averaged rating and the depth rasters sliced to create ID specific libraries.\n\n\nCritical RAS2FIM inputs\n\nA mapping surface\nA RAS2FIM compliant HEC-RAS model.\n\n\n\nRAS2FIM 2D\nAndy Carter et al. Given the preference for 2D modeling efforts, the logical next step of the RAS2FIM evolution is to translate the processes from RAS2FIM to 2D models.\n\n\nUnit handling\nIn RAS2FIM V1, units needed to be accounted for by the end user prior to generating a run. These units were used as the flag for the model parsing, and did not reflect the units of the input DEM. These could inadvertently be mismatched and the code did not account for that edge case. After version 1.11.0 (and beyond that) the units of the provided model would dictate the conversion of the DEM file regardless of their native value. So for example, if you had staged a DEM surface in meters and then provided models whose elevations were in feet, the program automatically converted that to the correct reference frame before mapping the resultant model surface.\n\n\nHEC-RAS Model Limitations of RAS2FIM\n\nKnowing beforehand wheather a model is or is not going to successfully run through RAS2FIM is typically something we identify by trying, but there are a few halmarks of a model that we aim to handle or are expecting that you should know. See those here.\n\nRAS2FIM currently assumed that all HEC-RAS models are composed of a single reach and a single river. If multiple reaches and rivers are in the HEC-RAS geometry file, only the first reach and the first river is evaluated.\nOnly steady-state one-dimensional models can be evaluated. Unsteady and 2-dimensional models are not allowed to be used as input to the RAS2FIM workflow.\nBridges, culverts and in-line structures are allowed in the provided HEC-RAS models. Storage areas, lateral structures, SA/2D Connections and 2D flow areas are not allowed.\nAll models provided in RAS2FIM must be created in the same projection (CRS)\n\n\n\nUSGS 3DEP Terrain\n\nTerrain data provided in by the USGS 3DEP web coverage service is a composite of multiple sources in a variety of horizontal resolutions and accuracy. The RAS2FIM routine that grabs terrain from this service does not know the underlying source. In some portions of the the U.S., the terrain returned my be an up-sampling of a lower resolution source. (Example 3m returned from a 10m USGS source)\nA large request may result in a time-out of the USGS 3DEP WCS service and halt the terrain harvesting script.\n\n\n\nConflation\n\nConflation may return the wrong HEC-RAS model especially for matches with low counts of snapped points. Confluences are the most likely area for a false snap.\nTwo HEC-RAS models may ‘split’ a feature-id. The most popular wins, however a portion of each of the two HEC-RAS model maybe more correct. RAS2FIM simply picks the most popular and ignores the other.\n\n\n\nCreate Depth Grids\n\nScripts may stall when a ‘bad’ HEC-RAS input is processed. This maybe points in the cross section that do not have increasing station. There are likely several other input file issues that can stall the HEC-RAS process.\nIn HEC-RAS, cross sections that do not contain the flows will assume a glass wall along the edge. While the RAS2FIM will run, the glass walling will incorrectly compute a reach averaged depth for the given discharge and introduce error into the Synthetic Rating Curve (SRC).\n\n\n\nSynthetic Rating Curves\n\nWhile rare, it is possible to compute a non-monotonic synthetic rating curve. This may cause the calculations of interpolation to stall.\n\n\n\nHeadwater Streams\n\nFor upstream reaches strahler stream order 1, the assumption of a constant flow based on the entire contributing drainage area of the reach could lead to an over estimation of flood limits and depth on the upstream end.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "RAS2FIM"
    ]
  },
  {
    "objectID": "PX.html#how-to",
    "href": "PX.html#how-to",
    "title": "Proximity",
    "section": "How to ",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Math",
      "Proximity"
    ]
  },
  {
    "objectID": "PH.html#how-to",
    "href": "PH.html#how-to",
    "title": "Photogrammetry",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing",
      "Photogrammetry"
    ]
  },
  {
    "objectID": "PD_quadrant.html",
    "href": "PD_quadrant.html",
    "title": "Quadrant",
    "section": "",
    "text": "https://mermaid.live/edit#pako:eNptks9PwjAUx_-Vl543ggwEdzBRwHgQD3owkXF4ro-tydrOrkUI4X-3rPxIpjt1_Xze23ev3bNcc2Ip-3bIDSo7LdHYTIF_rLAVwRthXgIqDqQKLEiSsqDXkKOsURSqCfI2xq1o4EX_nCri-B6eRVGG1yDtrtL82uxiXveCfs4U38AHQVNqV_kU29qH6QgDeCXiYDXURkttqcMTnyKmDVYO_7AhLHAHXwRC-uINnXpPT_8HDyks-70kgn7vdtVhjy0bjo5wkHTptKWjcVt616Wzlo4nR5oMu3QeOvf_p08hU_vd8WTFIibJSBTcn-T-6GbMln6SGUv9ktMaXWUzlqmDV9FZ_b5TOUutcRQxV3M_lZnAwqA8bxIXVptFuBztHYmYH_yn1hfFaFeULF1j1dDhFy2Mt2Q"
  },
  {
    "objectID": "PD_quadrant.html#examples",
    "href": "PD_quadrant.html#examples",
    "title": "Quadrant",
    "section": "",
    "text": "https://mermaid.live/edit#pako:eNptks9PwjAUx_-Vl543ggwEdzBRwHgQD3owkXF4ro-tydrOrkUI4X-3rPxIpjt1_Xze23ev3bNcc2Ip-3bIDSo7LdHYTIF_rLAVwRthXgIqDqQKLEiSsqDXkKOsURSqCfI2xq1o4EX_nCri-B6eRVGG1yDtrtL82uxiXveCfs4U38AHQVNqV_kU29qH6QgDeCXiYDXURkttqcMTnyKmDVYO_7AhLHAHXwRC-uINnXpPT_8HDyks-70kgn7vdtVhjy0bjo5wkHTptKWjcVt616Wzlo4nR5oMu3QeOvf_p08hU_vd8WTFIibJSBTcn-T-6GbMln6SGUv9ktMaXWUzlqmDV9FZ_b5TOUutcRQxV3M_lZnAwqA8bxIXVptFuBztHYmYH_yn1hfFaFeULF1j1dDhFy2Mt2Q"
  },
  {
    "objectID": "PD_Violin.html",
    "href": "PD_Violin.html",
    "title": "Violin",
    "section": "",
    "text": "ggplot(mtcars, aes(x = factor(cyl), y = mpg)) + \n  geom_violin() + \n  labs(title = \"Violin Plot of MPG by Cylinder Count\", \n       x = \"Number of Cylinders\", y = \"Miles Per Gallon (MPG)\",\n       caption=\"Created by rstudiodatalab.com\")\n# Create a customized violin plot with additional aesthetics\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_violin(trim = FALSE, scale = \"width\", adjust = 1.5, alpha = 0.7) +\n  geom_boxplot(width = 0.1, fill = \"white\", outlier.shape = NA) +\n  labs(title = \"Customized Violin Plot of MPG by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\",\n       fill=\"CYL\",\n       caption=\"Created by rstudiodatalab.com\") +\n  theme_minimal()\n\n\n\n# Create a violin plot with median and mean\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_violin(trim = FALSE, scale = \"width\", adjust = 1.5, alpha = 0.7) +\n  stat_summary(fun = median, geom = \"point\", shape = 23, size = 3, fill = \"white\") +\n  stat_summary(fun = mean, geom = \"point\", shape = 21, size = 3, fill = \"red\") +\n  labs(title = \"Violin Plot of MPG by Cylinder Count with Median and Mean\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()\n\n\n\n# Create a violin plot with a standard deviation\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_violin(trim = FALSE, scale = \"width\", adjust = 1.5, alpha = 0.7) +\n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Violin Plot of MPG by Cylinder Count with Standard Deviation\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()\n\n\n\n# Create a violin plot with Gaussian kernel density estimation\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_violin(kernel = \"gaussian\") +\n  labs(title = \"Violin Plot with Gaussian Kernel Density Estimation\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\")\n# Create a violin plot with different bandwidths for comparison\np1 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_violin(adjust = 0.5) +\n  labs(title = \"Bandwidth = 0.5\")\np2 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_violin(adjust = 1) +\n  labs(title = \"Bandwidth = 1\")\np3 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_violin(adjust = 2) +\n  labs(title = \"Bandwidth = 2\")\n# Arrange plots in a grid for comparison\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n# Create a violin plot with added dot plots\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_violin(trim = FALSE, scale = \"width\", adjust = 1.5, alpha = 0.7) +\n  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = 0.5) +\n  labs(title = \"Violin Plot with Dot Plots\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()"
  },
  {
    "objectID": "PD_Violin.html#examples",
    "href": "PD_Violin.html#examples",
    "title": "Violin",
    "section": "",
    "text": "ggplot(mtcars, aes(x = factor(cyl), y = mpg)) + \n  geom_violin() + \n  labs(title = \"Violin Plot of MPG by Cylinder Count\", \n       x = \"Number of Cylinders\", y = \"Miles Per Gallon (MPG)\",\n       caption=\"Created by rstudiodatalab.com\")\n# Create a customized violin plot with additional aesthetics\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_violin(trim = FALSE, scale = \"width\", adjust = 1.5, alpha = 0.7) +\n  geom_boxplot(width = 0.1, fill = \"white\", outlier.shape = NA) +\n  labs(title = \"Customized Violin Plot of MPG by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\",\n       fill=\"CYL\",\n       caption=\"Created by rstudiodatalab.com\") +\n  theme_minimal()\n\n\n\n# Create a violin plot with median and mean\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_violin(trim = FALSE, scale = \"width\", adjust = 1.5, alpha = 0.7) +\n  stat_summary(fun = median, geom = \"point\", shape = 23, size = 3, fill = \"white\") +\n  stat_summary(fun = mean, geom = \"point\", shape = 21, size = 3, fill = \"red\") +\n  labs(title = \"Violin Plot of MPG by Cylinder Count with Median and Mean\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()\n\n\n\n# Create a violin plot with a standard deviation\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_violin(trim = FALSE, scale = \"width\", adjust = 1.5, alpha = 0.7) +\n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Violin Plot of MPG by Cylinder Count with Standard Deviation\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()\n\n\n\n# Create a violin plot with Gaussian kernel density estimation\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_violin(kernel = \"gaussian\") +\n  labs(title = \"Violin Plot with Gaussian Kernel Density Estimation\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\")\n# Create a violin plot with different bandwidths for comparison\np1 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_violin(adjust = 0.5) +\n  labs(title = \"Bandwidth = 0.5\")\np2 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_violin(adjust = 1) +\n  labs(title = \"Bandwidth = 1\")\np3 &lt;- ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_violin(adjust = 2) +\n  labs(title = \"Bandwidth = 2\")\n# Arrange plots in a grid for comparison\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n# Create a violin plot with added dot plots\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_violin(trim = FALSE, scale = \"width\", adjust = 1.5, alpha = 0.7) +\n  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = 0.5) +\n  labs(title = \"Violin Plot with Dot Plots\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()"
  },
  {
    "objectID": "PD_Sunburst.html",
    "href": "PD_Sunburst.html",
    "title": "Sunburst",
    "section": "",
    "text": "```{python}\nimport pandas as pd\nimport plotly.express as px\n\ndata = {'site': {1: 'Site 1', 2: 'Site 2', },\n        'link': {1: '&lt;a href=\"http://www.google.com\"&gt;link text&lt;/a&gt;', 2: 'http://www.facebook.com'},\n        'popularity': {1: 50, 2: 50}}\n\ndf = pd.DataFrame(data)\n\nfig = px.sunburst(df,\n                  path=['site', 'link'],\n                  values='popularity',\n                  maxdepth=2,)\n\nfig.show()\n```"
  },
  {
    "objectID": "PD_Sunburst.html#examples",
    "href": "PD_Sunburst.html#examples",
    "title": "Sunburst",
    "section": "",
    "text": "```{python}\nimport pandas as pd\nimport plotly.express as px\n\ndata = {'site': {1: 'Site 1', 2: 'Site 2', },\n        'link': {1: '&lt;a href=\"http://www.google.com\"&gt;link text&lt;/a&gt;', 2: 'http://www.facebook.com'},\n        'popularity': {1: 50, 2: 50}}\n\ndf = pd.DataFrame(data)\n\nfig = px.sunburst(df,\n                  path=['site', 'link'],\n                  values='popularity',\n                  maxdepth=2,)\n\nfig.show()\n```"
  },
  {
    "objectID": "PD_Barplot.html",
    "href": "PD_Barplot.html",
    "title": "Barplot",
    "section": "",
    "text": "https://mermaid.live/edit#pako:eNqtUTFuwzAM_ApBdGgBGbCLpkk8dOrapdkad2Bs2lYgS4YsBTGC_L2SleQF1XA6kqejQF6wNg1jiZWGcM5z3ZN12YEdpYyTTjFUuCPFE3zzibXn6i7P6Cwn2B9JC2j5IGAgK4BGG9ks4Oh1BBVyvhMw8SjA1E6ANicBDde_yWhORhXeGsCz1PD0UiG85XkOWfYBRRFYUh_Iwn4VQgHvC65XETevEbcLL_J0FXmKUmnBzVJZP57fvqCk5v90RYED24FkE6Z7iT0qdD0PYXhloA235JWLk7wGKXlndrOusXTWs0A_NuT4U1JnabgnuZHO2K-0sGVvAkfSP8Y8JNb4rseyJTXx9Q9UmYMW ## Examples ### Callouts"
  },
  {
    "objectID": "PD.html",
    "href": "PD.html",
    "title": "Plot Diagrams",
    "section": "",
    "text": "Mimicing “Data to Vis” and “The R Graph Gallery”",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Plot Diagrams"
    ]
  },
  {
    "objectID": "PD.html#clusterings",
    "href": "PD.html#clusterings",
    "title": "Plot Diagrams",
    "section": "Clusterings",
    "text": "Clusterings\nDistribution\n* Violin\n* Density\n* Histograms\n* Boxplot\n* Ridgeline\nRelated: [[20241109103700]] Distribution visualization choice\nCorrolation\n* Scatter\n* Heatmap\n* Correlogram\n* Bubble\n* Connected scatter\n* Density\n* 3D\n* Taylor Diagram - Taylor Diagram - [[20241028134259]] Taylor Diagram NSE vs KGE\nRanking\n* Barplot\n* Spider / Radar\n* Wordcloud\n* Parallel\n* Lollipop\n* Circular Barplot\nPart of a whole * Treemap\n* Venn diagram\n* Doughnut\n* Pie chart\n* Dendrogram\n* Circular packing\n* Sunburst\n* quad\nEvolution\n* Line \n* Area\n* Stacked area\n* Streamchart\nMap\n* Choropleth\n* Hexbin map\n* Cartogram\n* Connection\n* Bubble map\nRelated: Brushes of the landscape\nFlow\n* Chord diagram\n* Network\n* Sankey\n* Arc diagram\n* Edge bundling\n* Hive??",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Plot Diagrams"
    ]
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#flowline-fim",
    "href": "OWP_hydrofabric_flowline_fim.html#flowline-fim",
    "title": "So long as you get the GIS&T of it",
    "section": "Flowline FIM",
    "text": "Flowline FIM\n\n\n\n\n\n\nGoals:\n\nTo walk through the use of prototype flowline data model within the FIM4 codebase.\nDocument the FIM4 Pipeline using this format.\n\nOutcomes and Takeaways:\n\nA deeper understanding of\n\nthe semantics of the hydrofabric flowline model.\nThe operational steps performed in the FIM4 Pipeline.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation (placeholder)\n\n\n\n\nSlide layout:  default     Items linked/bordered in green are cited in the tooltip on hover.\n: narrative-oriented     Items linked/bordered in blue are hyperlinked to relevant resources.\n\n\nPreliminary\n\n\nRunning flowlines through the FIM4 Pipeline Background image from: OWP template"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#some-semantic-distinctions",
    "href": "OWP_hydrofabric_flowline_fim.html#some-semantic-distinctions",
    "title": "So long as you get the GIS&T of it",
    "section": "Some Semantic Distinctions",
    "text": "Some Semantic Distinctions\n\n\nHydrofabric as a concept\n\nThe HY_Features conceptual model attempts to standardize the language we use to describe “whatever exists as part of a network of waterbody and surface depressions and surface channels”.\n\nHydrofabric as a realization\n\nFlowpath: The HY_Features (OGC-16-032r2) engineering report interprets that concept to represent those flowpaths as a linear centerlines\nDivide: Divides are the polygon partitioning of the landscape\nFlowline: A flowline is a one-dimensional (linear) feature that represents a flowing body of water and is functionally similar to a flowpath but does not realize the catchment concept and as such does not have flow from or to a hydrologic nexus. A flowline should be thought of as a hydrographic connector with an inlet and an outlet that does not receive lateral flow from a hydrologic unit."
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#a-proposed-alteration",
    "href": "OWP_hydrofabric_flowline_fim.html#a-proposed-alteration",
    "title": "So long as you get the GIS&T of it",
    "section": "A Proposed Alteration",
    "text": "A Proposed Alteration\n\n\nHydrofabric as a concept\n\nThe HY_Features conceptual model attempts to standardize the language we use to describe “whatever exists as part of a network of waterbody and surface depressions and surface channels”.\n\nHydrofabric as a realization\n\nFlowpath: The HY_Features (OGC-16-032r2) engineering report interprets that concept to represent those flowpaths as a linear centerlines\nDivide: Divides are the polygon partitioning of the landscape\nFlowline: A flowline is a one-dimensional (linear) feature that represents a flowing body of water and is functionally similar to a flowpath but does not realize the catchment concept and as such does not have flow from or to a hydrologic nexus. A flowline should be thought of as a hydrographic connector with an inlet and an outlet that does not receive lateral flow from a hydrologic unit."
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#some-real-world-situations",
    "href": "OWP_hydrofabric_flowline_fim.html#some-real-world-situations",
    "title": "So long as you get the GIS&T of it",
    "section": "Some “Real-world” Situations",
    "text": "Some “Real-world” Situations\n\n\n\n\n\n\n\n\n\nHydrofabric is a contiguous dendritic network Hydralics is modeled as discrete and independent domains"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#the-fim-codebase-process",
    "href": "OWP_hydrofabric_flowline_fim.html#the-fim-codebase-process",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Codebase Process",
    "text": "The FIM Codebase Process"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#the-fim-codebase-process-1",
    "href": "OWP_hydrofabric_flowline_fim.html#the-fim-codebase-process-1",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Codebase Process",
    "text": "The FIM Codebase Process"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section",
    "href": "OWP_hydrofabric_flowline_fim.html#section",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "The FIM process"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-1",
    "href": "OWP_hydrofabric_flowline_fim.html#section-1",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "HAND &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"rem_zeroed_masked_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n\nHAND_values &lt;- terra::values(HAND)\npal &lt;- leaflet::colorNumeric(RColorBrewer::brewer.pal(5, \"Blues\"), c(min(HAND_values,na.rm = TRUE),max(HAND_values,na.rm = TRUE)),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(HAND, colors = pal, opacity = 1,maxBytes=50*1024*1024) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\n\ngg_df &lt;- as.data.frame(HAND,xy = T)\ngglegend &lt;- ggplot2::ggplot() +\n  ggplot2::geom_raster(data = gg_df, ggplot2::aes(x = x, y = y, fill = rem_zeroed_masked_2678000065)) +\n  ggplot2::scale_fill_stepsn(name = \"HAND (m)\",\n                             colors =RColorBrewer::brewer.pal(5, \"Blues\"),\n                             breaks = round(seq(plyr::round_any(min(na.omit(gg_df$rem_zeroed_masked_2678000065)), 10, f = floor),\n                                                plyr::round_any(max(na.omit(gg_df$rem_zeroed_masked_2678000065)), 10, f = ceiling), length = 7), 1),\n                             values = scales::rescale(\n                               seq(plyr::round_any(min(na.omit(gg_df$rem_zeroed_masked_2678000065)), 10, f = floor),\n                                   plyr::round_any(max(na.omit(gg_df$rem_zeroed_masked_2678000065)), 10, f = ceiling), length = 6)))\nllegend &lt;- cowplot::get_legend(gglegend)\ncowplot::save_plot(plot=cowplot::plot_grid(llegend),filename=file.path(path_to_tmp_output,glue::glue(\"test_legend.png\"),fsep = .Platform$file.sep))\n\nmap &lt;- magick::image_read(file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nlegend &lt;- magick::image_read(file.path(path_to_tmp_output,glue::glue(\"test_legend.png\"),fsep = .Platform$file.sep)) %&gt;%\n  magick::image_trim() %&gt;%\n  magick::image_fill(\n    color = \"transparent\",\n    refcolor = \"white\",\n    fuzz = 4,\n    point = \"+1+1\"\n  ) %&gt;%\n  magick::image_scale(\"200\")\nmagick::image_write(image = magick::image_mosaic(c(map, legend)),path=file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_HAND.png\"),fsep = .Platform$file.sep))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfile &lt;- file.path(\"~/data/raw/NWM/20241011/short_range_coastal_atlgulf/nwm.t00z.short_range_coastal.total_water.f013.atlgulf.nc\",fsep=.Platform$file.sep)\nnc_file &lt;- ncdf4::nc_open(file)\nnc_file\n\n\nflowdir_d8_burned_filled &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"flowdir_d8_burned_filled_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\npal &lt;- leaflet::colorFactor(c(\"#862627\",\"#26207a\",\"#6563b7\",\"#6563b7\",\"#cbc6c0\",\"#fee08a\",\"#f4a429\",\"#c2631f\"),c(1,2,3,4,5,6,7,8),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths$levpa_id==branch_polys_selction$levpa_id,],sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(flowdir_d8_burned_filled, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmap1\n\n\n\n\n\nrmarkdown::render(file.path(\"~/Dropbox/root/projects/floodmapping/tools/FrankenFIM/FrankenFIM/vignettes/FIM4_workflow.Rmd\",fsep=.Platform$file.sep),\n                  output_file=file.path(\"~/Dropbox/root/projects/floodmapping/tools/FrankenFIM/FrankenFIM/vignettes/FIM4_flowlines.html\",fsep=.Platform$file.sep))\n   \n\n\n\nunit_to_map &lt;- '01050002'\n\npath_to_output &lt;- file.path(\"~/Dropbox/root/projects/floodmapping/tools/FrankenFIM/FrankenFIM/vignettes/vis\",fsep=.Platform$file.sep)\npath_to_tmp_output &lt;- file.path(path_to_output,\"tmp\",fsep = .Platform$file.sep)\npath_to_maine_stock &lt;- file.path(\"~/Dropbox/root/projects/floodmapping/methods/OWP_FIM/outputs/FIM4_maine/\",fsep = .Platform$file.sep)\n\npath_to_fim4_unit_outputs &lt;- path_to_maine_stock\n\nall_hucs &lt;- sf::st_read(file.path(\"~/Dropbox/root/database/hosted/water/HUC8.fgb\",fsep = .Platform$file.sep))\n\nwbd8_clp_inputs &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"wbd8_clp.gpkg\",fsep=.Platform$file.sep))\nlevelpaths &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"nwm_subset_streams_levelPaths.gpkg\",fsep = .Platform$file.sep))\nnearby_hucs &lt;- sf::st_transform(all_hucs,sf::st_crs(wbd8_clp_inputs))[wbd8_clp_inputs %&gt;% sf::st_buffer(1000),]\n\ntest_pkg &lt;- file.path(\"~/data/temp/test.gpkg\",fsep = .Platform$file.sep)\nhf_flowpaths &lt;- sf::read_sf(test_pkg, \"flowpaths\")\nhf_divides &lt;-  sf::read_sf(test_pkg, \"divides\")\nhf_flowlines &lt;- sf::read_sf(test_pkg, \"flowlines\")\nhf_network &lt;- sf::read_sf(test_pkg, \"network\")\n\nhuc_flowpaths &lt;- hf_flowpaths[wbd8_clp_inputs[4,],]\nhuc_divides &lt;- hf_divides[hf_divides$id %in% huc_flowpaths$divide_id,]\nhuc_network &lt;- hf_network[hf_network$divide_id %in% huc_flowpaths$divide_id,]\ng = dplyr::select(huc_network, id, toid) %&gt;%\n  dplyr::distinct() %&gt;%\n  igraph::graph_from_data_frame(directed = TRUE) \nvisNetwork::visIgraph(g) %&gt;%\n  visNetwork::visOptions(highlightNearest = list(enabled = T, hover = T), nodesIdSelection = T)\n\nhuc_flowpaths &lt;- hf_flowpaths[wbd8_clp_inputs[4,],]\nhuc_divides &lt;- hf_divides[hf_divides$id %in% huc_flowpaths$divide_id,]\nhuc_network &lt;- hf_network[hf_network$divide_id %in% huc_flowpaths$divide_id,]\n\nhf_bounds &lt;- sf::st_bbox(sf::st_transform(huc_divides,sf::st_crs('EPSG:4326'))) %&gt;% unlist() %&gt;% unname()\nhf_divide_values &lt;- unique(huc_divides$divide_id)\nhf_divide_pal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(hf_divide_values)),hf_divide_values,na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(huc_divides,sf::st_crs('EPSG:4326')),stroke = TRUE, opacity = 0.8,fillOpacity = 1,fill = TRUE,weight = 1,fillColor=hf_divide_pal(huc_divides$divide_id)) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(huc_flowpaths,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"blue\") %&gt;%\n  # leaflet::addLabelOnlyMarkers(data=sf::st_transform(forecast_basins %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n  #                              label = ~`ID`,\n  #                              labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n  #                              group=\"Index Labels\") %&gt;%\n  leaflet::fitBounds(hf_bounds[3],hf_bounds[2],hf_bounds[1],hf_bounds[4])\n\n\n\n\ndem_meters &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"dem_meters_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n\ngg_df &lt;- as.data.frame(dem_meters,xy = T)\ngglegend &lt;- ggplot2::ggplot() +\n  ggplot2::geom_raster(data = gg_df, ggplot2::aes(x = x, y = y, fill = dem_meters_2678000065)) +\n  ggplot2::scale_fill_stepsn(name = \"3DEP 10m (m)\", \n                             colors =c(\"forestgreen\",\"yellow\",\"tan\",\"brown\"),\n                             breaks = round(seq(plyr::round_any(min(na.omit(gg_df$dem_meters_2678000065)), 10, f = floor),\n                                          plyr::round_any(max(na.omit(gg_df$dem_meters_2678000065)), 10, f = ceiling), length = 7), 1),\n                             values = scales::rescale(\n                               seq(plyr::round_any(min(na.omit(gg_df$dem_meters_2678000065)), 10, f = floor),\n                                   plyr::round_any(max(na.omit(gg_df$dem_meters_2678000065)), 10, f = ceiling), length = 6))) \ngglegend\nllegend &lt;- cowplot::get_legend(gglegend)\ncowplot::save_plot(plot=cowplot::plot_grid(llegend),filename=file.path(path_to_tmp_output,glue::glue(\"test_legend.png\"),fsep = .Platform$file.sep))\npal &lt;- leaflet::colorNumeric(c(\"forestgreen\",\"yellow\",\"tan\",\"brown\"), terra::values(dem_meters),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths$levpa_id==branch_polys_selction$levpa_id,],sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  # leafem::addGeoRaster(dem_meters,colorOptions = leafem:::colorOptions(palette = pal,\n  #                                                                      # breaks = as.numeric(c(0:xx$max_ffreq)),\n  #                                                                      domain = c(dem_meters_values_min,dem_meters_values_max))) %&gt;%\n  # leafem::addGeoRaster(dem_meters,color= pal) %&gt;%\n  leaflet::addRasterImage(dem_meters, colors = pal, opacity = 0.9,maxBytes=42*1024*1024) %&gt;%\n  # leaflet::addLegend(\"bottomleft\",title = \"3DEP 10m (m)\", pal = pal, values = terra::values(dem_meters)) %&gt;% \n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\nmap1\n\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nmap &lt;- magick::image_read(file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nlegend &lt;- magick::image_read(file.path(path_to_tmp_output,glue::glue(\"test_legend.png\"),fsep = .Platform$file.sep)) %&gt;% \n  magick::image_trim() %&gt;% \n  magick::image_fill(\n    color = \"transparent\", \n    refcolor = \"white\", \n    fuzz = 4,\n    point = \"+1+1\"\n  ) %&gt;% \n  magick::image_scale(\"200\")\nmagick::image_write(image = magick::image_mosaic(c(map, legend)),path=file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\n\n\nVPU01 inputs"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-2",
    "href": "OWP_hydrofabric_flowline_fim.html#section-2",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "FIM is a HUC level function"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-3",
    "href": "OWP_hydrofabric_flowline_fim.html#section-3",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "run_unit_wb.sh\n\n## DERIVE LEVELPATH  ##\necho -e $startDiv\"Generating Level Paths for $hucNumber\"\n$srcDir/derive_level_paths.py -i $tempHucDataDir/nwm_subset_streams.gpkg \\\n    -s $tempHucDataDir/wbd_buffered_streams.gpkg \\\n    -b $branch_id_attribute \\\n    -r \"ID\" \\\n    -o $tempHucDataDir/nwm_subset_streams_levelPaths.gpkg \\\n    -d $tempHucDataDir/nwm_subset_streams_levelPaths_dissolved.gpkg \\\n    -de $tempHucDataDir/nwm_subset_streams_levelPaths_dissolved_extended.gpkg \\\n    -e $tempHucDataDir/nwm_headwaters.gpkg \\\n    -c $tempHucDataDir/nwm_catchments_proj_subset.gpkg \\\n    -t $tempHucDataDir/nwm_catchments_proj_subset_levelPaths.gpkg \\\n    -n $tempHucDataDir/nwm_subset_streams_levelPaths_dissolved_headwaters.gpkg \\\n    -w $tempHucDataDir/nwm_lakes_proj_subset.gpkg \\\n    -wbd $tempHucDataDir/wbd.gpkg \\\n    -u $hucNumber\n\n\n\nrun_unit_wb.sh\n\n## STREAM BRANCH POLYGONS\necho -e $startDiv\"Generating Stream Branch Polygons for $hucNumber\"\n$srcDir/buffer_stream_branches.py -a $tempHucDataDir/$dem_domain_filename \\\n    -s $tempHucDataDir/nwm_subset_streams_levelPaths_dissolved.gpkg \\\n    -i $branch_id_attribute \\\n    -d $branch_buffer_distance_meters \\\n    -b $tempHucDataDir/branch_polygons.gpkg\n\n\n\n\n\n\nThe FIM process"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-4",
    "href": "OWP_hydrofabric_flowline_fim.html#section-4",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "The FIM process"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-5",
    "href": "OWP_hydrofabric_flowline_fim.html#section-5",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "The FIM process"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-6",
    "href": "OWP_hydrofabric_flowline_fim.html#section-6",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "The FIM process"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-7",
    "href": "OWP_hydrofabric_flowline_fim.html#section-7",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "run_unit_wb.sh\n\n## PRODUCE BRANCH ZERO HAND\n$srcDir/delineate_hydros_and_produce_HAND.sh \"unit\""
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-8",
    "href": "OWP_hydrofabric_flowline_fim.html#section-8",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "delineate_hydros_and_produce_HAND.sh\n\n## Level is equal to the parent script: 'unit' or 'branch'\nlevel=$1\n\nif [ \"$level\" = \"branch\" ]; then\n    b_arg=$tempCurrentBranchDataDir/nwm_subset_streams_levelPaths_$current_branch_id.gpkg\n    z_arg=$tempCurrentBranchDataDir/nwm_catchments_proj_subset_levelPaths_$current_branch_id.gpkg\nelif [ \"$level\" = \"unit\" ]; then\n    # Branch zero has a different source for -b and -z arguments\n    b_arg=$tempHucDataDir/nwm_subset_streams.gpkg\n    z_arg=$tempHucDataDir/nwm_catchments_proj_subset.gpkg\nfi"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-9",
    "href": "OWP_hydrofabric_flowline_fim.html#section-9",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "delineate_hydros_and_produce_HAND.sh\n\n## D8 FLOW ACCUMULATIONS ##\necho -e $startDiv\"D8 Flow Accumulations $hucNumber $current_branch_id\"\npython3 $srcDir/accumulate_headwaters.py \\\n    -fd $tempCurrentBranchDataDir/flowdir_d8_burned_filled_$current_branch_id.tif \\\n    -fa $tempCurrentBranchDataDir/flowaccum_d8_burned_filled_$current_branch_id.tif \\\n    -wg $tempCurrentBranchDataDir/headwaters_$current_branch_id.tif \\\n    -stream $tempCurrentBranchDataDir/demDerived_streamPixels_$current_branch_id.tif \\\n    -thresh 1"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-10",
    "href": "OWP_hydrofabric_flowline_fim.html#section-10",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "delineate_hydros_and_produce_HAND.sh\n\n## PREPROCESSING FOR LATERAL THALWEG ADJUSTMENT ###\necho -e $startDiv\"Preprocessing for lateral thalweg adjustment $hucNumber $current_branch_id\"\npython3 $srcDir/unique_pixel_and_allocation.py \\\n    -s $tempCurrentBranchDataDir/demDerived_streamPixels_$current_branch_id.tif \\\n    -o $tempCurrentBranchDataDir/demDerived_streamPixels_ids_$current_branch_id.tif\n\n\n\ndelineate_hydros_and_produce_HAND.sh\n\n## ADJUST THALWEG MINIMUM USING LATERAL ZONAL MINIMUM ##\necho -e $startDiv\"Performing lateral thalweg adjustment $hucNumber $current_branch_id\"\npython3 $srcDir/adjust_thalweg_lateral.py \\\n    -e $tempCurrentBranchDataDir/dem_meters_$current_branch_id.tif \\\n    -s $tempCurrentBranchDataDir/demDerived_streamPixels_$current_branch_id.tif \\\n    -a $tempCurrentBranchDataDir/demDerived_streamPixels_ids_\"$current_branch_id\"_allo.tif \\\n    -d $tempCurrentBranchDataDir/demDerived_streamPixels_ids_\"$current_branch_id\"_dist.tif \\\n    -t 50 \\\n    -o $tempCurrentBranchDataDir/dem_lateral_thalweg_adj_$current_branch_id.tif \\\n    -th $thalweg_lateral_elev_threshold\n\n\n\ndelineate_hydros_and_produce_HAND.sh\n\n## MASK BURNED DEM FOR STREAMS ONLY ###\necho -e $startDiv\"Mask Burned DEM for Thalweg Only $hucNumber $current_branch_id\"\ngdal_calc.py --quiet --type=Int32 --overwrite --co \"COMPRESS=LZW\" --co \"BIGTIFF=YES\" --co \"TILED=YES\" \\\n    -A $tempCurrentBranchDataDir/flowdir_d8_burned_filled_$current_branch_id.tif \\\n    -B $tempCurrentBranchDataDir/demDerived_streamPixels_$current_branch_id.tif \\\n    --calc=\"A*B\" \\\n    --outfile=\"$tempCurrentBranchDataDir/flowdir_d8_burned_filled_flows_$current_branch_id.tif\" \\\n    --NoDataValue=0"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-11",
    "href": "OWP_hydrofabric_flowline_fim.html#section-11",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "delineate_hydros_and_produce_HAND.sh\n\n## GAGE WATERSHED FOR PIXELS ##\necho -e $startDiv\"Gage Watershed for Pixels $hucNumber $current_branch_id\"\nmpiexec -n $ncores_gw $taudemDir/gagewatershed \\\n    -p $tempCurrentBranchDataDir/flowdir_d8_burned_filled_\"$current_branch_id\".tif \\\n    -gw $tempCurrentBranchDataDir/gw_catchments_pixels_$current_branch_id.tif \\\n    -o $tempCurrentBranchDataDir/flows_points_pixels_$current_branch_id.gpkg \\\n    -id $tempCurrentBranchDataDir/idFile_$current_branch_id.txt"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#section-12",
    "href": "OWP_hydrofabric_flowline_fim.html#section-12",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "delineate_hydros_and_produce_HAND.sh\n\n## D8 REM ##\necho -e $startDiv\"D8 REM $hucNumber $current_branch_id\"\n$srcDir/make_rem.py -d $tempCurrentBranchDataDir/dem_thalwegCond_\"$current_branch_id\".tif \\\n    -w $tempCurrentBranchDataDir/gw_catchments_pixels_$current_branch_id.tif \\\n    -o $tempCurrentBranchDataDir/rem_$current_branch_id.tif \\\n    -t $tempCurrentBranchDataDir/demDerived_streamPixels_$current_branch_id.tif"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#the-implications-of-a-reference-fabric-flowline-fim",
    "href": "OWP_hydrofabric_flowline_fim.html#the-implications-of-a-reference-fabric-flowline-fim",
    "title": "So long as you get the GIS&T of it",
    "section": "The Implications of a Reference Fabric Flowline FIM",
    "text": "The Implications of a Reference Fabric Flowline FIM\n\n\n\n\n\nrun_unit_wb.sh: Cuts 12% and 4 steps: ~32/253 seconds  delineate_hydros_and_produce_HAND.sh: Cuts n*(11% and 3 steps): ~67/584 seconds  See teh FIM4 Pipeline in even more detail here"
  },
  {
    "objectID": "OWP_hydrofabric_flowline_fim.html#whats-next",
    "href": "OWP_hydrofabric_flowline_fim.html#whats-next",
    "title": "So long as you get the GIS&T of it",
    "section": "What’s next?",
    "text": "What’s next?\n\n\n\nOutcomes and Takeaways:\n\nA deeper understanding of\n\nthe semantics of the hydrofabric flowline model\nThe operational steps performed in the FIM4 Pipeline\n\n\nNext Steps:\n\nDemonstrate the ability to couple explicit and implicit forecasts across the network.\nExplore/quantify impacts of this altered access pattern.\nSolve Hydrology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation (placeholder)\n\n\n\n\nThis is an active WIP, please reach out and help us!"
  },
  {
    "objectID": "OV.html#how-to",
    "href": "OV.html#how-to",
    "title": "Overlay",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Math",
      "Overlay"
    ]
  },
  {
    "objectID": "NWM.html",
    "href": "NWM.html",
    "title": "National Water Model",
    "section": "",
    "text": "fho"
  },
  {
    "objectID": "NWM.html#quick-links",
    "href": "NWM.html#quick-links",
    "title": "National Water Model",
    "section": "Quick links:",
    "text": "Quick links:\nFlood Hazard Outlook (pictured IMAGE LIVE UPDATES) | National Water Center Products | NWS GIS Map"
  },
  {
    "objectID": "NWM.html#executive-summary",
    "href": "NWM.html#executive-summary",
    "title": "National Water Model",
    "section": "Executive Summary",
    "text": "Executive Summary\nPrior to 2016, the Nation relied on a piecemeal approach to forecasting the operational condition of our nations waterways; leaning on the local expertise of regional river forecast centers and the National weather service. With the release of the National Water Model, run out of the Office of Water Prediction in Tuscaloosa Alabama, the nation suddenly had a single, unified picture of the nations water ways at an unprecedented scale and resolution. Since then, improvements have enabled different model formulations to be run simultaneously which allows for the right model to be used in the right geography for the right reasons, greatly increasing the explainability of these predictions. These and other outputs from OWP are also what enable continental-scale flood inundation and impact mapping efforts. As a framework and particular implementation of the BMI standard and not any one single model, the National Water Model is currently composed of models like SCHISM and T-Route, and the outputs of these runs are used to drive FIM workflows."
  },
  {
    "objectID": "NWM.html#howto",
    "href": "NWM.html#howto",
    "title": "National Water Model",
    "section": "HowTo",
    "text": "HowTo"
  },
  {
    "objectID": "NWM.html#tutorials",
    "href": "NWM.html#tutorials",
    "title": "National Water Model",
    "section": "Tutorials",
    "text": "Tutorials"
  },
  {
    "objectID": "NWM.html#explanation",
    "href": "NWM.html#explanation",
    "title": "National Water Model",
    "section": "explanation",
    "text": "explanation\nConcepts across the National Water Model: https://water.noaa.gov/about/nwm\nReference time: The time that the model is forecasting (At 12 z the rivers all had this value)"
  },
  {
    "objectID": "NWM.html#references",
    "href": "NWM.html#references",
    "title": "National Water Model",
    "section": "References",
    "text": "References\n\nPull NWM data from Google Cloud Platform\nGCP Bucket: link\n```{google cloud SDK shell}\ngsutil -m cp -r \"gs://national-water-model/nwm.{yyyymmdd}/short_range_coastal_atlgulf/\" G:\\data\\raw\\NWM\\{yyyymmdd}\\short_range_coastal_atlgulf\\\ngsutil -m cp -r \"gs://national-water-model/nwm.{yyyymmdd}/short_range_coastal_atlgulf_psurge/\" G:\\data\\raw\\NWM\\{yyyymmdd}\\short_range_coastal_atlgulf_psurge\\\n```\nAnd then push them to an open bucket so that the rest of the team can get to them.\n```{aws}\naws s3 sync G:\\data\\raw\\NWM\\{yyyymmdd}\\ s3://captian_filecopy/silly/NWM/{yyyymmdd}/\n```\n\n\nRetro data\nOther sources: https://github.com/NOAA-OWP/hydrotools/issues/157\n\n\n  version type                    ncml\n1     1.2 USGS     nwm_retro_full.ncml\n2     2.0 USGS  nwm_v2_retro_full.ncml\n3     2.1 USGS nwm_v21_retro_full.ncml\n                                                                               url\n1     https://cida.usgs.gov/thredds/dodsC/demo/morethredds/nwm/nwm_retro_full.ncml\n2  https://cida.usgs.gov/thredds/dodsC/demo/morethredds/nwm/nwm_v2_retro_full.ncml\n3 https://cida.usgs.gov/thredds/dodsC/demo/morethredds/nwm/nwm_v21_retro_full.ncml\n            startDate             endDate                            time_units\n1 1993-01-01 00:00:00 2017-12-31 23:00:00 minutes since 1970-01-01 00:00:00 UTC\n2 1993-01-01 00:00:00 2018-12-31 23:00:00   Hours since 1970-01-01 00:00:00 UTC\n3 1979-02-01 01:00:00 2020-12-31 23:00:00 minutes since 1970-01-01 00:00:00 UTC\n\n\n\n\nForcings\n\n\nRandom code fragments"
  },
  {
    "objectID": "NBIHydrolocation.html#section",
    "href": "NBIHydrolocation.html#section",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "NBI Hydrolocation Considerations\n\nGoals:\n\nOrient ourselves to the National Bridge Inventory (NBI)\nDocument the conflation process applied to create hydrolocations\nProvide database\n\nOutcomes:\n\nA better understanding of the data and processing algorithm applied in the creation of a national database\nA file useful for hydrofabric network creation and OSM/NBI ID conflation workflows\n\n\n\n\n\n\nFor: Background images from:\n\n\nOWP presentation template background: ./_Water_color.png or ./_Water_bw.jpg - “cover”\n\n\nMy own efforts from Water PNGs by Vecteezy, https://www.vecteezy.com/members/ahasanaraakter: _vecteezy_ai-generated-water-wave-splash.png / _vecteezy_ai-generated-water-wave-splash_bw.png / _vecteezy_ai-generated-water-wave-splash_bw_blur.png - “contain”"
  },
  {
    "objectID": "NBIHydrolocation.html#presentation-navigation-tips",
    "href": "NBIHydrolocation.html#presentation-navigation-tips",
    "title": "So long as you get the GIS&T of it",
    "section": "Presentation Navigation Tips",
    "text": "Presentation Navigation Tips\n\n\n\nSlide layout : This deck has one axis, use any key to advance.\n: This deck is narrative-oriented\n\nItems linked/bordered in green are cited in the tooltip on hover.\nItems linked/bordered in blue are hyperlinked to relevant resources.\n\n Photos are Allowed |  Questions are Encouraged\n: ~5 minutes | Last updated:08/25/2025 20:49:09\n! PLEASE !\nInterrupt me and ask questions or clarifications.\nI’m here to talk with you now, not to these slides.\n\n\nControl tips\n\nMy preferred (FOSS) flavor of slidedecks, revealjs, has intuitive but none the less unconventional PowerPoint presentation controls:\n\nSlides dynamically resize to use the entirety of the browser window, but you can still fullscreen with F.\n\nThis slide has a red border indicating the content extent.\n\nSlide navigation is mode dependent. If there are vertical slides, press space, N, or the down arrow key, not the right arrow to advance slides\nPress M to open to the menu, Press O for the slide deck overview, Press B to black out the presentation screen, Press S for a speaker view.\nYou can use the chalkboard to freemouse/touchpad draw.\nSlides should render as designed1 but you can press Alt/Opt + click on the slide to zoom in. Increase text size with Alt/Opt + +, Alt/Opt + - to decrease, and Alt/Opt + 0 reset to the default scale.\nPress C to declare victory and head home.2\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\n\n\n\n\nSpeaker notes\n\ntested in an updated version of google chromeA favored quote from one of my giants: Dr. David Maidment"
  },
  {
    "objectID": "NBIHydrolocation.html#what-is-nbi",
    "href": "NBIHydrolocation.html#what-is-nbi",
    "title": "So long as you get the GIS&T of it",
    "section": "What is NBI",
    "text": "What is NBI\nThe National Bridge Inventory (NBI) is our nations way of accounting for and coarsely administering the nations critical bridges. This inventory is used by state and federal partners and provides “the Recording and Coding Guide for the Structure Inventory and Appraisal of the Nation’s Bridges…” as administered by the US Federal Highway Administration (FHWA). This data can be accessed (in ASCII form) from here.\n\n\n   STATE_CODE_001 STRUCTURE_NUMBER_008 RECORD_TYPE_005A ROUTE_PREFIX_005B\n            &lt;int&gt;               &lt;char&gt;           &lt;char&gt;             &lt;int&gt;\n1:              1      00000000000S702                1                 6\n2:              1      00000000000S703                1                 6\n3:              1      0000000000M0022                1                 8\n4:              1      000000883039900                1                 4\n5:              1      000001014002450                1                 3\n6:              1      000001331700710                1                 6\n   SERVICE_LEVEL_005C ROUTE_NUMBER_005D DIRECTION_005E HIGHWAY_DISTRICT_002\n                &lt;int&gt;            &lt;char&gt;          &lt;int&gt;               &lt;char&gt;\n1:                  1             04007              0                   09\n2:                  1             04003              0                   09\n3:                  1             00000              0                   00\n4:                  1             00088              0                   02\n5:                  1             00101              0                   02\n6:                  2             00000              0                   02\n   COUNTY_CODE_003 PLACE_CODE_004        FEATURES_DESC_006A\n             &lt;int&gt;          &lt;int&gt;                    &lt;char&gt;\n1:              53              0           'PERDIDO CREEK'\n2:              53           3004           'PERDIDO CREEK'\n3:             113          19000             'UCHEE CREEK'\n4:              59              0 'LITTLE BEAR CR. DAM SPW'\n5:              79              0         'TENNESSEE RIVER'\n6:              33              0         'TENNESSEE RIVER'\n   CRITICAL_FACILITY_006B FACILITY_CARRIED_007              LOCATION_009\n                   &lt;lgcl&gt;               &lt;char&gt;                    &lt;char&gt;\n1:                     NA   'IRR BIA RTE 4007' '11.4KM NW OF ATMORE  AL'\n2:                     NA   'IRR BIA RTE 4003'      'ON LYNN MCGEE ROAD'\n3:                     NA  '101ST AIRBORNE RD' '3.4 KM S OF SUNSHINE RD'\n4:                     NA          'CO. RD 88'      'LITTLE BEAR CR DAM'\n5:                     NA             'SR 101'          'ON WHEELER DAM'\n6:                     NA             'Res Rd'           'ON WILSON DAM'\n   MIN_VERT_CLR_010 KILOPOINT_011 BASE_HWY_NETWORK_012 LRS_INV_ROUTE_013A\n              &lt;num&gt;         &lt;num&gt;                &lt;int&gt;             &lt;char&gt;\n1:            99.99         1.500                    0         0000000000\n2:            99.99         1.100                    0         0000000000\n3:            99.99         0.000                    0                   \n4:            99.99         0.000                    0                   \n5:             4.52        39.429                    0                   \n6:            99.99         0.000                    0                   \n   SUBROUTE_NO_013B  LAT_016 LONG_017 DETOUR_KILOS_019 TOLL_020 MAINTENANCE_021\n              &lt;int&gt;    &lt;num&gt;    &lt;num&gt;            &lt;int&gt;    &lt;int&gt;           &lt;int&gt;\n1:                0 31061094 87341348                3        3              62\n2:                0 31062020 87340890                3        3              62\n3:               NA 32174330 84583799               18        3              74\n4:               NA 34270600 87583100               16        3              67\n5:               NA 34485200 87225400               42        3              67\n6:               NA 34482400 87374000                6        3              67\n   OWNER_022 FUNCTIONAL_CLASS_026 YEAR_BUILT_027 TRAFFIC_LANES_ON_028A\n       &lt;int&gt;                &lt;int&gt;          &lt;int&gt;                 &lt;int&gt;\n1:        62                    9           1999                     2\n2:        62                    9           2002                     2\n3:        74                    9           1942                     2\n4:        67                    8           1974                     2\n5:        67                    6           1937                     2\n6:        67                   17           1924                     2\n   TRAFFIC_LANES_UND_028B ADT_029 YEAR_ADT_030 DESIGN_LOAD_031\n                    &lt;int&gt;   &lt;int&gt;        &lt;int&gt;          &lt;char&gt;\n1:                      0      50         2021               5\n2:                      0     159         2021               5\n3:                      0     375         2021               0\n4:                      0     205         2021               5\n5:                      0    5358         2021               4\n6:                      2    2841         2021               4\n   APPR_WIDTH_MT_032 MEDIAN_CODE_033 DEGREES_SKEW_034 STRUCTURE_FLARED_035\n               &lt;num&gt;           &lt;int&gt;            &lt;int&gt;                &lt;int&gt;\n1:               9.7               0               30                    0\n2:               6.5               0                0                    0\n3:              10.4               0                0                    0\n4:              11.6               0               99                    0\n5:               7.9               0                0                    0\n6:               7.9               0                0                    0\n   RAILINGS_036A TRANSITIONS_036B APPR_RAIL_036C APPR_RAIL_END_036D HISTORY_037\n          &lt;char&gt;           &lt;char&gt;         &lt;char&gt;             &lt;char&gt;       &lt;int&gt;\n1:             1                1              0                  1           5\n2:             1                1              1                  1           5\n3:             0                0              1                  1           5\n4:             1                1              1                  1           5\n5:             1                0              0                  0           4\n6:             1                1              1                  1           1\n   NAVIGATION_038 NAV_VERT_CLR_MT_039 NAV_HORR_CLR_MT_040\n           &lt;char&gt;               &lt;num&gt;               &lt;num&gt;\n1:              0                 0.0                 0.0\n2:              0                 0.0                 0.0\n3:              0                 0.0                 0.0\n4:              N                 0.0                 0.0\n5:              1                17.9                33.5\n6:              1                17.3                33.5\n   OPEN_CLOSED_POSTED_041 SERVICE_ON_042A SERVICE_UND_042B STRUCTURE_KIND_043A\n                   &lt;char&gt;           &lt;int&gt;            &lt;int&gt;               &lt;int&gt;\n1:                      A               1                5                   5\n2:                      A               1                5                   5\n3:                      P               1                5                   1\n4:                      A               1                9                   5\n5:                      A               5                5                   3\n6:                      A               5                6                   4\n   STRUCTURE_TYPE_043B APPR_KIND_044A APPR_TYPE_044B MAIN_UNIT_SPANS_045\n                 &lt;int&gt;          &lt;int&gt;          &lt;int&gt;               &lt;int&gt;\n1:                   5              0              0                   1\n2:                   1              0              0                   1\n3:                   4              0              0                   3\n4:                   5              0              0                   5\n5:                  10              3              2                   2\n6:                   3              1             20                   8\n   APPR_SPANS_046 HORR_CLR_MT_047 MAX_SPAN_LEN_MT_048 STRUCTURE_LEN_MT_049\n            &lt;int&gt;           &lt;num&gt;               &lt;num&gt;                &lt;num&gt;\n1:              0             9.7                14.7                 15.0\n2:              0             9.7                12.9                 13.6\n3:              0             7.3                18.3                 43.0\n4:              0             8.6                14.0                 65.5\n5:            143             6.0                53.3               1981.2\n6:             78             6.0                46.9               1512.4\n   LEFT_CURB_MT_050A RIGHT_CURB_MT_050B ROADWAY_WIDTH_MT_051 DECK_WIDTH_MT_052\n               &lt;num&gt;              &lt;num&gt;                &lt;num&gt;             &lt;num&gt;\n1:               0.0                0.0                  9.7               9.7\n2:               0.3                0.3                  9.1               9.8\n3:               0.6                0.6                  7.3               9.0\n4:               0.0                0.0                  8.7              10.5\n5:               0.9                0.2                  6.1               7.8\n6:               1.7                0.5                  6.1               8.3\n   VERT_CLR_OVER_MT_053 VERT_CLR_UND_REF_054A VERT_CLR_UND_054B\n                  &lt;num&gt;                &lt;char&gt;             &lt;num&gt;\n1:                99.99                     N              0.00\n2:                99.99                     N              0.00\n3:                99.99                     N              0.00\n4:                99.99                     N              0.00\n5:                 4.52                     N              0.00\n6:                99.99                     H             12.19\n   LAT_UND_REF_055A LAT_UND_MT_055B LEFT_LAT_UND_MT_056 DECK_COND_058\n             &lt;char&gt;           &lt;num&gt;               &lt;num&gt;        &lt;char&gt;\n1:                N               0                   0             7\n2:                N               0                   0             7\n3:                N               0                   0             5\n4:                N               0                   0             7\n5:                N               0                   0             5\n6:                H               6                   0             5\n   SUPERSTRUCTURE_COND_059 SUBSTRUCTURE_COND_060 CHANNEL_COND_061\n                    &lt;char&gt;                &lt;char&gt;           &lt;char&gt;\n1:                       5                     7                6\n2:                       6                     7                6\n3:                       5                     6                6\n4:                       7                     7                7\n5:                       6                     5                8\n6:                       5                     5                8\n   CULVERT_COND_062 OPR_RATING_METH_063 OPERATING_RATING_064\n             &lt;char&gt;              &lt;char&gt;                &lt;num&gt;\n1:                N                   2                 44.5\n2:                N                   2                 84.8\n3:                N                   0                 32.7\n4:                N                   1                 44.2\n5:                N                   1                 40.0\n6:                N                   1                 58.4\n   INV_RATING_METH_065 INVENTORY_RATING_066 STRUCTURAL_EVAL_067\n                &lt;char&gt;                &lt;num&gt;              &lt;char&gt;\n1:                   2                 32.7                   5\n2:                   2                 35.1                   6\n3:                   0                 22.5                   5\n4:                   1                 26.6                   6\n5:                   1                 23.9                   5\n6:                   1                 35.0                   5\n   DECK_GEOMETRY_EVAL_068 UNDCLRENCE_EVAL_069 POSTING_EVAL_070\n                   &lt;char&gt;              &lt;char&gt;            &lt;int&gt;\n1:                      7                   N                5\n2:                      6                   N                5\n3:                      5                   N                2\n4:                      6                   N                5\n5:                      2                   N                5\n6:                      2                   9                5\n   WATERWAY_EVAL_071 APPR_ROAD_EVAL_072 WORK_PROPOSED_075A WORK_DONE_BY_075B\n              &lt;char&gt;              &lt;int&gt;              &lt;int&gt;             &lt;int&gt;\n1:                 7                  8                 NA                NA\n2:                 6                  8                 NA                NA\n3:                 8                  6                 38                 1\n4:                 9                  6                 NA                NA\n5:                 8                  8                 31                 1\n6:                 9                  3                 31                 1\n   IMP_LEN_MT_076 DATE_OF_INSPECT_090 INSPECT_FREQ_MONTHS_091 FRACTURE_092A\n            &lt;num&gt;               &lt;int&gt;                   &lt;int&gt;        &lt;char&gt;\n1:            0.0                 221                      24             N\n2:            0.0                 221                      24             N\n3:           52.8                1021                      24             N\n4:             NA                 622                      24             N\n5:         1981.2                 621                      24           Y24\n6:         1512.4                 622                      24           Y24\n   UNDWATER_LOOK_SEE_092B SPEC_INSPECT_092C FRACTURE_LAST_DATE_093A\n                   &lt;char&gt;            &lt;char&gt;                   &lt;int&gt;\n1:                      N                 N                      NA\n2:                      N                 N                      NA\n3:                    Y60                 N                      NA\n4:                      N                 N                      NA\n5:                      N                 N                     621\n6:                      N                 N                     622\n   UNDWATER_LAST_DATE_093B SPEC_LAST_DATE_093C BRIDGE_IMP_COST_094\n                     &lt;int&gt;               &lt;int&gt;               &lt;int&gt;\n1:                      NA                  NA                   0\n2:                      NA                  NA                   0\n3:                    1021                  NA                   1\n4:                      NA                  NA                  NA\n5:                      NA                  NA               50000\n6:                      NA                  NA               35000\n   ROADWAY_IMP_COST_095 TOTAL_IMP_COST_096 YEAR_OF_IMP_097\n                  &lt;int&gt;              &lt;int&gt;           &lt;int&gt;\n1:                    0                  0            2021\n2:                    0                  0            2021\n3:                    1                  2            2021\n4:                   NA                 NA              NA\n5:                 2000              60000              NA\n6:                 2000              45000              NA\n   OTHER_STATE_CODE_098A OTHER_STATE_PCNT_098B OTHR_STATE_STRUC_NO_099\n                  &lt;char&gt;                 &lt;int&gt;                  &lt;char&gt;\n1:                                           0                        \n2:                                           0                        \n3:                                          NA                        \n4:                                          NA                        \n5:                                          NA                        \n6:                                          NA                        \n   STRAHNET_HIGHWAY_100 PARALLEL_STRUCTURE_101 TRAFFIC_DIRECTION_102\n                  &lt;int&gt;                 &lt;char&gt;                 &lt;int&gt;\n1:                    0                      N                     2\n2:                    0                      N                     2\n3:                    0                      N                     2\n4:                    0                      N                     2\n5:                    0                      N                     2\n6:                    0                      N                     2\n   TEMP_STRUCTURE_103 HIGHWAY_SYSTEM_104 FEDERAL_LANDS_105\n               &lt;char&gt;              &lt;int&gt;             &lt;int&gt;\n1:                                     0                 1\n2:                                     0                 1\n3:                                     0                 0\n4:                                     0                 0\n5:                                     0                 0\n6:                                     0                 0\n   YEAR_RECONSTRUCTED_106 DECK_STRUCTURE_TYPE_107 SURFACE_TYPE_108A\n                    &lt;int&gt;                  &lt;char&gt;            &lt;char&gt;\n1:                      0                       2                 6\n2:                      0                       2                 6\n3:                      0                       1                 1\n4:                      0                       1                 0\n5:                   1962                       1                 0\n6:                   1958                       1                 5\n   MEMBRANE_TYPE_108B DECK_PROTECTION_108C PERCENT_ADT_TRUCK_109\n               &lt;char&gt;               &lt;char&gt;                 &lt;int&gt;\n1:                  0                    1                     1\n2:                  0                    8                     1\n3:                  0                    0                    35\n4:                  0                    0                     5\n5:                  0                    0                    10\n6:                  0                    0                     8\n   NATIONAL_NETWORK_110 PIER_PROTECTION_111 BRIDGE_LEN_IND_112\n                  &lt;int&gt;               &lt;int&gt;             &lt;char&gt;\n1:                    0                  NA                  Y\n2:                    0                  NA                  Y\n3:                    0                  NA                  Y\n4:                    0                  NA                  Y\n5:                    0                   1                  Y\n6:                    0                   1                  Y\n   SCOUR_CRITICAL_113 FUTURE_ADT_114 YEAR_OF_FUTURE_ADT_115 MIN_NAV_CLR_MT_116\n               &lt;char&gt;          &lt;int&gt;                  &lt;int&gt;              &lt;num&gt;\n1:                  8             59                   2041                  0\n2:                  7            200                   2041                  0\n3:                  5            400                   2043                 NA\n4:                  5            250                   2041                 NA\n5:                  5           8000                   2041                 NA\n6:                  5           3000                   2041                 NA\n   FED_AGENCY SUBMITTED_BY BRIDGE_CONDITION LOWEST_RATING DECK_AREA\n       &lt;char&gt;        &lt;int&gt;           &lt;char&gt;        &lt;char&gt;     &lt;num&gt;\n1:          Y           62                F             5    145.50\n2:          Y           62                F             6    133.28\n3:          Y           74                F             5    387.00\n4:          Y           67                G             7    687.75\n5:          Y           67                F             5  15453.36\n6:          Y           67                F             5  12552.92\n\n\n\nSpeaker notes"
  },
  {
    "objectID": "NBIHydrolocation.html#nbi-field-descriptions",
    "href": "NBIHydrolocation.html#nbi-field-descriptions",
    "title": "So long as you get the GIS&T of it",
    "section": "NBI field descriptions",
    "text": "NBI field descriptions\n\nSpeaker notes"
  },
  {
    "objectID": "NBIHydrolocation.html#refresher-hydrolocations",
    "href": "NBIHydrolocation.html#refresher-hydrolocations",
    "title": "So long as you get the GIS&T of it",
    "section": "Refresher: Hydrolocations",
    "text": "Refresher: Hydrolocations\nHydrolocations store the source, identifier, and index of a critical location Points of Interest (POIs) aggregate co-located hydrolocations to singular network locations\n\nSpeaker notes"
  },
  {
    "objectID": "NBIHydrolocation.html#conflation-method",
    "href": "NBIHydrolocation.html#conflation-method",
    "title": "So long as you get the GIS&T of it",
    "section": "Conflation method",
    "text": "Conflation method\n\n\n\n\n\n\nSpeaker notes"
  },
  {
    "objectID": "NBIHydrolocation.html#select-cases",
    "href": "NBIHydrolocation.html#select-cases",
    "title": "So long as you get the GIS&T of it",
    "section": "Select cases",
    "text": "Select cases\n\nSpeaker notes"
  },
  {
    "objectID": "NBIHydrolocation.html#reprocessing",
    "href": "NBIHydrolocation.html#reprocessing",
    "title": "So long as you get the GIS&T of it",
    "section": "Reprocessing",
    "text": "Reprocessing\n\nNoted misalignment/errors with previous process\n\nSome bridges appear misaligned with basemap ever so slightly.\nLarge areas missing geographic coverage.\nInconsistent processing methodology applied to scale.\n\nTraced to bridge line data\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes"
  },
  {
    "objectID": "NBIHydrolocation.html#concluding-reconmendations",
    "href": "NBIHydrolocation.html#concluding-reconmendations",
    "title": "So long as you get the GIS&T of it",
    "section": "Concluding reconmendations",
    "text": "Concluding reconmendations\nPre-clip is good! But, brittle tools and tool wiring are bad Suggestion: Input staging following an operational versioning schema Snapshot Versioning Utilitarian versioning - metadata changes Operational versioning - major version reconstruction Archival versioning - data overload!\n# Read data out of the OSM database\nbridges_data &lt;- osmextract::oe_read(\n        path_to_roadlines,\n        layer = \"lines\", \n        boundary = sf::st_bbox(vpus[vpus$huc2==vpu_unit, ]), \n        boundary_type = \"spat\",\n        extra_tags = c(\"bridge\", \"highway\"), \n        quiet = !is_verbose          \n    )\n\n# Pull out all lines tagged as \"Bridge\"\nbridges_sf &lt;- bridges_data[grepl(\"yes|viaduct|aqueduct|footbridge|suspension_bridge|movable_bridge|railway_bridge\", bridges_data$bridge, ignore.case = TRUE) & \n                           !is.na(bridges_data$bridge), ]\n    \n# Filter to valid bridge codes\nnbi_sf &lt;- nbi_sf[(nbi_sf$CULVERT_COND_062 == 'N' & \n                  nbi_sf$SERVICE_UND_042B %in% c('5','6', '7', '8', '9') & \n                  !(nbi_sf$SERVICE_ON_042A %in% c('2', '3', '9'))), ]\n\nSpeaker notes"
  },
  {
    "objectID": "NBIHydrolocation.html#whats-next",
    "href": "NBIHydrolocation.html#whats-next",
    "title": "So long as you get the GIS&T of it",
    "section": "What’s next?",
    "text": "What’s next?\n\n\n\nOutcomes and Takeaways:\n\nOrient ourselves to the NBI database\nDocument the conflation process\nProvide database: here\n\nNext Steps:\n\nEstablish targets workflow\nExplore better conflation methods.\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation"
  },
  {
    "objectID": "MO.html#how-to",
    "href": "MO.html#how-to",
    "title": "Mosaic",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Data Management",
      "Mosaic"
    ]
  },
  {
    "objectID": "MF.html#how-to",
    "href": "MF.html#how-to",
    "title": "Math Function",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "MA.html",
    "href": "MA.html",
    "title": "Map Algebra",
    "section": "",
    "text": "ll MF? ZS",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Math"
    ]
  },
  {
    "objectID": "MA.html#example",
    "href": "MA.html#example",
    "title": "Map Algebra",
    "section": "Example",
    "text": "Example",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Math"
    ]
  },
  {
    "objectID": "MA.html#how-to",
    "href": "MA.html#how-to",
    "title": "Map Algebra",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Math"
    ]
  },
  {
    "objectID": "LS.html#how-to",
    "href": "LS.html#how-to",
    "title": "Line of Sight Visibility",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Line of Sight Visibility"
    ]
  },
  {
    "objectID": "LL.html#how-to",
    "href": "LL.html#how-to",
    "title": "Local",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Math",
      "Local"
    ]
  },
  {
    "objectID": "KG.html#how-to",
    "href": "KG.html#how-to",
    "title": "Kreging",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Kreging"
    ]
  },
  {
    "objectID": "JT.html#how-to",
    "href": "JT.html#how-to",
    "title": "Join Table",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Join Table"
    ]
  },
  {
    "objectID": "IP.html#how-to",
    "href": "IP.html#how-to",
    "title": "Interpolation",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Interpolation"
    ]
  },
  {
    "objectID": "IC.html#how-to",
    "href": "IC.html#how-to",
    "title": "Image Classification",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing",
      "Image Classification"
    ]
  },
  {
    "objectID": "HEC.html#quick-links",
    "href": "HEC.html#quick-links",
    "title": "HEC family",
    "section": "Quick links",
    "text": "Quick links\nHEC-RAS User Manual",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "HEC family"
    ]
  },
  {
    "objectID": "HEC.html#hec-hms",
    "href": "HEC.html#hec-hms",
    "title": "HEC family",
    "section": "HEC-HMS",
    "text": "HEC-HMS\nThe HEC Hydrologic Modeling System (HEC-HMS) is a 1 dimensional rainfall-runoff modeling software used to estimate discharge based on rainfall at watershed scales. Beyond simplistic channel routing schema representation, this utility provides states for ground and surface water estimates, and with the decrease in dimensional and representational resolutions, can be run for 100’s of years at hourly timescales and wide geographies in a short time.\nRelevant utilities: A Lightweight HMS runner for linux",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "HEC family"
    ]
  },
  {
    "objectID": "HEC.html#hec-ras-1d",
    "href": "HEC.html#hec-ras-1d",
    "title": "HEC family",
    "section": "HEC-RAS 1D",
    "text": "HEC-RAS 1D\nThe Hydrologic Engineering Center River Analysis System, (HEC-RAS) is a 1 dimensional implicit finite difference solver using the full dynamic form of the 1D Saint venant equation. As a 1D model, cross sections are made to represent planes of perpetual flow and geographic representation is only loosely enforced. Given it’s extensive prominence and legacy deployments across hydraulic efforts, the form and standard data preparation procedures can vary wildly, although more recent standards provide some level of robustness as in maintains deep legacy backwards compatibility as it keeps pace with technical advancements.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "HEC family"
    ]
  },
  {
    "objectID": "HEC.html#hec-ras-2d",
    "href": "HEC.html#hec-ras-2d",
    "title": "HEC family",
    "section": "HEC-RAS 2D",
    "text": "HEC-RAS 2D\nAs the new defacto standard for modeling, HEC-RAS 2D solves the finite volume forms of the 2D Diffusion Wave equations; Shallow Water Equations (SWE-ELM) with a Eulerian-Lagrangian approach to solving for advection; or a Shallow Water Equation solver (SWE-EM) that uses an Eulerian approach for advection.\n…And just to round out the set…",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "HEC family"
    ]
  },
  {
    "objectID": "HEC.html#telemac-openfoam",
    "href": "HEC.html#telemac-openfoam",
    "title": "HEC family",
    "section": "Telemac & OpenFOAM",
    "text": "Telemac & OpenFOAM\nAdding a 3rd dimension to the solver adds not insignificant mathematical complexity to the problem but is as close to encapsulating our full understanding of the laws governing reality as we can achieve. This software, while not within the HEC umbrella, accomplishes that task; solving the 3D shallow water equations while providing the modeler with a host of controls for model interactions. Although the goal of modeling, particularly in this context, is to get as close a representation of the real world as we can, it’s worth noting that meeting that goal with these tools is as much an art as it is a science. Grid spacing, timing, boundary conditions, and model parametrization all factor in to getting a “good” result out of the models, and in fact it can possible to get the right answer for the wrong reason, a reasonable approximation for the wrong reason, or the desired answer for justifiable reasons.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "HEC family"
    ]
  },
  {
    "objectID": "HEC.html#tutorials",
    "href": "HEC.html#tutorials",
    "title": "HEC family",
    "section": "Tutorials",
    "text": "Tutorials",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "HEC family"
    ]
  },
  {
    "objectID": "HEC.html#explainers",
    "href": "HEC.html#explainers",
    "title": "HEC family",
    "section": "Explainers",
    "text": "Explainers\n\nThe HEC-RAS data format\n The HEC-RAS data format is a neat web of interconnected, dependent, temporary, and otherwise critical sidecar files which all contain the data needed by the HEC-RAS engine to run the model specified. Since it’s introduction in 1995, more modern formats of storing that data have given rise to additional sidecar files that typically represent transformations of the raw files into HDF equivalents and that conversion is handled by the HEC-RAS engine internally. In current releases of the software, if you open a model that does not have these files already located, the software creates them for you, and their naming schema is simply that base filename with a .hdf file extension added and so these are not diagrammed out in the figure above for simplicity. Each of these sidecar files has a particular objective in parameterizing the model and the following table describes what each of these represent:\n\n\n\n\n\n\n\nFile grep Pattern (# denotes single numeric wildcard)\nHEC-RAS Model Use\n\n\n\n\n.g##\nGeometry definitions\n\n\n.prj\nProjection (can be non-standard proj4 string defined file)\n\n\n.prj\nProject (same extension, defines how RAS models are wired)\n\n\n.p##\nPlan file, used to drive the model\n\n\n.f##\nSteady Flow file. Profile information, flow data and boundary conditions\n\n\n.h##\nHydraulic Design data file\n\n\n.v##\nVelocity file\n\n\n.o##\nOutput file\n\n\n.r##\nRun file for steady flow\n\n\n.u##\nunsteady Flow file. Profile information, flow data and boundary conditions\n\n\n.x##\nRun file for unsteady flow\n\n\n.dss\nData files\n\n\n.rasmap\nOutput plan\n\n\n\n\n\n\n\n\n\nFigure 1: A minimial, hydralic-orented, HEC-RAS model diagram alongside a more complex model demonstrating the potentially tortouously interconnected nature of the sidecar files HEC-RAS can orchastrate. Note that this was not just an arbitrary orientation of model files, but a reproduction of a formal diagram found in the latest version of the official documentation (“HEC-RAS Documentation” n.d.)\n\n\n\nWhile more simplistic model might look like the left side of Figure 1, model flow can quickly get complicated. It’s not uncommon, even in operations, to come across models that get incredibly creative with how they use this data format to represent the system. In addition to the general lag in software versions you’d normally find, it’s not unheard of to find models with multiple plans, plans that point to multiple geometry and flow files, as we see on the right side of Figure 1\n\nBase Level Engineering\nQuickstart resource: https://www.fema.gov/sites/default/files/documents/fema_state-quick-guide-base-level-engineering.pdf\nBase Level Engineering (BLE) is an attempt at setting a standard model format that one can expect. While that standardization is not always consistent, across time, across regions, or even within a particular vendor. However, these standards and commonalities make it easier to know what to expect and give you a rough idea of the sort of modeling effort that went into creating these data. Although BLE represents a subset of the available HEC-RAS data, it is by far one of the most centralized sources of HEC data.\nBase level engineering, as the name implies, is a baseline solution to understanding the risk of a location to flood recurrence intervals. To do this, the Federal Emergency Management Agency contracts out engineering firms to develop what were traditionally rather coarse one-dimensional HEC-RAS modeling efforts with a few simple water surface profiles which are intended to represent common high flow magnitudes. These modeling efforts have since evolved into more robust one and two dimensional HEC-RAS rain-on-grids with multiple water surface profiles. These resulting maps offer a solid credible engineering analysis and modeling foundation for communities to start planning and enforcing development and insurance codes through the establishment of a Base Flood Elevation (a 100 year water surface elevation) which forms the basis for Flood Insurance Rate Maps as provided by the National Flood Insurance Program. In short, Base Level Engineering provides the modeling surface off which the elevations are pulled to create the mapping surface off which insurance enforcement operates.\nWhile BLE represents a standard, there are variations on that standard that make some models more robust than others. The “baseline standard” typically involves a series of hydrologic and hydraulic analyses that follow/target the rough general highlights:\n\nThe input DEM data for the terrain must meet vertical accuracy standards (3DEP is appropriate)\n\nCell size no greater than 10 meters\nFull (at least 90%) spatial coverage of the domain of interest\n\nHydrology using NRCS rainfall-runoff methods to define upstream hydrograph\n\nCalibrated to stream gage where appropriate\nTypically identify 6 storm amounts (volumes) corresponding to…\n\n10%: (once every 10 years)\n4%: (once every 25 years)\n2%: (once every 50 years)\n1%: (once every 100 years)\n0.2%: (once every 500 years)\n\n\nHydraulics from one (and since 2020 two dimensional HEC-RAS) models\n\nRoughness from landcover reclassification1\nAutomated hydraulic feature placement followed by manual engineering judgment tweaks.\nCalibrated where feasible\n\nDeliverable and Mapped outputs\n\nHydraulic models\nMapped surfaces representing floodplain/flood depth grids\nAncillary analyses, relevant metadata, and reports as prescribed by the client(typically county, state, or FEMA)\n\n\nThere are early attempts at making the reading of some of these attributes programatically with the HEC-REAS model integrity check tool. It’s important to note that BLE is not a regulatory product (the FIRM is that product), but can be adopted as such. FEMA Region 6 has done a phenomenal job opening and publishing their BLE models, but others place them into FEMA’s closed database, called the Mapping Interface Platform (MIP). Learn more about these data in particular at their data source page, or at the RRASSLER utility developed to help make wressling the technical friction of those data.\n\n\n\nWhy RAS cross sections are not Hydrofabric transects\nA more technical stonewall: hydrofabric is a directed non-dendritic graph with contiguous spatial coverage and RAS models are singular, isolated entries in an overlapping mosaic of sparse coverage. Walking them back and forth requires some sort of informed crosswalk, the construction of which is 1) unique to the view of each dataset and 2) can be accomplished and justified in a number of different ways.\nHEC RAS cross sections are often conflated with hydrofabric transects, but are more appropriately thought of as a seed source for such for a few reasons.\nThe first and most obvious reason is that 1D HEC-RAS is not a spatial model, and the cross sections are not strictly a spatial object. Improvements in standards and the derivation of that core data have largely removed this concern, as the vast majority of “modern” 1D HEC-RAS models are represented such that the direct parameterization of the model data from traditional spatial data is 1:1.\nThe second reason that these models are not seamlessly cross-pollinateable is that the hydrofabric is a flowline –&gt; Catchment representation of the landscape, and the flowline is the “seed” for the geometrically derived line which defines the cross sections that constitute the hydrofabric cross section database. For RAS models, the modeled flow line is not encompassed by a single hydrofabric flow line, and the cross section of the RAS model is designed to match the model flowline. Therefore, if we’re going to tie the spatial representation of the ras cross section to the hydrofabric, not only does that modeled stream centerline need to be tied to the hydrofabric representation, but the relationship of the hydrofabric line needs to be explicitly tied to the geometry of your new network. This is certinly quite fesable, but is a relativly expensive transformation to fit into memory, and that scale grows proportianally with the size of the transects.\nOne of the hydrofabrics greatest strengths is that it can be manipulated and massaged at scale very nicely. To do that, some form of consistent parameterization is applied (e.g.: there are 10, evenly spaced transects per flowline). However, RAS cross sections are rarely evenly spaced and certainly not consistent (and therefore, predictable) from model to model. This adds additional geographic variation, which is not a function of the geometric representation of the flow network.\nFinally, spacing for traditional RAS model cross sections is designed with a stage-based prediction outcome in mind, whereas t-route is intended to accurately represent the timing and routing of water. Although in an ideal world those should match very closely, in practice that does not appear to be the case at all, and can be quite expensive to gather the same resolution of prediction\nThese databases can be made more compatible but in order to do so the RAS cross sections need to be matched with the appropriate hydrofabric flowline. Depending on the technical limitations of the model in which you are applying those cross sections too and how pedantic you feel like being, you may also need to “shift” the 0 of the RAS cross section to align with the intended river centerline, and many applications will desire a subset of the RAS cross sections which involves selecting the desired ones from the database. Finally, in order to conform to the described data model of the hydrofabric, the calculated attributes including the hydrofabric flowline ID, the order of the cross sections along that flow line, and the distance from the start of the flow line all need to be calculated and applied to the database in order to match the specification.\n\n\nManipulating HEC-RAS data\nLike most models and constraints associated with maintaining backwards compatibility with There are several utilities related or immediately adjacent to HEC-RAS mod\nI touch on ways we can create HEC-RAS data in “A future Open HEC” below:\n\nAcquiring\nAcquiring HEC-RAS models is a manual process but you can find some utilities that help facilitate this in RRASSLER including the scrape_ble_lib function.\n\n\nTransformations\n\nTo get a file-level appreciation for how complex and nuanced writing a HEC-RAS data reader can be, see FEMA_BLE by Ali Forghani: a pythonic implementation of a HEC-RAS model reader.\nTo make the HEC-RAS model data accountable (as a model) and spatial for further geospatial data accessibility and reuse cases see RRASSLER.\n\n\n\nCataloging\nKeeping track of the many disparate files is typically accomplished by the archivist, but the power user is one who needs to keep track of the different versions of the models they might need to deploy and facilitate access to the model in the same shape as geospatial data. While downloaded aArchives might be constructed by the model providers or by source venders themselves, inter-agency and inter-vendor HEC-RAS model sharing is far less standardized than other, more common datasets. Both RRASSLER and MCAT-RAS provide this service, and you can also construct a STAC resource if you can tolerate the unsorted nature of the system\n\n\n\nA future Open HEC\n With all of the tools at our disposal, it’s certainly not impossible to see a pathway towards a free and open source suite of HEC-RAS model generation utilities. This would drastically reduce the pain associated with parameterizing and instantiating HEC-RAS models, both 1 and 2D, but there’s a massive lack of hand work going into the creation of this tooling and not enough time in the day. If this is something you’d like to collaborate on, please reach out!",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "HEC family"
    ]
  },
  {
    "objectID": "HEC.html#footnotes",
    "href": "HEC.html#footnotes",
    "title": "HEC family",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n https://www.hec.usace.army.mil/confluence/rasdocs/r2dum/latest/developing-a-terrain-model-and-geospatial-layers/creating-land-cover-mannings-n-values-and-impervious-layers↩︎",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "HEC family"
    ]
  },
  {
    "objectID": "GV.html#how-to",
    "href": "GV.html#how-to",
    "title": "Generalize Vector",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "GS.html#how-to",
    "href": "GS.html#how-to",
    "title": "Geometry Shape",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "GEOINT_syl.html",
    "href": "GEOINT_syl.html",
    "title": "Syllabus and course policies",
    "section": "",
    "text": "Tip\n\n\n\nThis creation of this course was funded though a course creation scholarship through the University of Kansas Intelligence Community Centers for Academic Excellence",
    "crumbs": [
      "Introduction to GEOINT",
      "Syllabus and course policies"
    ]
  },
  {
    "objectID": "GEOINT_syl.html#course-description",
    "href": "GEOINT_syl.html#course-description",
    "title": "Syllabus and course policies",
    "section": "Course Description",
    "text": "Course Description\nThis course is an introduction to basic geospatial intelligence concepts and geospatial technologies. By blending broad brush conceptual lectures and hands-on experiences with mapping technologies, participants will learn how to identify, collect, and transform information about locations, people, objects, environments, events, and phenomena into digital representations of the world and generate end-products of geospatial analysis using modern (unclassified) tools. Overall, it contains four main parts: geospatial data and GPS, geographic information systems, remote sensing, and geospatial applications. Students will learn how to acquire and develop geospatial data as the sources for mapping, the skills of analyzing and interpreting spatial information, and how geovisualization can be used in addressing real-world problems in the Intelligence Community.\nSatisfies: Lab and Field Experiences (LFE), Natural Science (N), Part of the KU Core and the undergraduate certificate and minor in intelligence & national security studies",
    "crumbs": [
      "Introduction to GEOINT",
      "Syllabus and course policies"
    ]
  },
  {
    "objectID": "GEOINT_syl.html#student-responsibilities",
    "href": "GEOINT_syl.html#student-responsibilities",
    "title": "Syllabus and course policies",
    "section": "Student Responsibilities",
    "text": "Student Responsibilities\nIt is student’s responsibility to attend the lectures, do the readings, and finish lab assignments in a timely manner. All students should be prepared to participate in class discussion and answer questions when called upon. All students are expected to complete labs and exams on time. Labs are typically due at the beginning of the lab period one week after they are assigned, unless otherwise noted.",
    "crumbs": [
      "Introduction to GEOINT",
      "Syllabus and course policies"
    ]
  },
  {
    "objectID": "GEOINT_syl.html#expectations",
    "href": "GEOINT_syl.html#expectations",
    "title": "Syllabus and course policies",
    "section": "Expectations",
    "text": "Expectations\nAttend every Class/lab and ask questions! At times, the ArcGIS software can be very unforgiving, and GIS can seem like a foreign language. The last thing I want is for you to walk out of this class without understanding the material. I expect and hope that this will be a rewarding experience for everyone. We are available to answer questions and otherwise offer assistance during the office hours indicated above or by appointment. Additionally, if you have any questions, feel free to drop me an e-mail or use the slack channel (The link will be passed around on the signup sheet at the beginning of the year). I try to be diligent about responding to emails and by using slack you are likely to get a very rapid response.",
    "crumbs": [
      "Introduction to GEOINT",
      "Syllabus and course policies"
    ]
  },
  {
    "objectID": "GEOINT_syl.html#computers",
    "href": "GEOINT_syl.html#computers",
    "title": "Syllabus and course policies",
    "section": "Computers",
    "text": "Computers\nThe computers in the lab are not cheap to replace. Please do your part to lengthen their lives by not spilling coffee/soda/liquids of any kind on the computer or any of its parts. You do NOT need to go out and purchase a computer for this class, although I do recommend having a personal computer with ArcGIS installed.\n\nComputer recommendations\nArcGIS is a demanding program, and a lower grade computer is not recommended. If you are looking to purchase a computer, I offer these general guidelines. You should aim for a 64-bit computer (x64), and aside from that I like to prioritize CPU & RAM &gt; video & hard drive speed &gt; hard drive space (External hard drives are really cheap, and as long as the computer has a USB port you are good to go). For the last few years I’ve deferred to the pre-built pc choices on https://buildmypc.net/ or https://pcpartpicker.com/. In general, I tend to stay away from the “gaming” laptops. For those prices, you can build a much more powerful desktop and get a lot more mileage out of it, but that’s just me. If you want to go that route however virtually all will meet your school related needs. My “Ideal” build (This is excessive, visualization grade)",
    "crumbs": [
      "Introduction to GEOINT",
      "Syllabus and course policies"
    ]
  },
  {
    "objectID": "GEOINT_syl.html#resources",
    "href": "GEOINT_syl.html#resources",
    "title": "Syllabus and course policies",
    "section": "Resources",
    "text": "Resources\nThe lab is open for students whenever there is no scheduled class (see the calendar posted on the lab door. Additionally, ArcMap is available in the GIS lab on the second floor of Watson Library, and on the Architecture and Engineering computers if you have access to those. A signup sheet will also be passed around at the beginning of the semester for those who wish to install ArcGIS on your own personal computers.",
    "crumbs": [
      "Introduction to GEOINT",
      "Syllabus and course policies"
    ]
  },
  {
    "objectID": "GEOINT_syl.html#academic-integrity",
    "href": "GEOINT_syl.html#academic-integrity",
    "title": "Syllabus and course policies",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nCheating and/or plagiarism is not tolerated. Feel free to discuss the labs with your classmates, but all work you turn in should be your own, and it’s insert year here, how hard is it to give someone credit for their work? I use Zotero for my citation needs, as a KU student you have access to Endnote through Office 365, and in a pinch Cite This for Me will also push you across the finish line but you really should try to be more efficient with your time.",
    "crumbs": [
      "Introduction to GEOINT",
      "Syllabus and course policies"
    ]
  },
  {
    "objectID": "GEOINT_syl.html#disabilities",
    "href": "GEOINT_syl.html#disabilities",
    "title": "Syllabus and course policies",
    "section": "Disabilities",
    "text": "Disabilities\nAnyone who has a disability that may prevent full demonstration of academic ability should contact me as soon as possible to ensure accommodations can be made to allow for full educational benefit.",
    "crumbs": [
      "Introduction to GEOINT",
      "Syllabus and course policies"
    ]
  },
  {
    "objectID": "GEOINT_syl.html#course-evaluation",
    "href": "GEOINT_syl.html#course-evaluation",
    "title": "Syllabus and course policies",
    "section": "Course Evaluation",
    "text": "Course Evaluation\nTODO - breakdown\n\nLabs\nThere will be 14 computer labs which count for 50% of your final grade and each lab is equally weighted. Labs make up the bulk of your final grade because it is in the labs that you will gain the practical knowledge of how to use the geospatial technologies. Lab TA will help with your questions. Lab assignments are typically due in a week and should be submitted through Blackboard before the next lab meeting. Any assignment that is turned in after the due time is considered late. The penalty for a late assignment is based on the number of days late (including weekends) and will be penalized 10% per day.\n\n\nExams\nMid-term exam will cover the materials from the first half of the class, and the final exam will focus on the second half. The final exam is not comprehensive, but since we will be building on concepts throughout the semester, materials from the first half of the semester will inevitably be part of the final exam. Either exam may include a practical portion. This practical portion will consist of instructions to prepare some sort of short analysis or demonstration of practical knowledge related to geointellegence. These will be posted either a week before the exam and due on the date of the exam, or posted the day of the exam and due a week after.\n\n\nGrade determination\nKU uses a 10 point grade scale with +/- options (sans A+). As material is graded you may use the following grade sheet helper to determine what your grade in the course is. At the end of the term I export the blackboard gradebook and put it into this sheet to submit grades, so you can periodically check your grade this way should you have concerns. Letter grades are determined based on the following grading scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nA-\nB+\nB\nB-\nC+\nC\nC-\nD+\nD\nD-\nF\n\n\n\n\n100-93\n92-90\n89-87\n86-83\n82-80\n79-77\n76-73\n72-70\n69-67\n66-63\n62-60\n&lt;60",
    "crumbs": [
      "Introduction to GEOINT",
      "Syllabus and course policies"
    ]
  },
  {
    "objectID": "GEOINT_lab14.html",
    "href": "GEOINT_lab14.html",
    "title": "GEOINT Lab 14",
    "section": "",
    "text": "Lab 14 - Final Lab activity\nFuture content in progress",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 14"
    ]
  },
  {
    "objectID": "GEOINT_lab12.html",
    "href": "GEOINT_lab12.html",
    "title": "GEOINT Lab 12",
    "section": "",
    "text": "Lab 12 - Landsat 8 Imagery\n\nThis lab is a gratefully modified version of lab 11 from Bradley A. Shellito’s Introduction to Geospatial Technologies p814\n\nLearning Objective This chapter’s lab builds on the remote sensing basics of Chapter 10 and returns to using the MultiSpec program. In this exercise, you’ll be starting with a Landsat 8 scene and creating a subset of it with which to work. During the lab, you’ll examine the uses for several Landsat 8 band combinations in remote sensing analysis. The goals for you to take away from this exercise:\nFamiliarize yourself further and work with satellite imagery in MultiSpec Create a subset image of a Landsat 8 scene Examine different Landsat 8 bands in composites and compare the results Examine various landscape features in multiple Landsat 8 bands and compare them Apply visual image interpretation techniques to Landsat 8 imagery Outline:\nLearning Objective Submission requirements Open raster data Display in 5-4-3 Submission Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab2Questions.docx Handout to turn in You are answering the questions (laid out in the word doc above and also included in the tutorial below) as you work through the lab. Use full sentences as necessary to answer the prompts and submit it to blackboard. Copy the folder Chapter 11, which contains a Landsat 8 OLI/TIRS satellite image (called LandsatJuly) of northeastern Ohio from July 18, 2018, which was constructed from data supplied via EarthExplorer. The Landsat 8 image bands refer to the following portions of the electromagnetic (EM) spectrum in micrometers (µm): * Band 1: Coastal (0.43 to 0.45 µm) * Band 2: Blue (0.45 to 0.51 µm) * Band 3: Green (0.53 to 0.59 µm) * Band 4: Red (0.64 to 0.67 µm) * Band 5: Near infrared (0.85 to 0.88 µm) * Band 6: Shortwave infrared 1 (1.57 to 1.65 µm) * Band 7: Shortwave infrared 2 (2.11 to 2.29 µm) * Band 8: Panchromatic (0.50 to 0.68 µm) * Band 9: Cirrus (1.36 to 1.38 µm) * Band 10: Thermal infrared 1 (10.60 to 11.19 µm) * Band 11: Thermal infrared 2 (11.50 to 12.51 µm) Bands 1–9 are sensed by OLI, while bands 10 and 11 are sensed by TIRS. Keep in mind that Landsat 8 imagery has a 30-meter spatial resolution (except for the panchromatic band, which is 15 meters). Open raster data Display in 5-4-3 11.2 Using Landsat 8 Imagery Bands The Landsat OLI/TIRS image has several different bands, each with its own use. (See Table 11.1 for information on which band represents which wavelengths.) For instance, looking at the entire Landsat scene now (the 5-4-3) combination, you have a broad overview of a large slice of northern Ohio using the near-infrared, red, and green bands.\nVegetated areas (such as grass or trees) are reflecting a lot of near-infrared light in the red color gun, causing those areas to appear in shades of red. However, there are a lot of other things in the image as well. Examine the Landsat scene and zoom in on some of the cyan areas on the lakeshore of Lake Erie and then answer Question 11.1. Question 11.1 The features on the image in cyan are largely urbanized and developed areas. Why are they displayed in cyan on this image with the 5-4-3 band combination? Open the LandsatJuly image again—but this time use band 10 in the red color gun, band 10 again in the green color gun, and band 10 again in the blue color gun. These settings use band 10 in all three guns, so you see only this band in grayscale. This version of the LandsatJuly image loads in a separate window. Arrange the two windows (LandsatJuly in the 5-4-3 combination and LandsatJuly in the 10-10-10 combination) side by side so you can see both of them together. Keep in mind that band 10 in the Landsat 8 imagery is one of the thermal bands sensed by TIRS. Examine both of the Landsat scenes and answer Question 11.2. Question 11.2 What do the brighter places on the 10-10-10 image correspond with? Why do these places mostly appear brighter than their surroundings in the 10-10-10 image? Close the 10-10-10 version of LandsatJuly.5. Open the LandsatJuly image again, this time using a 9-9-9 combination (i.e., load the image with band 9 in the red gun, band 9 in the green gun, and band 9 in the blue gun). A new window opens with this image, which is band 9 in grayscale. Place this 9-9-9 image side by side with your original 5-4-3 image. Band 9 in the Landsat 8 imagery is designed for detecting cirrus clouds in imagery. Answer Question 11.3. Question 11.3 Where are the cirrus clouds in this section of northern Ohio in the image? Why are they so hard to see in the regular 5-4-3 image? Close the 9-9-9 image when you’re done so you’re only working with the 5-4-3 image. 11.3 Subsetting Images and Examining Landsat Imagery Right now, you’re working with an entire Landsat 8 scene, as shown on the next page, which is an area roughly 170 kilometers ×183 kilometers. For this lab, you will focus only on the area surrounding downtown Cleveland (i.e., path 18, row 31). You will have to create a subset; in essence, you will “clip” out the area that you’re interested in and create a new image from that imcenter\nZoom to the part of the LandsatJuly image that shows Cleveland (as in the following graphic): imcenter2. In the image, you should be able to see many features that make up downtown Cleveland —the waterfront area, a lot of urban development, major roads, and water features. 3. In order to create a new image that shows only Cleveland (a suggested region is shown in the graphic above), from the Processor pull-down menu, choose Reformat and then choose Change Image File Format. 4. You can draw a box around the area you want to subset by using the cursor, and the new image that’s created will have the boundaries of the box you’ve drawn on the screen. However, for the sake of consistency in this exercise, use the following values for Area to Reformat: a. Lines: ► Start 9015 ► End 10261 ► Interval 1 b. Columns: ► Start 847 ► End 2143 ► Interval 1 Leave the other defaults alone and click OK\nIn the Save As dialog box that appears, save this new image in the Chapter 11 folder with the name clevsub.img. Choose Multispectral for the Save as Type option (from the pull-down menu options next to Save as Type). When you’re ready, click Save.\nBack in MultiSpec, minimize the window containing the LandsatJuly image.\nOpen the clevsub image you just created in a new window. (In the Open dialog box, you may have to change the Files of Type that it’s asking about to All Files to be able to select the clevsub image option.)\nOpen the clevsub image with a 5-4-3 combination (band 5 in the red gun, band 4 in the green gun, and band 3 in the blue gun).9. Use the other defaults for the Enhancement options: stretch set to Linear and Min-max set to Clip 2% of tails.\nIn the Set Histogram Specifications dialog box that opens, select the Compute new histogram method and use the default Area to Histogram settings.\nClick OK when you’re done with the settings. The new subset image shows that the Cleveland area is ready to use.\nZoom in on the downtown Cleveland area, especially the areas along the waterfront. Also open Google Earth Pro and compare the Landsat image to its very crisp resolution imagery. Answer Questions 11.4 and 11.5. Question 11.4 What kinds of features on the Cleveland waterfront cannot be distinguished at the 30-meter resolution you’re examining in the Landsat image? Question 11.5 Conversely, what specific features on the Cleveland waterfront are apparent at the 30-meter resolution you’re examining in the Landsat image? 11.4 Examining Landsat Bands and Band Combinations\nZoom in on the Cleveland waterfront area in the clevsub image, so you can see FirstEnergy Stadium, home to the Cleveland Browns, and its immediate surrounding area.\nOpen another version of the clevsub image using the 4-3-2 combination.\nArrange the two windows on the screen so that you can examine them together, expanding and zooming in as needed to be able to view the stadium in both three-band combinations.Question 11.6 Which one of the two band combinations best brought the stadium and its field to prominence? Question 11.7 Why did this band combination best help in viewing the stadium and the field? (Hint: You may want to do some brief online research into the nature of the stadium and its field.)\nReturn to the view of Cleveland’s waterfront area and examine the water features (particularly Lake Erie and the river). Paying careful attention to the water features, open four display windows and then expand and arrange them side by side so you can look at differences between them. Create the following image composites, one for each of the four windows: a. 4-3-2 b. 7-6-5 c. 5-3-2 d. 2-2-2 Question 11.8 Which band combination(s) is best for letting you separate water bodies from land? Why?\nReturn to the view of Cleveland’s waterfront area. Focus on the urban features and vegetated features. (Zoom and pan where necessary to get a good look at urbanization.) Open new windows with the following new band combinations and note how things change with each combinations: a. 7-5-3 b. 2-3-4 c. 5-4-3\nQuestion 11.9 Which band combination(s) is(are) best for separating urban areas from other forms of land cover (i.e., vegetation, trees, etc.)? Why?\nSubmission All you have to turn into blackboard for this week is the final image you created above.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 12"
    ]
  },
  {
    "objectID": "GEOINT_lab10.html",
    "href": "GEOINT_lab10.html",
    "title": "GEOINT Lab 10",
    "section": "",
    "text": "This lab is a gratefully modified version of lab 9 from Bradley A. Shellito’s Introduction to Geospatial Technologies\n\n\n\nThis lab will help you start thinking about what objects look like from the sky rather than from the ground. You’ll be examining a series of images and applying the elements of visual image interpretation discussed in the chapter. As you do, you’ll need to think like a detective and search the image for clues to help figure out just what it is you’re looking at. You should be able to figure out exactly what you’re looking at or at least narrow down the possibilities to a short list before you turn to support data for help. Although the items you’ll be examining in this lab will all be large and prominent—buildings and other very visible features—the application of visual image interpretation elements for these simple (and hopefully fun) examples will help get you started looking at the world from above. The goals for you to take away from this exercise:\nThink of how objects look from an aerial perspective Apply the elements of visual image interpretation (as described in the chapter) to identify objects in the images Outline:\nLearning Objective Submission requirements Tutorial Viewing images Applying Elements of Visual Image Interpretation Visual Image Interpretation Wrapping up Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab2Questions.docx Handout to turn in You are answering the questions (laid out in the word doc above and also included in the tutorial below) as you work through the lab. Use full sentences as necessary to answer the prompts and submit it to blackboard. Tutorial Viewing images To examine a particular image with more control than your browser, you can download the zipped folder above, unzip it, and open an image using your photo viewer of choice. You can also open the image in either ArcGIS Pro or QGIS. These are .png images so they will load in and request to create pyrimids\nApplying Elements of Visual Image Interpretation Take a look at each of the images and try to determine what you’re looking at. There are questions related to each of them. Although everyone’s application of the elements may vary, there are some guidelines that you may find useful, as described in the following steps. Several images may contain multiple items, but all of them work together to help define one key solution. Let’s start with this sample image:\nimcenter\nFirst, look at the image as a whole for items to identify:\nThe central object is obviously some sort of structure—and it is large, judging from the shadow being cast. The relative size of the structure (compared with the size of several of the cars visible throughout the image) suggests that it’s probably a building and not a monument of some kind. It’s set on a body of water (the tone of the water is different from the tone of the nearby concrete or greenery). Thus, the site can be fixed. One of the most notable features is the large triangular shape of the main portion of the building; note too that its texture is very different from the rest of the building (the three white sections). The shape of the entrance is also very distinctive: It is large and round, with a couple of concentric circles. Based on your interpretation of the initial items, start looking for specifics and relationships between the items in the image:\nThe pattern of the concentric circles at the entrance plaza—two outer rings and a third inner ring with some sort of a design placed in it—is striking. Judging from the relative size of a person (pick out shadows near the plaza or on the walkway along the water), the plaza is fairly large. The pattern sort of resembles a giant vinyl record album—and the large curved feature that follows around the plaza on its right looks like the arm of an old record player. The texture of the triangular portions of the building suggests that they could be transparent, as if that whole portion of the structure were made of glass. There’s no parking lot associated with the building, so you’d have to park somewhere else and walk to this building. Further, there are no major roads nearby, again indicating that this is not some sort of destination that you just drive up to and park next to. The large body of water adjacent to large concrete walkways gives a clue that this building is in a large city. Put the clues together and come up with an idea of what the object or scene could be:\nThe building is distinctive enough to be a monument, but it is probably too large to be one. It could be some sort of museum, casino, shopping center, or other attraction. However, the building is too small to be a casino or resort center, and it lacks the necessary parking to be a casino, an office building, or a shopping center. The very distinct “record” motif of the front plaza seems to indicate that music (and, more specifically, older music, given the “vinyl record” styling) plays a large part in whatever the building is. From the clues in the image, this could be some sort of music museum, like the Rock & Roll Hall of Fame, which is located in Cleveland, Ohio, right on the shore of Lake Erie. Finally, use auxiliary sources to verify your deduction or to eliminate some potential choices. By using Google Earth to search for the Rock & Roll Hall of Fame, you can confirm the identification. An image search on Google will also turn up some non-aerial pictures of the site to verify your conclusion. This lab does not provide such extensive details on all the images, and you will not necessarily use all the elements to examine every image. For instance, there may be one or two items in an image that will help you make a quick identification. However, there are enough clues in each image to figure out what you’re looking at—or at least narrow down the choices far enough that some research using other sources may be able to lead you to a positive identification. (For instance, by putting together all the clues from the sample image, you could start searching for large museums dedicated to music. Add the triangular glass structures as part of your search, and the Rock & Roll Hall of Fame will certainly come up.)\nVisual Image Interpretation Important note: All images used in this exercise are from areas within the United States.\nFor each image, answer the questions presented below and then explain which of the elements of visual image interpretation led you to this conclusion and what they helped you decide. Writing “shadow and shape helped identify traits of the building” would be unacceptable. It would be much better to write something that answers the question “What was so special about the shadows in the scene or the shape of items that helped?” When you identify the items, be very specific (for instance, not just “a baseball stadium” but “Progressive Field” in Cleveland, Ohio). Use external sources for extra information, such as Websites, search engines, books, or maps. When you think you finally have an image properly identified, you may want to use something like Google Earth or Bing Maps to obtain a view of the object you think you’re looking at in the image to help verify your answer.\nimcenter Question 1 Examine image 1. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 2 Examine image 2. There are many similar objects in this image, but you’ll notice some differences too. What (specifically) is being displayed in this image, and what is its location? What elements of visual image interpretation lead you to draw this conclusion? Question 3 imcenter Examine image 3. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 4 Examine image 4. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 5 Examine image 5. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 6 Examine image 6. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 7 Examine image 7. There are several items in this image but they all add up to one specific thing. What, specifically, is this image showing, and what is its location? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 8 Examine image 8. There are several items visible in the image, but there’s one that’s more prominent than the others. What, specifically, is this item? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 9 Examine image 9. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? For instance, it’s obviously an island, but what type of features does the island have (or not have) that may aid in identifying what is being shown here? imcenter Question 10 Examine image 10. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? Wrapping up There is no need to save anything from this lab, so when done you can simply close without saving. Submit your answers to the questions on blackboard.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 10"
    ]
  },
  {
    "objectID": "GEOINT_lab10.html#learning-objective",
    "href": "GEOINT_lab10.html#learning-objective",
    "title": "GEOINT Lab 10",
    "section": "",
    "text": "This lab will help you start thinking about what objects look like from the sky rather than from the ground. You’ll be examining a series of images and applying the elements of visual image interpretation discussed in the chapter. As you do, you’ll need to think like a detective and search the image for clues to help figure out just what it is you’re looking at. You should be able to figure out exactly what you’re looking at or at least narrow down the possibilities to a short list before you turn to support data for help. Although the items you’ll be examining in this lab will all be large and prominent—buildings and other very visible features—the application of visual image interpretation elements for these simple (and hopefully fun) examples will help get you started looking at the world from above. The goals for you to take away from this exercise:\nThink of how objects look from an aerial perspective Apply the elements of visual image interpretation (as described in the chapter) to identify objects in the images Outline:\nLearning Objective Submission requirements Tutorial Viewing images Applying Elements of Visual Image Interpretation Visual Image Interpretation Wrapping up Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab2Questions.docx Handout to turn in You are answering the questions (laid out in the word doc above and also included in the tutorial below) as you work through the lab. Use full sentences as necessary to answer the prompts and submit it to blackboard. Tutorial Viewing images To examine a particular image with more control than your browser, you can download the zipped folder above, unzip it, and open an image using your photo viewer of choice. You can also open the image in either ArcGIS Pro or QGIS. These are .png images so they will load in and request to create pyrimids\nApplying Elements of Visual Image Interpretation Take a look at each of the images and try to determine what you’re looking at. There are questions related to each of them. Although everyone’s application of the elements may vary, there are some guidelines that you may find useful, as described in the following steps. Several images may contain multiple items, but all of them work together to help define one key solution. Let’s start with this sample image:\nimcenter\nFirst, look at the image as a whole for items to identify:\nThe central object is obviously some sort of structure—and it is large, judging from the shadow being cast. The relative size of the structure (compared with the size of several of the cars visible throughout the image) suggests that it’s probably a building and not a monument of some kind. It’s set on a body of water (the tone of the water is different from the tone of the nearby concrete or greenery). Thus, the site can be fixed. One of the most notable features is the large triangular shape of the main portion of the building; note too that its texture is very different from the rest of the building (the three white sections). The shape of the entrance is also very distinctive: It is large and round, with a couple of concentric circles. Based on your interpretation of the initial items, start looking for specifics and relationships between the items in the image:\nThe pattern of the concentric circles at the entrance plaza—two outer rings and a third inner ring with some sort of a design placed in it—is striking. Judging from the relative size of a person (pick out shadows near the plaza or on the walkway along the water), the plaza is fairly large. The pattern sort of resembles a giant vinyl record album—and the large curved feature that follows around the plaza on its right looks like the arm of an old record player. The texture of the triangular portions of the building suggests that they could be transparent, as if that whole portion of the structure were made of glass. There’s no parking lot associated with the building, so you’d have to park somewhere else and walk to this building. Further, there are no major roads nearby, again indicating that this is not some sort of destination that you just drive up to and park next to. The large body of water adjacent to large concrete walkways gives a clue that this building is in a large city. Put the clues together and come up with an idea of what the object or scene could be:\nThe building is distinctive enough to be a monument, but it is probably too large to be one. It could be some sort of museum, casino, shopping center, or other attraction. However, the building is too small to be a casino or resort center, and it lacks the necessary parking to be a casino, an office building, or a shopping center. The very distinct “record” motif of the front plaza seems to indicate that music (and, more specifically, older music, given the “vinyl record” styling) plays a large part in whatever the building is. From the clues in the image, this could be some sort of music museum, like the Rock & Roll Hall of Fame, which is located in Cleveland, Ohio, right on the shore of Lake Erie. Finally, use auxiliary sources to verify your deduction or to eliminate some potential choices. By using Google Earth to search for the Rock & Roll Hall of Fame, you can confirm the identification. An image search on Google will also turn up some non-aerial pictures of the site to verify your conclusion. This lab does not provide such extensive details on all the images, and you will not necessarily use all the elements to examine every image. For instance, there may be one or two items in an image that will help you make a quick identification. However, there are enough clues in each image to figure out what you’re looking at—or at least narrow down the choices far enough that some research using other sources may be able to lead you to a positive identification. (For instance, by putting together all the clues from the sample image, you could start searching for large museums dedicated to music. Add the triangular glass structures as part of your search, and the Rock & Roll Hall of Fame will certainly come up.)\nVisual Image Interpretation Important note: All images used in this exercise are from areas within the United States.\nFor each image, answer the questions presented below and then explain which of the elements of visual image interpretation led you to this conclusion and what they helped you decide. Writing “shadow and shape helped identify traits of the building” would be unacceptable. It would be much better to write something that answers the question “What was so special about the shadows in the scene or the shape of items that helped?” When you identify the items, be very specific (for instance, not just “a baseball stadium” but “Progressive Field” in Cleveland, Ohio). Use external sources for extra information, such as Websites, search engines, books, or maps. When you think you finally have an image properly identified, you may want to use something like Google Earth or Bing Maps to obtain a view of the object you think you’re looking at in the image to help verify your answer.\nimcenter Question 1 Examine image 1. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 2 Examine image 2. There are many similar objects in this image, but you’ll notice some differences too. What (specifically) is being displayed in this image, and what is its location? What elements of visual image interpretation lead you to draw this conclusion? Question 3 imcenter Examine image 3. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 4 Examine image 4. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 5 Examine image 5. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 6 Examine image 6. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 7 Examine image 7. There are several items in this image but they all add up to one specific thing. What, specifically, is this image showing, and what is its location? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 8 Examine image 8. There are several items visible in the image, but there’s one that’s more prominent than the others. What, specifically, is this item? What elements of visual image interpretation lead you to draw this conclusion? imcenter Question 9 Examine image 9. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? For instance, it’s obviously an island, but what type of features does the island have (or not have) that may aid in identifying what is being shown here? imcenter Question 10 Examine image 10. What (specifically) is being displayed in this image? What elements of visual image interpretation lead you to draw this conclusion? Wrapping up There is no need to save anything from this lab, so when done you can simply close without saving. Submit your answers to the questions on blackboard.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 10"
    ]
  },
  {
    "objectID": "GEOINT_lab09A.html",
    "href": "GEOINT_lab09A.html",
    "title": "GEOINT Lab 09 A",
    "section": "",
    "text": "Lab 09A - 3D Modeling and Visualization\nFuture content here",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 09 A"
    ]
  },
  {
    "objectID": "GEOINT_lab07.html",
    "href": "GEOINT_lab07.html",
    "title": "GEOINT Lab 07",
    "section": "",
    "text": "This lab is a gratefully modified version of lab 13 from Bradley A. Shellito’s Introduction to Geospatial Technologies 934\n\n\n\nThis chapter’s lab introduces some of the basics of digital terrain modeling: working with DTMs, slope, viewsheds, and imagery draped over the terrain model. You’ll be using the free Google Earth Pro for this lab. The goals for you to take away from this lab:\nExamine pseudo-3D terrain and navigate across it in Google Earth Pro Examine the effects of different levels of vertical exaggeration on the terrain Create a viewshed and analyze the result Create an animation that simulates flight over 3D terrain in Google Earth Pro Create an elevation profile for use in examining a DTM and slope information Outline:\nLearning Objective Submission requirements Tutorial Examining Landscapes and Terrain with Google Earth Pro Vertical Exaggeration and Measuring Elevation Height Values Working with Viewsheds in Google Earth Pro Flying and Recording Animations in Google Earth Measuring Profiles and Slopes in Google Earth Pro Wrapping up Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab08Questions.docx Handout to turn in Tutorial Examining Landscapes and Terrain with Google Earth Pro Start Google Earth Pro (GEP). By default, GEP’s Terrain option is turned on for you. Look in the Layers box to ensure that there is a checkmark next to Terrain. This option drapes the imagery over a DTM (Digital Terrain Model). There are several sources for this terrain, so let’s make sure we are using the highest quality one. Because this lab is focused on digital terrain modeling, you should use the most detailed DTM possible, so from the Tools pull-down menu, select Options. In the Google Earth Options dialog box that appears, make sure the checkmark Use high quality terrain (disable for quicker resolution and faster rendering) is on. While in the options, click on the Navigation tab and set the Automatically tilt when zooming radio button. Click Apply and then OK. In the Search box, type “Zion National Park, UT” and GEP zooms down to the area of Zion National Park. In the Layers box, expand the More option and place a checkmark next to Parks/Recreation Areas. You should then see Zion outlined in green. Zoom out so that you can see the entire layout of Zion in the view. imcenter\nYou see a question mark symbol labeled Visitor Center next to a label Park Headquarters. Center the view on this area of Zion and scroll in so your view tilts into a perspective from which you can see the sides of the mountains and canyons in Zion in pseudo-3D. imcenter\nThere are two references to heights or elevation values in the bottom portion of the view:\nThe heading marked “elev” shows the height of the terrain model (that is, the height above the vertical datum) where the cursor is placed. The heading marked “Eye alt” shows GEP’s measurement for how high above the terrain (or the vertical datum) your vantage point is. Use a combination of your mouse wheel and the Ctrl key to maneuver yourself into a southward facing direction at ~4200 ft, positioned just above the visitor center as shown below. Use this view to answer question 1. imcenter\nQuestion 1 How does the pseudo-3D view from this position and altitude aid in bringing out the terrain features of Zion (compared to what you originally saw in the overhead view)? Vertical Exaggeration and Measuring Elevation Height Values GEP allows you to alter the vertical exaggeration of the terrain layer. Vertical exaggeration changes the vertical scale but keeps the horizontal scale the same. This effect makes it easier to identify topographic variations, but distorts reality (more so than is already distorted through the computer screen) Therefore, it should be used for visualization purposes only. In GEP the settings to change this are located in the To look at different levels of vertical exaggeration by clicking Tools &gt; Options.\nIn the box marked Elevation Exaggeration, you can type a value (between 0.01 and 3) to vertically exaggerate GEP’s terrain. Play around, and then type a value of “2”, click Apply and OK, and then reexamine the area surrounding the park headquarters in Zion. Use this process to answer questions 2 and 3. imcenter\nQuestion 2 How did the vertical exaggeration value of 2 affect the view of Zion? Question 3 Try the following values for vertical exaggeration: 0.5, 1, and 3. How did each value affect the visualization of the terrain? In addition to the value of 2 you examined in Question 13.2, which value of vertical exaggeration was the most useful for a visual representation of Zion and why? Reset the Elevation Exaggeration to a value of “1” when you’re done. From here, you’ll examine the elevation values of the terrain surface. Wherever you move the cursor on the screen, a new value for elevation is shown in the elevation at the bottom of the view. By zooming and placing the cursor on its symbol on a spot on the screen, you can determine the elevation value for that location. Question 4 At what elevation is the height of the park headquarters/visitor center? Working with Viewsheds in Google Earth Pro Now that you have a pretty good idea of how the landscape of Zion looks in the areas near the park headquarters, you can create a viewshed that will allow you to see what is visible and what is blocked from sight at a certain location. To begin, you’ll see what areas of Zion can be seen and cannot be seen from the park headquarters. Zoom in closely on the question mark symbol that GEP uses to mark the park headquarters. This symbol is what you’ll use as the location for creating a viewshed.\nClick the Add Placemark icon on the toolbar and put the placemark right on the park headquarters question mark symbol (Review Lab 1 if you need a quick refresher). Name this new placemark “Park HQ”. In the Places box, right-click the new Park HQ placemark and select Show Viewshed. If prompted about the placemark being too low, click on Adjust automatically. GEP will compute the viewshed. Zoom out a bit until you can see the extent of the viewshed; all of the areas covered in green are the places that can be seen from the location of the Park HQ placemark and the areas not in green cannot be seen from there. After zooming out, look at the areas immediately south and southeast of the Park HQ placemark to help you answer question 5. Question 5 Can the ranger station, the campground, or the two picnic areas just south and southeast of the park headquarters be seen from the Park HQ vantage point? Click on Exit viewshed (in the upper left corner of the map) to remove the viewshed layer. About 2 miles to the northeast of the park headquarters is a scenic overlook. Its symbol in GEP is a small green arrow pointing to a green star. Move over to that location so that you can see it in the view and zoom in closely on the overlook symbol. Put a new placemark directly on the green star and name this placemark “Zion Overlook”. Create a viewshed for the Zion Overlook point and answer Questions 6 and 7. Exit the viewshed when you’re done. Question 6 Can the area labeled as parking to the immediate north of your Zion Overlook point be seen from the overlook? Question 7 What is blocking your view of the Sand Beach trail from the overlook? Be specific in your answer. Hint: You may want to zoom in closely to your Zion Overlook placemark and position the view as if you were standing at that spot and looking in the direction of the Sand Beach trail. Flying and Recording Animations in Google Earth GEP allows you to record high-definition videos of the areas you view. In this section, you’ll be capturing a video of the high-resolution imagery draped over the terrain to record a virtual tour of a section of Zion. Before recording, use the mouse and the Move tools to practice flying around the Zion area. You can fly over the terrain, dip in and out of valleys, and skim over the mountaintops. Don’t forget that you can also hold down the Ctrl key and move the mouse to tilt your view. It’s important to have a good feel of the controls, as any movements within the view will be recorded to the video. Therefore, the first few steps take you through a dry run. When you feel confident in your ability to fly over 3D terrain in Google Earth Pro, move on to the next step.\nThe tour you will be recording will start at the park headquarters (your Park HQ placemark), move to the scenic overlook (your Zion Overlook placemark), and then finish at the lodging/parking area about a mile to the north of the overlook. Do a short dry run of this before you record. To begin, double-click on the Park HQ placemark in the Places box, and the view shifts there. Double-click on the Zion Overlook placemark in the Places box, and the view jumps to there. Use the mouse and Move tools to fly manually over the terrain a mile north of the overlook to the parking and lodging area and end the tour there. If you needed, prior to recording repeat this dry run maneuvering among the three points until you feel comfortable. When you’re ready to make your tour video, double-click on the Park HQ placemark in the Places box to return to the starting point of the tour. Also, take away the checkmarks next to the Park HQ and Zion Overlook placemarks in the Places box so that the two placemarks will not appear in the view (and thus not appear in the video) and so all you will see is the GEP imagery and terrain. On Google Earth’s toolbar, select the Record a Tour button. A new set of controls appears at the bottom of the view: imcenter\nTo start recording the video, click the circular red record button. If you have a microphone hooked up to your computer, you can get really creative and narrate your tour; your narration or sounds will be saved along with your video. After showing the park headquarters in the view for a couple seconds, double-click on the Zion Overlook placemark in the Places box to jump to there. Show the overlook for a few seconds and then use the mouse and move commands to fly to the lodging/parking area to the north. When you’re done, click the circular red record button again to stop recording. A new set of controls appears at the bottom of the screen, and Google Earth Pro begins to play your video. Use the rewind and fast-forward buttons to skip around in the video, and also use the play/pause button to start or stop. You can click the button with the two arrows to repeat the tour or put it on a loop to keep playing. Once you’ve created a tour you are happy with, save it by clicking the Save Tour button. Call it “Ziontour” in the dialog box that opens. The saved tour is then added to your Places box (just like all the other GEP layers). imcenter\nRight now, the tour can only be played in GEP. You’ll want to export your tour to a high-definition video that can be watched by others or shared on the Web. To start this process, first close the DVR control box in the view by clicking on the x in the upper-right corner of the controls. Next, from the Tools pull-down menu, select Movie Maker. In the Movie Maker dialog box, choose 1280 × 720 (HD) for Video parameters to create a high-definition (HD) video of your tour. Next, under Record from choose the radio button for A saved tour and choose Ziontour (My Places). This will be the source of your video. From the pull-down menu next to File type, choose MJPEG (.mp4). Under Save to, use the browse button to navigate to the drive to which you want to save your video and call it “Ziontourmovie”. Leave the other defaults alone and click Create Movie. A dialog box appears, showing the conversion process involved in creating your video. imcenter\nReturn to the folder where you saved your movie and view it by using your computer’s media player.\nQuestion 8 Submit the final video file of your video tour of Zion to your instructor, who will check it over for its quality and completeness for you to get credit for this question. Measuring Profiles and Slopes in Google Earth Pro Now let’s examine some profiles of the digital terrain model and find the slope information. To begin, return the view to the Park HQ placemark and zoom out so that you can see both the Park HQ and the Zion Overlook placemarks clearly in the view. To examine the terrain profile, you must first draw a path between the two points. Click the ruler tool on the GEP toolbar, and in the dialog box that opens, select the Path tab. In the view, you’ll see that your cursor has changed to a crosshairs symbol. click once on the Park HQ placemark location and then click once on the Zion Overlook placemark position. You should see a yellow line drawn between them, and the length of this line is shown in the Ruler dialog box. In the Ruler dialog box, click Save. A new dialog box opens, allowing you to name this path that you’ve drawn. Call it “ParkHQOverlookPath” and click OK in the naming dialog box. You see a new item called ParkHQOverlookPath added to the Places box. Right-click on it and select Show Elevation Profile. A new window opens below the view, showing the elevation profile between the two points. This profile shows you, in two dimensions, the outline and elevations of the terrain in between the two points. imcenter\nAs you move your cursor along the outline of the terrain in the profile, information appears about the elevation and the slope at the location of your cursor. You also see the corresponding location highlighted in the view. The slope information is a positive percentage as you move uphill from the park headquarters to the overlook and a negative percentage as you move downhill. Carefully examine the profile (and the digital terrain) and answer Questions 9 - 11. Question 9 What is the average slope for the 2.2-mile path between the park headquarters and the overlook (both uphill and downhill)? Question 10 What is the steepest uphill slope between the two points, and where is it located? Question 11 What is the steepest downhill slope between the two points, and where is this located? Select a small section of the path and examine only the profile of that small subset. To do so, click the mouse at a place in the profile where you want to begin. Hold down the left mouse button and move the mouse to the place in the profile where you want to end. Release the mouse button, and you see that small section of the profile you chose in a darker red. Statistics for the elevation and slope of that subset are shown in a red box at the top of the profile. To examine one small part of the profile, on the far right side of the profile (the terrain nearest the overlook), you can see that a section of the landscape drops off sharply. (This should be around the 1.6-mile mark between the Park HQ and the overlook.) Using the method described above, highlight the subset of the profile from the location at the bottom of the drop-off to the overlook and then answer Question 12 For this section of the terrain, what is the maximum uphill and downhill slope? What is the average uphill and downhill slope? Wrapping up There is no need to save anything from this lab, so when done you can simply close without saving. Submit your answers to the questions on blackboard.\nIn lab 9 (~two weeks from now) we will use Google Earth Engine. This platform is still a beta product, and you will need to request access through your Google account here. These are still approved by hand and may take a few days to get. Once approved, you will need to follow the instructions in the email that is sent before you are able to access the platform. Therefore, be sure to do this step before you attempt to work though lab 9.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 07"
    ]
  },
  {
    "objectID": "GEOINT_lab07.html#learning-objective",
    "href": "GEOINT_lab07.html#learning-objective",
    "title": "GEOINT Lab 07",
    "section": "",
    "text": "This chapter’s lab introduces some of the basics of digital terrain modeling: working with DTMs, slope, viewsheds, and imagery draped over the terrain model. You’ll be using the free Google Earth Pro for this lab. The goals for you to take away from this lab:\nExamine pseudo-3D terrain and navigate across it in Google Earth Pro Examine the effects of different levels of vertical exaggeration on the terrain Create a viewshed and analyze the result Create an animation that simulates flight over 3D terrain in Google Earth Pro Create an elevation profile for use in examining a DTM and slope information Outline:\nLearning Objective Submission requirements Tutorial Examining Landscapes and Terrain with Google Earth Pro Vertical Exaggeration and Measuring Elevation Height Values Working with Viewsheds in Google Earth Pro Flying and Recording Animations in Google Earth Measuring Profiles and Slopes in Google Earth Pro Wrapping up Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab08Questions.docx Handout to turn in Tutorial Examining Landscapes and Terrain with Google Earth Pro Start Google Earth Pro (GEP). By default, GEP’s Terrain option is turned on for you. Look in the Layers box to ensure that there is a checkmark next to Terrain. This option drapes the imagery over a DTM (Digital Terrain Model). There are several sources for this terrain, so let’s make sure we are using the highest quality one. Because this lab is focused on digital terrain modeling, you should use the most detailed DTM possible, so from the Tools pull-down menu, select Options. In the Google Earth Options dialog box that appears, make sure the checkmark Use high quality terrain (disable for quicker resolution and faster rendering) is on. While in the options, click on the Navigation tab and set the Automatically tilt when zooming radio button. Click Apply and then OK. In the Search box, type “Zion National Park, UT” and GEP zooms down to the area of Zion National Park. In the Layers box, expand the More option and place a checkmark next to Parks/Recreation Areas. You should then see Zion outlined in green. Zoom out so that you can see the entire layout of Zion in the view. imcenter\nYou see a question mark symbol labeled Visitor Center next to a label Park Headquarters. Center the view on this area of Zion and scroll in so your view tilts into a perspective from which you can see the sides of the mountains and canyons in Zion in pseudo-3D. imcenter\nThere are two references to heights or elevation values in the bottom portion of the view:\nThe heading marked “elev” shows the height of the terrain model (that is, the height above the vertical datum) where the cursor is placed. The heading marked “Eye alt” shows GEP’s measurement for how high above the terrain (or the vertical datum) your vantage point is. Use a combination of your mouse wheel and the Ctrl key to maneuver yourself into a southward facing direction at ~4200 ft, positioned just above the visitor center as shown below. Use this view to answer question 1. imcenter\nQuestion 1 How does the pseudo-3D view from this position and altitude aid in bringing out the terrain features of Zion (compared to what you originally saw in the overhead view)? Vertical Exaggeration and Measuring Elevation Height Values GEP allows you to alter the vertical exaggeration of the terrain layer. Vertical exaggeration changes the vertical scale but keeps the horizontal scale the same. This effect makes it easier to identify topographic variations, but distorts reality (more so than is already distorted through the computer screen) Therefore, it should be used for visualization purposes only. In GEP the settings to change this are located in the To look at different levels of vertical exaggeration by clicking Tools &gt; Options.\nIn the box marked Elevation Exaggeration, you can type a value (between 0.01 and 3) to vertically exaggerate GEP’s terrain. Play around, and then type a value of “2”, click Apply and OK, and then reexamine the area surrounding the park headquarters in Zion. Use this process to answer questions 2 and 3. imcenter\nQuestion 2 How did the vertical exaggeration value of 2 affect the view of Zion? Question 3 Try the following values for vertical exaggeration: 0.5, 1, and 3. How did each value affect the visualization of the terrain? In addition to the value of 2 you examined in Question 13.2, which value of vertical exaggeration was the most useful for a visual representation of Zion and why? Reset the Elevation Exaggeration to a value of “1” when you’re done. From here, you’ll examine the elevation values of the terrain surface. Wherever you move the cursor on the screen, a new value for elevation is shown in the elevation at the bottom of the view. By zooming and placing the cursor on its symbol on a spot on the screen, you can determine the elevation value for that location. Question 4 At what elevation is the height of the park headquarters/visitor center? Working with Viewsheds in Google Earth Pro Now that you have a pretty good idea of how the landscape of Zion looks in the areas near the park headquarters, you can create a viewshed that will allow you to see what is visible and what is blocked from sight at a certain location. To begin, you’ll see what areas of Zion can be seen and cannot be seen from the park headquarters. Zoom in closely on the question mark symbol that GEP uses to mark the park headquarters. This symbol is what you’ll use as the location for creating a viewshed.\nClick the Add Placemark icon on the toolbar and put the placemark right on the park headquarters question mark symbol (Review Lab 1 if you need a quick refresher). Name this new placemark “Park HQ”. In the Places box, right-click the new Park HQ placemark and select Show Viewshed. If prompted about the placemark being too low, click on Adjust automatically. GEP will compute the viewshed. Zoom out a bit until you can see the extent of the viewshed; all of the areas covered in green are the places that can be seen from the location of the Park HQ placemark and the areas not in green cannot be seen from there. After zooming out, look at the areas immediately south and southeast of the Park HQ placemark to help you answer question 5. Question 5 Can the ranger station, the campground, or the two picnic areas just south and southeast of the park headquarters be seen from the Park HQ vantage point? Click on Exit viewshed (in the upper left corner of the map) to remove the viewshed layer. About 2 miles to the northeast of the park headquarters is a scenic overlook. Its symbol in GEP is a small green arrow pointing to a green star. Move over to that location so that you can see it in the view and zoom in closely on the overlook symbol. Put a new placemark directly on the green star and name this placemark “Zion Overlook”. Create a viewshed for the Zion Overlook point and answer Questions 6 and 7. Exit the viewshed when you’re done. Question 6 Can the area labeled as parking to the immediate north of your Zion Overlook point be seen from the overlook? Question 7 What is blocking your view of the Sand Beach trail from the overlook? Be specific in your answer. Hint: You may want to zoom in closely to your Zion Overlook placemark and position the view as if you were standing at that spot and looking in the direction of the Sand Beach trail. Flying and Recording Animations in Google Earth GEP allows you to record high-definition videos of the areas you view. In this section, you’ll be capturing a video of the high-resolution imagery draped over the terrain to record a virtual tour of a section of Zion. Before recording, use the mouse and the Move tools to practice flying around the Zion area. You can fly over the terrain, dip in and out of valleys, and skim over the mountaintops. Don’t forget that you can also hold down the Ctrl key and move the mouse to tilt your view. It’s important to have a good feel of the controls, as any movements within the view will be recorded to the video. Therefore, the first few steps take you through a dry run. When you feel confident in your ability to fly over 3D terrain in Google Earth Pro, move on to the next step.\nThe tour you will be recording will start at the park headquarters (your Park HQ placemark), move to the scenic overlook (your Zion Overlook placemark), and then finish at the lodging/parking area about a mile to the north of the overlook. Do a short dry run of this before you record. To begin, double-click on the Park HQ placemark in the Places box, and the view shifts there. Double-click on the Zion Overlook placemark in the Places box, and the view jumps to there. Use the mouse and Move tools to fly manually over the terrain a mile north of the overlook to the parking and lodging area and end the tour there. If you needed, prior to recording repeat this dry run maneuvering among the three points until you feel comfortable. When you’re ready to make your tour video, double-click on the Park HQ placemark in the Places box to return to the starting point of the tour. Also, take away the checkmarks next to the Park HQ and Zion Overlook placemarks in the Places box so that the two placemarks will not appear in the view (and thus not appear in the video) and so all you will see is the GEP imagery and terrain. On Google Earth’s toolbar, select the Record a Tour button. A new set of controls appears at the bottom of the view: imcenter\nTo start recording the video, click the circular red record button. If you have a microphone hooked up to your computer, you can get really creative and narrate your tour; your narration or sounds will be saved along with your video. After showing the park headquarters in the view for a couple seconds, double-click on the Zion Overlook placemark in the Places box to jump to there. Show the overlook for a few seconds and then use the mouse and move commands to fly to the lodging/parking area to the north. When you’re done, click the circular red record button again to stop recording. A new set of controls appears at the bottom of the screen, and Google Earth Pro begins to play your video. Use the rewind and fast-forward buttons to skip around in the video, and also use the play/pause button to start or stop. You can click the button with the two arrows to repeat the tour or put it on a loop to keep playing. Once you’ve created a tour you are happy with, save it by clicking the Save Tour button. Call it “Ziontour” in the dialog box that opens. The saved tour is then added to your Places box (just like all the other GEP layers). imcenter\nRight now, the tour can only be played in GEP. You’ll want to export your tour to a high-definition video that can be watched by others or shared on the Web. To start this process, first close the DVR control box in the view by clicking on the x in the upper-right corner of the controls. Next, from the Tools pull-down menu, select Movie Maker. In the Movie Maker dialog box, choose 1280 × 720 (HD) for Video parameters to create a high-definition (HD) video of your tour. Next, under Record from choose the radio button for A saved tour and choose Ziontour (My Places). This will be the source of your video. From the pull-down menu next to File type, choose MJPEG (.mp4). Under Save to, use the browse button to navigate to the drive to which you want to save your video and call it “Ziontourmovie”. Leave the other defaults alone and click Create Movie. A dialog box appears, showing the conversion process involved in creating your video. imcenter\nReturn to the folder where you saved your movie and view it by using your computer’s media player.\nQuestion 8 Submit the final video file of your video tour of Zion to your instructor, who will check it over for its quality and completeness for you to get credit for this question. Measuring Profiles and Slopes in Google Earth Pro Now let’s examine some profiles of the digital terrain model and find the slope information. To begin, return the view to the Park HQ placemark and zoom out so that you can see both the Park HQ and the Zion Overlook placemarks clearly in the view. To examine the terrain profile, you must first draw a path between the two points. Click the ruler tool on the GEP toolbar, and in the dialog box that opens, select the Path tab. In the view, you’ll see that your cursor has changed to a crosshairs symbol. click once on the Park HQ placemark location and then click once on the Zion Overlook placemark position. You should see a yellow line drawn between them, and the length of this line is shown in the Ruler dialog box. In the Ruler dialog box, click Save. A new dialog box opens, allowing you to name this path that you’ve drawn. Call it “ParkHQOverlookPath” and click OK in the naming dialog box. You see a new item called ParkHQOverlookPath added to the Places box. Right-click on it and select Show Elevation Profile. A new window opens below the view, showing the elevation profile between the two points. This profile shows you, in two dimensions, the outline and elevations of the terrain in between the two points. imcenter\nAs you move your cursor along the outline of the terrain in the profile, information appears about the elevation and the slope at the location of your cursor. You also see the corresponding location highlighted in the view. The slope information is a positive percentage as you move uphill from the park headquarters to the overlook and a negative percentage as you move downhill. Carefully examine the profile (and the digital terrain) and answer Questions 9 - 11. Question 9 What is the average slope for the 2.2-mile path between the park headquarters and the overlook (both uphill and downhill)? Question 10 What is the steepest uphill slope between the two points, and where is it located? Question 11 What is the steepest downhill slope between the two points, and where is this located? Select a small section of the path and examine only the profile of that small subset. To do so, click the mouse at a place in the profile where you want to begin. Hold down the left mouse button and move the mouse to the place in the profile where you want to end. Release the mouse button, and you see that small section of the profile you chose in a darker red. Statistics for the elevation and slope of that subset are shown in a red box at the top of the profile. To examine one small part of the profile, on the far right side of the profile (the terrain nearest the overlook), you can see that a section of the landscape drops off sharply. (This should be around the 1.6-mile mark between the Park HQ and the overlook.) Using the method described above, highlight the subset of the profile from the location at the bottom of the drop-off to the overlook and then answer Question 12 For this section of the terrain, what is the maximum uphill and downhill slope? What is the average uphill and downhill slope? Wrapping up There is no need to save anything from this lab, so when done you can simply close without saving. Submit your answers to the questions on blackboard.\nIn lab 9 (~two weeks from now) we will use Google Earth Engine. This platform is still a beta product, and you will need to request access through your Google account here. These are still approved by hand and may take a few days to get. Once approved, you will need to follow the instructions in the email that is sent before you are able to access the platform. Therefore, be sure to do this step before you attempt to work though lab 9.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 07"
    ]
  },
  {
    "objectID": "GEOINT_lab05.html",
    "href": "GEOINT_lab05.html",
    "title": "GEOINT Lab 05",
    "section": "",
    "text": "This lab is a gratefully modified version of lab 5 from Bradley A. Shellito’s Introduction to Geospatial Technologies\n\n\nThis lab introduces you to some of the basic features of GIS. You will be using a free open source program, QGIS, to navigate a GIS environment and begin working with geospatial data. The labs in Chapters 6 and 7 will show you how to utilize several more GIS features; the aim of this lab is to familiarize you with the basic GIS functions of the software. The goals for you to take away from this lab:\nFamiliarize yourself with the GIS software environment of your choice, including basic navigation and tool use with both the Map and Browser Examine characteristics of geospatial data, such as the coordinate system, datum, and projection information Familiarize yourself with adding data and manipulating data layer properties, including changing the symbology and appearance of geospatial data Familiarize yourself with data attribute tables in QGIS Join a table of non-spatial data to a layer’s attribute table in QGIS Make measurements between objects in QGIS Outline:\nLearning Objective Submission requirements Tutorial Downloading data Data prep Working in GIS software Submission Submission requirements Materials\nData Name Description GEOG111_Lab05Questions.docx Handout to turn in rawdata/ACSDT5Y2019.B01003_2021-04-18T051624.zip Population estimates for Kansas Counties from ACS - 2019 rawdata/GOVTUNIT_Kansas_State_GDB.zip Kansas boundaries from the National Map rawdata/STRUCT_Kansas_State_GDB.zip Kansas structures from the National Map rawdata/TRAN_Kansas_State_GDB.zip Kansas transportation layer from the National Map You are answering the questions (laid out in the word doc above and also included in the tutorial below) as you work through the lab. Use full sentences as necessary to answer the prompts and submit it to blackboard.\nTutorial Downloading data In this lab we’ll use data from two of the most common sources in the US, the National Map and the Census bureau. I’ve gone and downloaded the data for Kansas for you to use, but if you want to do this for a state you are more interested in…\nHow to download your own data ↕ Data prep As you become more versed in the geographic sciences, you’ll slowly learn how data needs to be formatted. One of these is that the Census data we downloaded has an extra row which we’ll need to remove before it’s ready to be used. To do this we first need to unzip the files we’ve downloaded. You can do this one at a time with the standard extract feature built into windows, but I recommend 7-Zip. If you highlight all three and right click &gt; 7-Zip &gt; Extract to “*” it will put them in that same location with the same name. imcenter\nRecall that we’re working in the “rawdata” folder, but we are about to (irreversibly) modify the data, so we should create a new folder. In general, I create a “storedata” folder for these intermediate products. So, create that folder and then copy the ****data_with_overlays file into it. Open it up (Excel is fine), and remove the second row by right clicking on the row number and press delete, and the save it and make sure it’s saved in a .csv format. imcenter\nWorking in GIS software Use the start menu to open up your software of choice.\nArcPRO Working with a Project in ArcGIS Pro In the Windows search bar type in “ArcGIS Pro” and open the program. In the top right corner make sure you are signed in to your ESRI account. ArcGIS Pro opens and asks how you want to proceed. You can either open an existing project, begin using a blank project template, or start without a project template. In ArcGIS Pro, a “project” is the means by which you organize all the various layers, tables, maps, and so on of your work, and a “template” is how a project starts when you begin working with ArcGIS Pro. If you were just going to do something quick like examine a GIS layer, you wouldn’t need to use a template. However, as you’ll be doing several things in this lab and to get into the good habit of being digitally organized, click on Map in the New Blank Templates options. You are now prompted to give your project a name and indicate where it will be located. Let’s call this project “lab05” and save it in the lab05 folder. Finally, click OK.imcenter ArcGIS Pro opens to your project. The large area in the center is referred to as the View. Because you chose the Map template, ArcGIS Pro opened with a new Map for you to add your data to. You can see a new tab called Map in the View, and it contains a topographic map of the United States. In ArcGIS Pro, you add GIS data to a Map so you can work with it. You can have multiple different tabs in the View and switch back and forth between them as needed. For instance, you can have several Maps available at once, each containing different data. You can also have tabs open for one or more Layouts. For this lab, you’ll be using only this one Map. Along the top of the window you’ll find the toolbar area. This, along with the toolbox which we’ll cover later, are the primary means of interacting with your data. You see a Catalog pane on the right-hand side of the screen and a Contents pane on the left-hand side of the screen (with the word Contents at the top). The Contents pane shows what is contained in the tab that is currently being used. As you add items to whatever you’re working with, you’ll see them available in the Contents pane. Don’t see them? Click on the view tab at the top and click Reset Panels &gt; Reset Panels for Mapping (default) imcenter8. With several Maps or tabs open, it’s easy to lose track of what the various tabs contain. Even though you’re using only one Map in this exercise, for ease of use, you can change its name to something more distinctive than “Map.” With the Map tab in the View selected, return to the Contents pane. Right-click on Map and choose Properties. imcenter 9. The Map Properties dialog box opens. On the General tab, change the name of this Map from Map to “Kansas”. Then click OK to close the Map Properties dialog box. You should now see that the Map tab has changed to be the Kansas tab as well as having updated in the Contents panel. 10. Click the x on the Kansas Map tab to close the map window. To get this back, in the Catalog pane, you see a Maps folder. Expand this, and you’ll see a list of all the Maps you have available in this particular project. You should have only one: the Kansas Map. You can add this map back into the view by coming to this location and double-clicking the Map.\nAdd and manipulate basemap A fresh map template loads in a standard topographic map, but this can easily be changed on the Map tab under the Basemap icon. Take a second and explore the different options here. Navigate around map Navigating around the map is a critical skill you need in order to be a competent practitioner. Fortunately, Google controls became pretty standard and you likely know most of them at this point simply though intuition and general exposure. Manual controls While the Explore button is on, you can left click to pan around the map. Both the right click and the scroll wheel zooms the map in and out. Does the zoom seem backwards to you? You can change this (The Projects tab &gt; Options &gt; Navigation) There is also a Google Earth style navigation compass. If you right click in the map and select Navigator. This opens a navigator compass on the map which allows you to rotate and navigate around the map with a click. Clicking on the arrow reorients yourself northward. imcenter Go To point 1\nAdd Vector Data In the Catalog pane, expand the Folders option, and you see the folders and data available for this project. If you explore your folders, you’ll notice that none of your data is visible. To add data without moving your data between folders manually, right-click on the Folders option and choose Add Folder Connection. From there, navigate to the folder you’ve stored your data in (in this case, the Lab05 folder). imcenter Expand the geodatabases to verify that it contains the three feature classes you’ll be working with in this lab: Trans_AirportPoint, GU_CountyOrEquivalent, and Trans_TrailSegment. Adding Data to ArcGIS Pro and Working with the Contents Pane Return to the Map. Its corresponding area in the Contents pane is empty, so let’s add some data to work with. Click on the Map tab at the top of the screen, and then within the Layer group, click on the Add Data icon () and select Data. In the dialog box, navigate to the rawdata folder, then the GOVTUNIT_Kansas_State_GDB.gdb geodatabase, and then in the GovernmentUnits you’ll find the GU_CountyOrEquivalent layer. Click OK (or double-click) to add it to the map. imcenter You now see the GU_CountyOrEquivalent feature class listed in the Layers panel and its contents (a set of polygons displaying each of the counties in Kansas) displayed in the Map View. In the Layers panel, you see a checkmark in the box next to the layer. When this checkmark is displayed, the layer is shown in the Map View, and when the checkmark is removed, the layer is not displayed. Let’s add two more layers from the CalData geodatabase: CalAirports (a point layer) and CalTrails (a line layer). All three of your layers are now in the Layers panel. You can manipulate the “drawing order” of items in the Layers panel by grabbing a layer with the mouse and moving it up or down in the Layers panel. Whatever layer is at thebottom is drawn first, and the layers above it in the Layers panel are drawn on top of it. Thus, if you move the CalBounds layer to the top of the Layers panel, the other layers are no longer visible in the Map View because the polygons of the CalBounds layer are being drawn over them. Add Tabular Data Look at attribute tables Joining vector and tabular data Selections Manual pick Select by attribute Select by Location saving Next, you need to get some basic information about your GIS layers. Right-click on the CalBounds layer and choose Show Feature Count. A number in brackets appears after the CalBounds name, indicating how many features are in that layer. If each feature in the CalBounds layer represents a county in California, then this is the number of counties in the dataset. Repeat step 5 for the CalAirports and CalTrails layers and examine their information. Question 1 How many airports are represented as points in the CalAirports dataset? (Remember that each feature is a separate polygon.) How many trails are represented as lines in the CalTrails dataset? Symbology of Features in QGIS Notice that the symbology generated for each of the three objects is simple: Points, lines, and polygons have been assigned a random color. You can significantly alter the appearance of the objects in the Map View to customize your maps. Right-click on the CalBounds layer and select Properties. In the Properties dialog box, select the Symbology tab. Here, you can alter the appearance of the states by changing their style and color, as well as elements of how the polygons are outlined.imcenter Select one of the suggested appearances for your polygons. To select a different color for your layer, first click on Fill and then clicking on the solid color bar beside the Color pull-down menu and choose the color you want. Clicking once on Simple fill brings up more options for adjusting the color, border color, border thickness, and so on. These options allow you to alter the CalBounds layer to make it more appealing. After you’ve selected all the options you want, click Apply to make the changes and then click OK to close the dialog box. Repeat step 4 to change the appearances for the CalAirports and CalTrails feature classes. Note that you can also change the points for the airports into shapes such as stars, triangles, or a default airport symbol, and you can change the size and color of the airports. Several different styles are available for the lines of the trails as well. Obtaining Projection Information Chapter 2 discusses the importance of projections, coordinate systems, and datums. All of the information associated with these concepts can be accessed using QGIS. Right-click on the CalBounds feature class and select Properties. Click on the Information tab in the dialog box. Information about the layer (also known as the metadata, or “data about the data”) is shown. The top of the dialog box gives a variety of information, including some shorthand about the coordinate system being used. Carefully read through the abbreviated information to determine what units of measurement are being used for the CalBounds layer. Click OK to exit the CalBounds Properties dialog box. Question 2 What units of measurement (feet, meters, degrees, etc.) are being used in the CalTrails dataset? Question 3 What units of measurement (feet, meters, degrees, etc.) are being used in the CalAirports dataset? QGIS allows you to specify the projection information used by the entire project. Because your layers are in the U.S. National Atlas Equal Area projection, you need to tell QGIS to use this projection information while you’re using that data in this lab. To do this, select the Project pull-down menu and then choose Properties. In the Project Properties dialog box, select the CRS tab (which stands for Coordinate Reference System). In the filter box, type US National Atlas to limit your search to find the U.S. National Atlas Equal Area projection, which is just one of the hundreds of supported projections. In the bottom panel, select the US National Atlas Equal Area option. Question 4 To which “family” of projections does the U.S. National Atlas Equal Area projection belong? Hint: QGIS shows a hierarchical system of projections that can help you answer this question. imcenter\nClick Apply to make the changes and then click OK to close the dialog box. Examining Attribute Tables Each of your three layers is represented by points (airports), lines (trails), and polygons (the counties), but each layer also has a corresponding set of attributes that accompanies the spatial features. As noted in the chapter, these non-spatial attributes are stored in an attribute table for each layer. To get started working with this non-spatial data, in the Layers panel, right-click on the CalBounds layer and choose Open Attribute Table. imcenter\nA new window opens, showing the CalBounds attribute table. Each of the rows of the table is a record representing an individual feature (in this case, a single polygon of a California county), and the columns of the table are the fields representing the attributes for each of those records. By perusing the attribute table, you can find information about a particular record. However, with 58 records, it could take some time to find specific information. One way to organize the table is to sort it: To do this, scroll across the CalBounds attribute table until you find the field called County_Name, click the County_Name table header, and the table sorts itself into alphabetical order by the name of the county. Click on the County_Name table header a second time, and the table sorts itself into reverse alphabetical order. Click it a third time to return the sorting of the table to alphabetical order. With the table sorted, scroll through it to find the record with the County_Name attribute San Diego. Each county has a FIPS code assigned to it; this is a numerical value that is a government designation to uniquely identify things such as states, counties, and townships. The FIPS code information for each California county is located in this attribute table. Question 5 What is the county FIPS code for San Diego County? Joining a Table to an Attribute Table While there are a lot of records in the CalBounds attribute table, QGIS allows you to add further attributes to them. This can be done by taking another table that contains additional non-spatial information and joining it to the CalBounds attribute table. Tobegin, the first thing you need to do is add to QGIS a table of non-spatial data with which to work. From the Layer pull-down menu, choose Add Layer and then choose Add Delimited Text Layer. In the Data Source Manager dialog box, click the Browse button next to File Name, navigate to the C:5QGIS folder and choose the CAPop.csv file, and click Open. Under File Format, select the radio button for CSV (comma separated values). Expand the options for Geometry definition and select the radio button for No geometry (attribute only table) because this is only a table of values, not something that will be a spatial layer. Leave the other settings as their defaults. At the bottom of the dialog box you can see a preview of what your added table will look like. If everything looks okay, click Add. A new item called CAPop is added to the Layers panel. This is a table that contains a new set of attributes. Click Close in the Data Source Manager dialog box if it is still open. Right-click on CAPop and choose Open Attribute Table. You see the new non-spatial table open in QGIS; it contains a field with the county’s FIPS code, the name of that county, and the population of that county. Before you can join two tables together, they need to contain the same field; in this case, the CAPop table contains a field called N that contains the name of the county, and the CalBounds layer’s attribute table contains a field called County_Name that contains the same information. Because these two tables (the spatial data from CalBounds and the non-spatial data from CAPop) have a field in common (i.e., they both have a field containing the name of each county), they can be joined together. To perform the join, right-click on the CalBounds layer, select Properties, and select the Joins tab. imcenter\nAt the bottom of the Joins dialog box, click the green plus button to indicate that you want to join a table to the layer with which you’re working. In the Add Vector Join dialog, for Join Layer, select CAPop (which is the table containing the information you want to join to the spatial layer). For Join Field, select N (which is the name of the county name field in the CAPop table). For Target Field, select County_Name (which is the name of the county name field in the CalBounds layer). Put a checkmark in the Joined Fields box and put checkmarks next to the CAFIPS, N, and Pop2010 options to join the data from the Pop2010 field from CAPop to CalBounds. Put a checkmark in the Custom Field Name Prefix box so that any of the joined fields will start with the prefix CAPop_, which will help you keep them separate. Leave the other options alone and click OK to perform the join. Then click OK in the Layer Properties dialog box to close it imcenter\nOpen the CalBounds layer’s attribute table and scroll across to the far right. You now see the population values from the CAPop table joined for each record in the attribute table (i.e., each county now has a value for population joined to it).18. Now that CAPop_Pop2010 is a field in the attribute table, you can sort it from highest to lowest populations (or vice versa) by clicking on the field header, just as you can do to sort any other field (in the same way you sorted the counties alphabetically by name). Keeping this ability in mind, answer Questions 5.6–5.9. Question 6 What is the 2010 population of Los Angeles County? Question 7 What is the 2010 population of the California county with FIPS code 065? Question 8 What county in California has the lowest 2010 population, and what is that population value? Question 9 What county in California has the third-highest 2010 population, and what is that population value? Navigating the Map View You’ll now just focus on one area of California: San Diego County. In the CalBounds attribute table, scroll through until you find San Diego County and then click on the header of the record on the far-left of the attribute table. Back in the Map View, you see San Diego County now highlighted in yellow. imcenter\nQGIS provides a number of tools for navigating around the data layers in the view. Click on the icon that shows a magnifying glass with three arrows to zoom the Map View to the full extent of all the layers. It’s good to do this if you’ve zoomed too far in or out in the Map View or need to restart. The other magnifying glass icons allow you to zoom in (the plus icon) or out (the minus icon). You can zoom by clicking in the window or by clicking and dragging a box around the area you want to zoom in on. imcenter\nThe hand icon, which is the Pan tool, allows you to “grab” the Map View by clicking on it and dragging the map around the screen for navigation. Use the Zoom and Pan tools to center the Map View on San Diego County so that you’re able to see the entirety of the state and its airports and trails. Return to the CalBounds attribute table and hold down the Ctrl key on the keyboard while clicking the header record for the county again to remove the blue highlighting. Then close the attribute table. 5.8 Interactively Obtaining Information Even with the Map View centered on San Diego County, there are an awful lot of point symbols there, representing the airports. You are going to want to identify and use only a couple of them. QGIS has a searching tool that allows you to locate specific objects from a dataset. Right-click on the CalAirports layer and select Open Attribute Table. The attribute table for the layer appears as a new window. You need to find out which of these 1163 records represents San Diego International Airport. To begin searching the attribute table, click on the button at the bottom that says Show All Features and choose the option Field Filter. A list of all of the attributes appears; choose the one called Name. You can now search through the attribute table on the basis of the Name field. At the bottom of the dialog box, type San Diego International Airport. Click on the Enter key on the keyboard. The record where the Name attribute is “San Diego International” appears as the only record in the attribute table. Click on the record header next to the Name field (the tab should be labeled 269), and you see the record highlighted in dark blue, just like when you selected San Diego County previously. This indicates that you’ve selected the record. Drag the dialog box out of the way so that you can see both it and the Map View at the same time. You can see that the point symbol for the San Diego International Airport has changed to a yellow color. This means it has been selected by QGIS, and any actions performed on the CalAirports layer will affect only the selected items, not all the items in CalAirports. However, all that you’ve done so far is locate and select an object. To obtain information about it, you can use the Identify Features tool (the icon on the toolbar with a white cursor pointing to the letter i in a blue circle). Identify Features works only with the layer that is selected in the Map Legend, so make sure CalAirports (i.e., the layer in which you want to identify items) is highlighted in the Map Legend before choosing the tool Select the Identify Features tool and then click on the point representing San Diego International Airport (zooming in as necessary). A new panel called Identify Results appears, listing all the attributes for the CalAirports layer. If you click on the View Feature Form button at the top of the panel, you can see all the field attribute information that goes along with the record/point representing San Diego International Airport. imcenter\nQuestion 10 What is the three-letter FAA airport classification code for San Diego International Airport? Close the Identify Results panel after you’ve answered the question. Labeling Features Rather than dealing with several points on a map and trying to remember which airport is which, it’s easier to simply label each airport so that its name appears in the Map View.QGIS gives you the ability to do this by creating a label for each record and allowing you to choose the field with which to create the label. To start, right-click on the CalAirports layer in the Map Legend and select Properties. Click on the Labels tab. From the pull-down menu at the top of the dialog box, choose Single labels. From the pull-down menu next to Label with, choose Name. imcenter\nFor now, accept all the defaults and click Apply and then click OK. Back in the Map View, you see that labels of the airport names have been created and placed near the appropriate points. Change any of the options—font size, color, label placement, and even the angle—to set up the labels so that you can easily examine the map data. Measurements on a Map With all the airports now labeled, it’s easier to keep track of them. Your next task is to make a series of measurements between points to find the Euclidian (straight-line) distance between airports around San Diego. Zoom in tightly so that your Map View contains the airports San Diego International and North Island Naval Air Station. Select the Measure Line tool from the toolbar (the one that resembles a gray ruler with a line positioned over the top). imcenter\nYou see that the cursor has turned into a crosshairs. Place the crosshairs on the point representing San Diego International Airport and left-click the mouse. Drag the crosshairs south and west to the point representing North Island Naval Air Station and click the mouse there. The distance measurement appears in the Measure box. If you click on the Info text in the dialog box, you see that you are measuring “ellipsoidal” distance, referred to as the “surface” distance as measured on a sphere. Note that each line of measurement you make with the mouse counts as an individual “segment,” and the value listed in the box for “Total” is the sum of all segments. Question 11 What is the ellipsoidal distance from San Diego International Airport to North Island Naval Air Station?5. Clear all the lines of measurement by clicking on New in the Measure box. Zoom out a bit so that you can see some of the other airports in the region, particularly Montgomery Field and Brown Field Municipal Airport. Question 12 What is the total ellipsoidal distance in miles from San Diego International Airport to Montgomery Field, then from Montgomery Field to Brown Field Municipal Airport, and then finally from Brown Field Municipal Airport back to San Diego International Airport? Saving Your Work (and Working on It Later) When you’re using QGIS, you can save your work at any time and then return to it later. When work is saved in QGIS, a QGZ file is written to disk. Later, you can reopen this file to pick up your work where you left off. Saving to a QGZ file is done by selecting the Project pull-down menu and then selecting Save. Files can be reopened by choosing Open from the Project pull-down menu. Exit QGIS by selecting Exit QGIS from the Project pull-down menu. QGIS Submission All you have to turn into blackboard for this week is the final image you created above.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 05"
    ]
  },
  {
    "objectID": "GEOINT_lab05.html#learning-objective",
    "href": "GEOINT_lab05.html#learning-objective",
    "title": "GEOINT Lab 05",
    "section": "",
    "text": "This lab introduces you to some of the basic features of GIS. You will be using a free open source program, QGIS, to navigate a GIS environment and begin working with geospatial data. The labs in Chapters 6 and 7 will show you how to utilize several more GIS features; the aim of this lab is to familiarize you with the basic GIS functions of the software. The goals for you to take away from this lab:\nFamiliarize yourself with the GIS software environment of your choice, including basic navigation and tool use with both the Map and Browser Examine characteristics of geospatial data, such as the coordinate system, datum, and projection information Familiarize yourself with adding data and manipulating data layer properties, including changing the symbology and appearance of geospatial data Familiarize yourself with data attribute tables in QGIS Join a table of non-spatial data to a layer’s attribute table in QGIS Make measurements between objects in QGIS Outline:\nLearning Objective Submission requirements Tutorial Downloading data Data prep Working in GIS software Submission Submission requirements Materials\nData Name Description GEOG111_Lab05Questions.docx Handout to turn in rawdata/ACSDT5Y2019.B01003_2021-04-18T051624.zip Population estimates for Kansas Counties from ACS - 2019 rawdata/GOVTUNIT_Kansas_State_GDB.zip Kansas boundaries from the National Map rawdata/STRUCT_Kansas_State_GDB.zip Kansas structures from the National Map rawdata/TRAN_Kansas_State_GDB.zip Kansas transportation layer from the National Map You are answering the questions (laid out in the word doc above and also included in the tutorial below) as you work through the lab. Use full sentences as necessary to answer the prompts and submit it to blackboard.\nTutorial Downloading data In this lab we’ll use data from two of the most common sources in the US, the National Map and the Census bureau. I’ve gone and downloaded the data for Kansas for you to use, but if you want to do this for a state you are more interested in…\nHow to download your own data ↕ Data prep As you become more versed in the geographic sciences, you’ll slowly learn how data needs to be formatted. One of these is that the Census data we downloaded has an extra row which we’ll need to remove before it’s ready to be used. To do this we first need to unzip the files we’ve downloaded. You can do this one at a time with the standard extract feature built into windows, but I recommend 7-Zip. If you highlight all three and right click &gt; 7-Zip &gt; Extract to “*” it will put them in that same location with the same name. imcenter\nRecall that we’re working in the “rawdata” folder, but we are about to (irreversibly) modify the data, so we should create a new folder. In general, I create a “storedata” folder for these intermediate products. So, create that folder and then copy the ****data_with_overlays file into it. Open it up (Excel is fine), and remove the second row by right clicking on the row number and press delete, and the save it and make sure it’s saved in a .csv format. imcenter\nWorking in GIS software Use the start menu to open up your software of choice.\nArcPRO Working with a Project in ArcGIS Pro In the Windows search bar type in “ArcGIS Pro” and open the program. In the top right corner make sure you are signed in to your ESRI account. ArcGIS Pro opens and asks how you want to proceed. You can either open an existing project, begin using a blank project template, or start without a project template. In ArcGIS Pro, a “project” is the means by which you organize all the various layers, tables, maps, and so on of your work, and a “template” is how a project starts when you begin working with ArcGIS Pro. If you were just going to do something quick like examine a GIS layer, you wouldn’t need to use a template. However, as you’ll be doing several things in this lab and to get into the good habit of being digitally organized, click on Map in the New Blank Templates options. You are now prompted to give your project a name and indicate where it will be located. Let’s call this project “lab05” and save it in the lab05 folder. Finally, click OK.imcenter ArcGIS Pro opens to your project. The large area in the center is referred to as the View. Because you chose the Map template, ArcGIS Pro opened with a new Map for you to add your data to. You can see a new tab called Map in the View, and it contains a topographic map of the United States. In ArcGIS Pro, you add GIS data to a Map so you can work with it. You can have multiple different tabs in the View and switch back and forth between them as needed. For instance, you can have several Maps available at once, each containing different data. You can also have tabs open for one or more Layouts. For this lab, you’ll be using only this one Map. Along the top of the window you’ll find the toolbar area. This, along with the toolbox which we’ll cover later, are the primary means of interacting with your data. You see a Catalog pane on the right-hand side of the screen and a Contents pane on the left-hand side of the screen (with the word Contents at the top). The Contents pane shows what is contained in the tab that is currently being used. As you add items to whatever you’re working with, you’ll see them available in the Contents pane. Don’t see them? Click on the view tab at the top and click Reset Panels &gt; Reset Panels for Mapping (default) imcenter8. With several Maps or tabs open, it’s easy to lose track of what the various tabs contain. Even though you’re using only one Map in this exercise, for ease of use, you can change its name to something more distinctive than “Map.” With the Map tab in the View selected, return to the Contents pane. Right-click on Map and choose Properties. imcenter 9. The Map Properties dialog box opens. On the General tab, change the name of this Map from Map to “Kansas”. Then click OK to close the Map Properties dialog box. You should now see that the Map tab has changed to be the Kansas tab as well as having updated in the Contents panel. 10. Click the x on the Kansas Map tab to close the map window. To get this back, in the Catalog pane, you see a Maps folder. Expand this, and you’ll see a list of all the Maps you have available in this particular project. You should have only one: the Kansas Map. You can add this map back into the view by coming to this location and double-clicking the Map.\nAdd and manipulate basemap A fresh map template loads in a standard topographic map, but this can easily be changed on the Map tab under the Basemap icon. Take a second and explore the different options here. Navigate around map Navigating around the map is a critical skill you need in order to be a competent practitioner. Fortunately, Google controls became pretty standard and you likely know most of them at this point simply though intuition and general exposure. Manual controls While the Explore button is on, you can left click to pan around the map. Both the right click and the scroll wheel zooms the map in and out. Does the zoom seem backwards to you? You can change this (The Projects tab &gt; Options &gt; Navigation) There is also a Google Earth style navigation compass. If you right click in the map and select Navigator. This opens a navigator compass on the map which allows you to rotate and navigate around the map with a click. Clicking on the arrow reorients yourself northward. imcenter Go To point 1\nAdd Vector Data In the Catalog pane, expand the Folders option, and you see the folders and data available for this project. If you explore your folders, you’ll notice that none of your data is visible. To add data without moving your data between folders manually, right-click on the Folders option and choose Add Folder Connection. From there, navigate to the folder you’ve stored your data in (in this case, the Lab05 folder). imcenter Expand the geodatabases to verify that it contains the three feature classes you’ll be working with in this lab: Trans_AirportPoint, GU_CountyOrEquivalent, and Trans_TrailSegment. Adding Data to ArcGIS Pro and Working with the Contents Pane Return to the Map. Its corresponding area in the Contents pane is empty, so let’s add some data to work with. Click on the Map tab at the top of the screen, and then within the Layer group, click on the Add Data icon () and select Data. In the dialog box, navigate to the rawdata folder, then the GOVTUNIT_Kansas_State_GDB.gdb geodatabase, and then in the GovernmentUnits you’ll find the GU_CountyOrEquivalent layer. Click OK (or double-click) to add it to the map. imcenter You now see the GU_CountyOrEquivalent feature class listed in the Layers panel and its contents (a set of polygons displaying each of the counties in Kansas) displayed in the Map View. In the Layers panel, you see a checkmark in the box next to the layer. When this checkmark is displayed, the layer is shown in the Map View, and when the checkmark is removed, the layer is not displayed. Let’s add two more layers from the CalData geodatabase: CalAirports (a point layer) and CalTrails (a line layer). All three of your layers are now in the Layers panel. You can manipulate the “drawing order” of items in the Layers panel by grabbing a layer with the mouse and moving it up or down in the Layers panel. Whatever layer is at thebottom is drawn first, and the layers above it in the Layers panel are drawn on top of it. Thus, if you move the CalBounds layer to the top of the Layers panel, the other layers are no longer visible in the Map View because the polygons of the CalBounds layer are being drawn over them. Add Tabular Data Look at attribute tables Joining vector and tabular data Selections Manual pick Select by attribute Select by Location saving Next, you need to get some basic information about your GIS layers. Right-click on the CalBounds layer and choose Show Feature Count. A number in brackets appears after the CalBounds name, indicating how many features are in that layer. If each feature in the CalBounds layer represents a county in California, then this is the number of counties in the dataset. Repeat step 5 for the CalAirports and CalTrails layers and examine their information. Question 1 How many airports are represented as points in the CalAirports dataset? (Remember that each feature is a separate polygon.) How many trails are represented as lines in the CalTrails dataset? Symbology of Features in QGIS Notice that the symbology generated for each of the three objects is simple: Points, lines, and polygons have been assigned a random color. You can significantly alter the appearance of the objects in the Map View to customize your maps. Right-click on the CalBounds layer and select Properties. In the Properties dialog box, select the Symbology tab. Here, you can alter the appearance of the states by changing their style and color, as well as elements of how the polygons are outlined.imcenter Select one of the suggested appearances for your polygons. To select a different color for your layer, first click on Fill and then clicking on the solid color bar beside the Color pull-down menu and choose the color you want. Clicking once on Simple fill brings up more options for adjusting the color, border color, border thickness, and so on. These options allow you to alter the CalBounds layer to make it more appealing. After you’ve selected all the options you want, click Apply to make the changes and then click OK to close the dialog box. Repeat step 4 to change the appearances for the CalAirports and CalTrails feature classes. Note that you can also change the points for the airports into shapes such as stars, triangles, or a default airport symbol, and you can change the size and color of the airports. Several different styles are available for the lines of the trails as well. Obtaining Projection Information Chapter 2 discusses the importance of projections, coordinate systems, and datums. All of the information associated with these concepts can be accessed using QGIS. Right-click on the CalBounds feature class and select Properties. Click on the Information tab in the dialog box. Information about the layer (also known as the metadata, or “data about the data”) is shown. The top of the dialog box gives a variety of information, including some shorthand about the coordinate system being used. Carefully read through the abbreviated information to determine what units of measurement are being used for the CalBounds layer. Click OK to exit the CalBounds Properties dialog box. Question 2 What units of measurement (feet, meters, degrees, etc.) are being used in the CalTrails dataset? Question 3 What units of measurement (feet, meters, degrees, etc.) are being used in the CalAirports dataset? QGIS allows you to specify the projection information used by the entire project. Because your layers are in the U.S. National Atlas Equal Area projection, you need to tell QGIS to use this projection information while you’re using that data in this lab. To do this, select the Project pull-down menu and then choose Properties. In the Project Properties dialog box, select the CRS tab (which stands for Coordinate Reference System). In the filter box, type US National Atlas to limit your search to find the U.S. National Atlas Equal Area projection, which is just one of the hundreds of supported projections. In the bottom panel, select the US National Atlas Equal Area option. Question 4 To which “family” of projections does the U.S. National Atlas Equal Area projection belong? Hint: QGIS shows a hierarchical system of projections that can help you answer this question. imcenter\nClick Apply to make the changes and then click OK to close the dialog box. Examining Attribute Tables Each of your three layers is represented by points (airports), lines (trails), and polygons (the counties), but each layer also has a corresponding set of attributes that accompanies the spatial features. As noted in the chapter, these non-spatial attributes are stored in an attribute table for each layer. To get started working with this non-spatial data, in the Layers panel, right-click on the CalBounds layer and choose Open Attribute Table. imcenter\nA new window opens, showing the CalBounds attribute table. Each of the rows of the table is a record representing an individual feature (in this case, a single polygon of a California county), and the columns of the table are the fields representing the attributes for each of those records. By perusing the attribute table, you can find information about a particular record. However, with 58 records, it could take some time to find specific information. One way to organize the table is to sort it: To do this, scroll across the CalBounds attribute table until you find the field called County_Name, click the County_Name table header, and the table sorts itself into alphabetical order by the name of the county. Click on the County_Name table header a second time, and the table sorts itself into reverse alphabetical order. Click it a third time to return the sorting of the table to alphabetical order. With the table sorted, scroll through it to find the record with the County_Name attribute San Diego. Each county has a FIPS code assigned to it; this is a numerical value that is a government designation to uniquely identify things such as states, counties, and townships. The FIPS code information for each California county is located in this attribute table. Question 5 What is the county FIPS code for San Diego County? Joining a Table to an Attribute Table While there are a lot of records in the CalBounds attribute table, QGIS allows you to add further attributes to them. This can be done by taking another table that contains additional non-spatial information and joining it to the CalBounds attribute table. Tobegin, the first thing you need to do is add to QGIS a table of non-spatial data with which to work. From the Layer pull-down menu, choose Add Layer and then choose Add Delimited Text Layer. In the Data Source Manager dialog box, click the Browse button next to File Name, navigate to the C:5QGIS folder and choose the CAPop.csv file, and click Open. Under File Format, select the radio button for CSV (comma separated values). Expand the options for Geometry definition and select the radio button for No geometry (attribute only table) because this is only a table of values, not something that will be a spatial layer. Leave the other settings as their defaults. At the bottom of the dialog box you can see a preview of what your added table will look like. If everything looks okay, click Add. A new item called CAPop is added to the Layers panel. This is a table that contains a new set of attributes. Click Close in the Data Source Manager dialog box if it is still open. Right-click on CAPop and choose Open Attribute Table. You see the new non-spatial table open in QGIS; it contains a field with the county’s FIPS code, the name of that county, and the population of that county. Before you can join two tables together, they need to contain the same field; in this case, the CAPop table contains a field called N that contains the name of the county, and the CalBounds layer’s attribute table contains a field called County_Name that contains the same information. Because these two tables (the spatial data from CalBounds and the non-spatial data from CAPop) have a field in common (i.e., they both have a field containing the name of each county), they can be joined together. To perform the join, right-click on the CalBounds layer, select Properties, and select the Joins tab. imcenter\nAt the bottom of the Joins dialog box, click the green plus button to indicate that you want to join a table to the layer with which you’re working. In the Add Vector Join dialog, for Join Layer, select CAPop (which is the table containing the information you want to join to the spatial layer). For Join Field, select N (which is the name of the county name field in the CAPop table). For Target Field, select County_Name (which is the name of the county name field in the CalBounds layer). Put a checkmark in the Joined Fields box and put checkmarks next to the CAFIPS, N, and Pop2010 options to join the data from the Pop2010 field from CAPop to CalBounds. Put a checkmark in the Custom Field Name Prefix box so that any of the joined fields will start with the prefix CAPop_, which will help you keep them separate. Leave the other options alone and click OK to perform the join. Then click OK in the Layer Properties dialog box to close it imcenter\nOpen the CalBounds layer’s attribute table and scroll across to the far right. You now see the population values from the CAPop table joined for each record in the attribute table (i.e., each county now has a value for population joined to it).18. Now that CAPop_Pop2010 is a field in the attribute table, you can sort it from highest to lowest populations (or vice versa) by clicking on the field header, just as you can do to sort any other field (in the same way you sorted the counties alphabetically by name). Keeping this ability in mind, answer Questions 5.6–5.9. Question 6 What is the 2010 population of Los Angeles County? Question 7 What is the 2010 population of the California county with FIPS code 065? Question 8 What county in California has the lowest 2010 population, and what is that population value? Question 9 What county in California has the third-highest 2010 population, and what is that population value? Navigating the Map View You’ll now just focus on one area of California: San Diego County. In the CalBounds attribute table, scroll through until you find San Diego County and then click on the header of the record on the far-left of the attribute table. Back in the Map View, you see San Diego County now highlighted in yellow. imcenter\nQGIS provides a number of tools for navigating around the data layers in the view. Click on the icon that shows a magnifying glass with three arrows to zoom the Map View to the full extent of all the layers. It’s good to do this if you’ve zoomed too far in or out in the Map View or need to restart. The other magnifying glass icons allow you to zoom in (the plus icon) or out (the minus icon). You can zoom by clicking in the window or by clicking and dragging a box around the area you want to zoom in on. imcenter\nThe hand icon, which is the Pan tool, allows you to “grab” the Map View by clicking on it and dragging the map around the screen for navigation. Use the Zoom and Pan tools to center the Map View on San Diego County so that you’re able to see the entirety of the state and its airports and trails. Return to the CalBounds attribute table and hold down the Ctrl key on the keyboard while clicking the header record for the county again to remove the blue highlighting. Then close the attribute table. 5.8 Interactively Obtaining Information Even with the Map View centered on San Diego County, there are an awful lot of point symbols there, representing the airports. You are going to want to identify and use only a couple of them. QGIS has a searching tool that allows you to locate specific objects from a dataset. Right-click on the CalAirports layer and select Open Attribute Table. The attribute table for the layer appears as a new window. You need to find out which of these 1163 records represents San Diego International Airport. To begin searching the attribute table, click on the button at the bottom that says Show All Features and choose the option Field Filter. A list of all of the attributes appears; choose the one called Name. You can now search through the attribute table on the basis of the Name field. At the bottom of the dialog box, type San Diego International Airport. Click on the Enter key on the keyboard. The record where the Name attribute is “San Diego International” appears as the only record in the attribute table. Click on the record header next to the Name field (the tab should be labeled 269), and you see the record highlighted in dark blue, just like when you selected San Diego County previously. This indicates that you’ve selected the record. Drag the dialog box out of the way so that you can see both it and the Map View at the same time. You can see that the point symbol for the San Diego International Airport has changed to a yellow color. This means it has been selected by QGIS, and any actions performed on the CalAirports layer will affect only the selected items, not all the items in CalAirports. However, all that you’ve done so far is locate and select an object. To obtain information about it, you can use the Identify Features tool (the icon on the toolbar with a white cursor pointing to the letter i in a blue circle). Identify Features works only with the layer that is selected in the Map Legend, so make sure CalAirports (i.e., the layer in which you want to identify items) is highlighted in the Map Legend before choosing the tool Select the Identify Features tool and then click on the point representing San Diego International Airport (zooming in as necessary). A new panel called Identify Results appears, listing all the attributes for the CalAirports layer. If you click on the View Feature Form button at the top of the panel, you can see all the field attribute information that goes along with the record/point representing San Diego International Airport. imcenter\nQuestion 10 What is the three-letter FAA airport classification code for San Diego International Airport? Close the Identify Results panel after you’ve answered the question. Labeling Features Rather than dealing with several points on a map and trying to remember which airport is which, it’s easier to simply label each airport so that its name appears in the Map View.QGIS gives you the ability to do this by creating a label for each record and allowing you to choose the field with which to create the label. To start, right-click on the CalAirports layer in the Map Legend and select Properties. Click on the Labels tab. From the pull-down menu at the top of the dialog box, choose Single labels. From the pull-down menu next to Label with, choose Name. imcenter\nFor now, accept all the defaults and click Apply and then click OK. Back in the Map View, you see that labels of the airport names have been created and placed near the appropriate points. Change any of the options—font size, color, label placement, and even the angle—to set up the labels so that you can easily examine the map data. Measurements on a Map With all the airports now labeled, it’s easier to keep track of them. Your next task is to make a series of measurements between points to find the Euclidian (straight-line) distance between airports around San Diego. Zoom in tightly so that your Map View contains the airports San Diego International and North Island Naval Air Station. Select the Measure Line tool from the toolbar (the one that resembles a gray ruler with a line positioned over the top). imcenter\nYou see that the cursor has turned into a crosshairs. Place the crosshairs on the point representing San Diego International Airport and left-click the mouse. Drag the crosshairs south and west to the point representing North Island Naval Air Station and click the mouse there. The distance measurement appears in the Measure box. If you click on the Info text in the dialog box, you see that you are measuring “ellipsoidal” distance, referred to as the “surface” distance as measured on a sphere. Note that each line of measurement you make with the mouse counts as an individual “segment,” and the value listed in the box for “Total” is the sum of all segments. Question 11 What is the ellipsoidal distance from San Diego International Airport to North Island Naval Air Station?5. Clear all the lines of measurement by clicking on New in the Measure box. Zoom out a bit so that you can see some of the other airports in the region, particularly Montgomery Field and Brown Field Municipal Airport. Question 12 What is the total ellipsoidal distance in miles from San Diego International Airport to Montgomery Field, then from Montgomery Field to Brown Field Municipal Airport, and then finally from Brown Field Municipal Airport back to San Diego International Airport? Saving Your Work (and Working on It Later) When you’re using QGIS, you can save your work at any time and then return to it later. When work is saved in QGIS, a QGZ file is written to disk. Later, you can reopen this file to pick up your work where you left off. Saving to a QGZ file is done by selecting the Project pull-down menu and then selecting Save. Files can be reopened by choosing Open from the Project pull-down menu. Exit QGIS by selecting Exit QGIS from the Project pull-down menu. QGIS Submission All you have to turn into blackboard for this week is the final image you created above.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 05"
    ]
  },
  {
    "objectID": "GEOINT_lab03.html",
    "href": "GEOINT_lab03.html",
    "title": "GEOINT Lab 03",
    "section": "",
    "text": "Lab 03 - Coordinates and Position Measurements This lab is a gratefully modified version of lab 2 from Bradley A. Shellito’s Introduction to Geospatial Technologies\nLearning Objective In this lab we’ll continue using Google Earth Pro (GEP) to examine coordinate systems and the relationships between various sets of coordinates and the objects they represent in the real world. In addition, we’ll make more measurements using GEP and compare measurements made by using different coordinate systems. The goals for you to take away from this lab:\nSet up a graticule of lines in GEP Locate places and objects based only on their coordinates Make measurements across long and short distances and then compare measurements with surface distance calculations Translate latitude/longitude coordinates into UTM Outline:\nLearning Objective Submission requirements Tutorial Examining Coordinates and Distance Measurements in Google Earth Pro Using UTM Coordinates and Measurements in Google Earth Pro Wrapping up Submission requirements Data Name Description GEOG111_Lab3Questions.docx Handout to turn in You are answering the questions (laid out in the word doc above and also included in the tutorial below) as you work through the lab. Use full sentences as necessary to answer the prompts, and submit it to blackboard when done. Tutorial Examining Coordinates and Distance Measurements in Google Earth Pro Start Google Earth Pro. Once Earth settles into view, scroll your mouse around Earth. You’ll see a set of latitude and longitude coordinates appear at the bottom of the view; these represent the coordinates assigned to your mouse’s location. By default, GEP uses the GCS coordinate system and the WGS84 datum. To examine the full graticule of latitude and longitude lines, from the View menu make sure Grid is selected. Some key GCS lines will be highlighted in yellow amid the web of lat/long lines—the Equator, the Prime Meridian, the Antimeridian (how Google Earth labels the 180th meridian), the Tropic of Cancer, and the Tropic of Capricorn. We’ll begin our adventure in Lawrence, KS. Type that into the Search box. GEP will rotate and zoom to the area. You’ll also see the spaces between the lat/long lines grow smaller and new values appear as GEP zooms in. Next, we’ll go a specific location in Lawrence. Type “38.9584517, -95.251449” into the search box. These are the decimal degree lat/long coordinates of the Lindley Hall. Click the Placemark button on the toolbar (Review Lab 1 if you need a quick refresher). The yellow pushpin is automatically placed at the lat/long coordinates you had GEP search for so there’s no need to manually place it. In the new placemark dialog box, type “Lindley Hall” and change the pin symbology to something you like. The coordinates for Lindley Hall are in decimal degrees, but other methods of displaying coordinates are available in GEP. From the Tools pull-down menu, select Options. In the Show Lat/Long options, select the radio button next to Degrees, Minutes, Seconds, click Apply and then OK. imcenter\nFrom the Places box in GEP, right-click on the placemark Lindley Hall and then select Properties. The coordinates are changed to degrees, minutes, and seconds (DMS). Answer Question 1. Close the Lindley Hall placemark dialog box.\nQuestion 1 What are the coordinates for Lindley Hall, in degrees, minutes, and seconds? Decimal degree coordinates for Thai Diner are “38.9435517, -95.2424942”. Use the Search box to zoom to these coordinates and put a placemark at that spot. Name it Thai Diner, and answer Question 2. Question 2 What are the coordinates for the Thai Diner, in degrees, minutes, and seconds? Adjust the view so you can see both placemarks at the edges of the view. Select the Ruler tool and compute the distance between the Lindley Hall and the Thai Diner (again, review Lab 1 if you need a quick refresher). Answer Question 3. Question 3 According to GEP, what is the distance between Lindley Hall and Thai Diner? It’s such a short distance (relative to the size of the globe) from Lindley Hall to Thai Diner that the differences between measurements should be very small, so let’s look at some larger distances.\nLindley Hall (in London, England) is located at “51.4944692, -0.1344998”. Use the Search box to zoom to these coordinates. Once GEP arrives there, put a placemark at that spot, name it “Lindley Hall, London”, and then answer Question 4 Question 4 What are the coordinates for Lindley Hall, London, in degrees, minutes, and seconds? Use the Ruler to compute the distance between Lindley Hall and Lindley Hall, London. You will have to pan and zoom to get the ruler to measure between both locations. Answer Questions 5 and 6. Close the Ruler dialog box when you’re done. Question 5 According to your measurement in GEP, what is the computed distance (in miles) between the Lindley Hall and Lindley Hall, London? Question 6 Why is the line curved rather than straight? What kind of distance is being computed here? Of course, this is a rough estimate because the scale of the view and the imprecise nature of your point and click makes hitting the placemarks directly with the start point and the end point of the ruler difficult.\nTo check your measurements using a Web utility, go to https://www.movable-type.co.uk/scripts/latlong.html. This Website enables you to compute the surface (real-world) distance between two sets of lat/long coordinates. Type your answer to Question 1 as the degrees, minutes, and seconds for the latitude and longitude of Point 1, and your answer from Question 4 in Point 2. Note that you can simply write each of those coordinates with a space in between them; for instance, 40 degrees, 41 minutes, 52 seconds North latitude can be typed in the box as 40 41 52 N.\nClick the see it on a map button. The surface distance between the two points is computed in kilometers, and a zoomable map of the great circle distance from Lindley Hall to Lindley Hall, London is displayed for you. imcenter\nFinally, convert you answer from km to miles (a simple Google search will accomplish this, “### km to miles”). Answer Question 7. Question 7 According to this website measurement, what is the computed surface distance (in miles) between the Lindley Hall and Lindley Hall, London? Your answers to Questions 5 and 7 should be relatively similar, given how they were both computed; if they’re way off, redo your Question 5 measurement and make sure you’re inputting the correct numbers for Question 7.\nUsing UTM Coordinates and Measurements in Google Earth Pro Universal Transverse Mercator (UTM) is a projected coordinate system, and GEP can track coordinates using this UTM system as well as lat/long. From Tools on the menu bar, select Options. In the Show Lat/Long options, select the radio button for Universal Transverse Mercator, click Apply and then OK. UTM coordinates are measured in meters rather than degrees of latitude or longitude, which enables an easier method of determining coordinates. Moving the mouse east increases your easting, and moving it west decreases the easting. The same holds true for northing: Moving the mouse north increases the value of northing, and moving it south decreases the northing value.\nZoom out to see the whole world. You’ll also see that the graticule has changed from a web of latitude/longitude lines to the UTM grid draped over Earth. You should see the UTM zones laid out across the world, numbered from 1 through 60. Answer Question 8. Question 8 In what UTM zone is Lindley Hall, London located? In what UTM zone is Lindley Hall located? Double-click on the Lindley Hall placemark(in the Places box), and GEP rotates and zooms back to the Lindley Hall. Scroll the mouse around the area. You’ll see new coordinates appear at the bottom of the view, but this time they are the zone, easting, and northing measurements. Open the Properties of The Lindley Hall placemark by right-clicking on it. The UTM easting, northing, and zone appear. Using these methods, answer questions 9 and 10. Question 9 What are the UTM coordinates of the Lindley Hall? Question 10 What are the UTM coordinates of the Thai Diner? Change the view so that you can see both Lindley Hall and Thai Diner. Using your answers for Questions 9 and 10 (and perhaps the Ruler tool or some basic arithmetic/trigonometry), answer Question 11. Question 11 How many meters away to the north and east is Lindley Hall from Thai Diner? UTM can also be used to easily determine the locations of other objects. Return to the Lindley Hall placemark again. Answer Questions 12 and 13. Question 12 What object is located approximately 66.69 meters north and 692 meters east of Lindley Hall? Question 13 What are the full UTM coordinates of this object? Wrapping up There is no need to save anything from this lab, so when done you can simply close without saving. Submit your answers to the questions on blackboard.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 03"
    ]
  },
  {
    "objectID": "GEOINT_lab01.html",
    "href": "GEOINT_lab01.html",
    "title": "GEOINT Lab 01",
    "section": "",
    "text": "This lab is not a beginner level “how to computer”, but we will cover a few critical PC setup tips that will only make your life easier, as well as a few lesser known tips and tricks that I’ve picked up and found useful.\n\n\n\nThere is nothing to submit for this week, but make sure you’ve followed along and set up your system correctly or you’ll have a rough time as we move through the semester.\n\n\n\n\n\n\nNote\n\n\n\nThis lab is almost exclusively designed for Windows although there are certainly these same settings in Apple and Linux systems. Some of these steps will also be specific to the lab PC’s only.\n\n\n\n\n\n\n\nWhen working on a computer, it is vital that you know “where” on the hard drive you are working. This “where” is the folder path. For instance, when you open up a new word document and attempt to save it, it will likely kick you to a save prompt that has some sort of file path displayed, ex:\n\nIf you click More Save Options, you should end up at the Windows File Explorer view, and at the top in the address bar or over on the navigation pane, you’ll see a full path and visial hierarchy of files.\n\nMake sure you know where that is, both as a folder path and what drive you are on. Sometimes I may tell you that you need to be working “one level up”, which just means that you need to be back on folder in your folder tree.\n\n\n\n\nOneDriveDropbox\n\n\nOneDrive could have been great but poor implementation on the IT end means we are often saddled with a three legged work horse. While some fields may never encounter the issues we are about to uncover, Geographic Information Science & Systems are a branch of data science and that means we have some pretty intense formats to work with. When you first log on to a PC, the first thing you should check is to see where the OneDrive folder is attempting to sync to, and how healthy that drive is. The easiest way to get this info at a glance is to open the File Explorer (usually the pinned folder icon on the task bar), and then click on This PC to see all the drives attached and their space available. If you right click on the OneDrive icon and go to properties, you’ll see where it lives (in my case, it’s on the H:)\n\nIf you are working on the Lindley Hall PC’s, they are set up so that each user has the default OneDrive location on the C:, and these fill up fast. If you find yourself short on space, you’ll want to move this to the D: like so:\n\nClick on the OneDrive icon in the taskbar\nSelect Help & Settings &gt; Settings\nIn the Account tab, select Unlink this PC, and when the OneDrive setup screen appears, you can close it.\nYou can now either move or delete that original OneDrive folder (If you’ve just started out, the easiest thing to do is just delete it). The safest and most logical place to put the new folder is in your user profile on the D:\n\n\nex: D:/Users/A123B456/OneDrive\n\n\nOn the start bar search for OneDrive and open the app.\nOn the OneDrive setup screen, select Get started, and then follow the instructions until you get to the screen where you can change your OneDrive folder’s location. Choose the new folder location, and then select OK.\n\n\n\nWait for your files to sync and you should be all set\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are using OneDrive though a company or though KU, you generally don’t get a whole lot of control over the name of the folder you choose. To make transferring files easer down the line, the first folder in my setup is always called “Root”, and everything lives under that. If you do have control over what your OneDrive folder is called, make sure it is short and ensure it does not have spaces or odd characters to save you headache down the line as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are a few system settings that you should always have shown in windows. If you search “Show File Extensions” in the windows search bar, the Settings For developers tab should pop up. Click on the settings and set a few of these options. The key one we are looking for here is to make sure Hide Extensions for known file types is unchecked. You always want to know what it is you are clicking on. Other handy ones I use are the Display the full path in the title bar and the Show hidden files options.\n\n\n\n\nMost screenshots look terribly unprofessional when simply pasted into a document, but they do convey the needed information with virtually no friction and are therefor pretty commonplace. For those times when creating a real image is more effort than it’s worth, the Snip tool (soon to be the snip and sketch tool) is your friend. Hit the windows search bar and type in snip to get the snip window to pop up. There are a few ways to snip including the option to add a delay if you need to navigate hidden menus before the screenshot is taken.\n\n\n\nUsing GIS more than once a week will greatly increase your familiarity and learning. You don’t need to run out and buy a new computer if you don’t want to though. If you are a KU student, you have a few means of remote access to compute resources. There is a large cluster of web based applications for use at http://virtualdesktop.ku.edu/, and you can log into the PC’s in Lindley using http://virtuallab.ku.edu/. I recommend the latter (virtual labs), as the hardware is a little more robust than the virtual desktop options. The first time you attempt to log on it will walk you through installing Citrix receiver, but you do not need to create an account or otherwise log into anything other than the KU logins.\n\n\n\n\nIf you have a PC capable of running GIS, there are instructions on how to go about installing it here.\n\n\n\n\nCopy - paste operations can get pretty heavy handed with how they handle formatting. If you right click on any of the Windows office products, there are generally options such as Paste with format, paste and merge format, paste as values, ect, that are more behaved than the standard ctrl-v option.\nPowerPoint is a pretty magical program when you drill into all the capabilities it offers, but one of the neater and more accessible functions is the ability to quickly remove a solid background color from a picture. Just add the image to a black slide, click on the Picture Format toolbar and then the Color &gt; Set Transparent Color tool and then click on the color you want to remove the background from.\n\n\n\nFind yourself working with tabular text data? A handy keyboard shortcut to highlight along a specific set of character columns is to alt-click (works in notepad++, word). The example below shows how this might be useful if you wanted to quickly remove the first two digits off the year column.\n\n\n\n\n\nIf you are on a PC in Lindley, make sure you save all your work and then log off. As the submission requirements outlines, there was nothing to submit for this lab, but make sure you do these steps. Note that although these settings should follow you should you move from PC to PC, take a quick second each time you log in to make sure that it’s set up as you expect (the OneDrive on the D: is the biggie)",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 01"
    ]
  },
  {
    "objectID": "GEOINT_lab01.html#learning-objective",
    "href": "GEOINT_lab01.html#learning-objective",
    "title": "GEOINT Lab 01",
    "section": "",
    "text": "This lab is not a beginner level “how to computer”, but we will cover a few critical PC setup tips that will only make your life easier, as well as a few lesser known tips and tricks that I’ve picked up and found useful.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 01"
    ]
  },
  {
    "objectID": "GEOINT_lab01.html#submission-requirements",
    "href": "GEOINT_lab01.html#submission-requirements",
    "title": "GEOINT Lab 01",
    "section": "",
    "text": "There is nothing to submit for this week, but make sure you’ve followed along and set up your system correctly or you’ll have a rough time as we move through the semester.\n\n\n\n\n\n\nNote\n\n\n\nThis lab is almost exclusively designed for Windows although there are certainly these same settings in Apple and Linux systems. Some of these steps will also be specific to the lab PC’s only.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 01"
    ]
  },
  {
    "objectID": "GEOINT_lab01.html#tutorial",
    "href": "GEOINT_lab01.html#tutorial",
    "title": "GEOINT Lab 01",
    "section": "",
    "text": "When working on a computer, it is vital that you know “where” on the hard drive you are working. This “where” is the folder path. For instance, when you open up a new word document and attempt to save it, it will likely kick you to a save prompt that has some sort of file path displayed, ex:\n\nIf you click More Save Options, you should end up at the Windows File Explorer view, and at the top in the address bar or over on the navigation pane, you’ll see a full path and visial hierarchy of files.\n\nMake sure you know where that is, both as a folder path and what drive you are on. Sometimes I may tell you that you need to be working “one level up”, which just means that you need to be back on folder in your folder tree.\n\n\n\n\nOneDriveDropbox\n\n\nOneDrive could have been great but poor implementation on the IT end means we are often saddled with a three legged work horse. While some fields may never encounter the issues we are about to uncover, Geographic Information Science & Systems are a branch of data science and that means we have some pretty intense formats to work with. When you first log on to a PC, the first thing you should check is to see where the OneDrive folder is attempting to sync to, and how healthy that drive is. The easiest way to get this info at a glance is to open the File Explorer (usually the pinned folder icon on the task bar), and then click on This PC to see all the drives attached and their space available. If you right click on the OneDrive icon and go to properties, you’ll see where it lives (in my case, it’s on the H:)\n\nIf you are working on the Lindley Hall PC’s, they are set up so that each user has the default OneDrive location on the C:, and these fill up fast. If you find yourself short on space, you’ll want to move this to the D: like so:\n\nClick on the OneDrive icon in the taskbar\nSelect Help & Settings &gt; Settings\nIn the Account tab, select Unlink this PC, and when the OneDrive setup screen appears, you can close it.\nYou can now either move or delete that original OneDrive folder (If you’ve just started out, the easiest thing to do is just delete it). The safest and most logical place to put the new folder is in your user profile on the D:\n\n\nex: D:/Users/A123B456/OneDrive\n\n\nOn the start bar search for OneDrive and open the app.\nOn the OneDrive setup screen, select Get started, and then follow the instructions until you get to the screen where you can change your OneDrive folder’s location. Choose the new folder location, and then select OK.\n\n\n\nWait for your files to sync and you should be all set\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are using OneDrive though a company or though KU, you generally don’t get a whole lot of control over the name of the folder you choose. To make transferring files easer down the line, the first folder in my setup is always called “Root”, and everything lives under that. If you do have control over what your OneDrive folder is called, make sure it is short and ensure it does not have spaces or odd characters to save you headache down the line as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are a few system settings that you should always have shown in windows. If you search “Show File Extensions” in the windows search bar, the Settings For developers tab should pop up. Click on the settings and set a few of these options. The key one we are looking for here is to make sure Hide Extensions for known file types is unchecked. You always want to know what it is you are clicking on. Other handy ones I use are the Display the full path in the title bar and the Show hidden files options.\n\n\n\n\nMost screenshots look terribly unprofessional when simply pasted into a document, but they do convey the needed information with virtually no friction and are therefor pretty commonplace. For those times when creating a real image is more effort than it’s worth, the Snip tool (soon to be the snip and sketch tool) is your friend. Hit the windows search bar and type in snip to get the snip window to pop up. There are a few ways to snip including the option to add a delay if you need to navigate hidden menus before the screenshot is taken.\n\n\n\nUsing GIS more than once a week will greatly increase your familiarity and learning. You don’t need to run out and buy a new computer if you don’t want to though. If you are a KU student, you have a few means of remote access to compute resources. There is a large cluster of web based applications for use at http://virtualdesktop.ku.edu/, and you can log into the PC’s in Lindley using http://virtuallab.ku.edu/. I recommend the latter (virtual labs), as the hardware is a little more robust than the virtual desktop options. The first time you attempt to log on it will walk you through installing Citrix receiver, but you do not need to create an account or otherwise log into anything other than the KU logins.\n\n\n\n\nIf you have a PC capable of running GIS, there are instructions on how to go about installing it here.\n\n\n\n\nCopy - paste operations can get pretty heavy handed with how they handle formatting. If you right click on any of the Windows office products, there are generally options such as Paste with format, paste and merge format, paste as values, ect, that are more behaved than the standard ctrl-v option.\nPowerPoint is a pretty magical program when you drill into all the capabilities it offers, but one of the neater and more accessible functions is the ability to quickly remove a solid background color from a picture. Just add the image to a black slide, click on the Picture Format toolbar and then the Color &gt; Set Transparent Color tool and then click on the color you want to remove the background from.\n\n\n\nFind yourself working with tabular text data? A handy keyboard shortcut to highlight along a specific set of character columns is to alt-click (works in notepad++, word). The example below shows how this might be useful if you wanted to quickly remove the first two digits off the year column.\n\n\n\n\n\nIf you are on a PC in Lindley, make sure you save all your work and then log off. As the submission requirements outlines, there was nothing to submit for this lab, but make sure you do these steps. Note that although these settings should follow you should you move from PC to PC, take a quick second each time you log in to make sure that it’s set up as you expect (the OneDrive on the D: is the biggie)",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 01"
    ]
  },
  {
    "objectID": "GC.html#how-to",
    "href": "GC.html#how-to",
    "title": "Geocoding",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Analysis",
      "Network",
      "Geocoding"
    ]
  },
  {
    "objectID": "FlatGrillDeck.html#the-grill",
    "href": "FlatGrillDeck.html#the-grill",
    "title": "So long as you get the GIS&T of it",
    "section": "The Grill",
    "text": "The Grill\n\n\n\n\n\n\n\nGoals:\n\nSolve hydrology\nSee 2026\n\nOutcomes:\n\nA better understanding of some of the tools, semantics, and shenanigans I encountered last week.\nA clearer picture of what’s next.\n\nTopics:\n\nLWIL (Last Week I Learned) / Story Time\n\n\n\n\n\n\nFor: Group updates, humanity, and human welfare (homage to (Chow 1988)) Title slide background from: https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExazFtdGlvYnVmcHl5Z3FpZGJkYTZsMmw3eXJuZ3A3YnJta2UxbTN2diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/111RMoa4bwH9zq/giphy.gif Background images from:\n\n\nOWP presentation template background: ./_Water_color.png or ./_Water_bw.jpg - “cover”\n\n\nMy own efforts from Water PNGs by Vecteezy, https://www.vecteezy.com/members/ahasanaraakter: _vecteezy_ai-generated-water-wave-splash.png / _vecteezy_ai-generated-water-wave-splash_bw.png / _vecteezy_ai-generated-water-wave-splash_bw_blur.png - “contain”"
  },
  {
    "objectID": "FlatGrillDeck.html#presentation-navigation-tips",
    "href": "FlatGrillDeck.html#presentation-navigation-tips",
    "title": "So long as you get the GIS&T of it",
    "section": "Presentation Navigation Tips",
    "text": "Presentation Navigation Tips\n\n\n\nSlide layout : This deck has one axis, use any key to advance.\n: This deck has interactive elements\n\nItems linked/bordered in green are cited in the tooltip on hover.\nItems linked/bordered in blue are hyperlinked to relevant resources.\n\n Photos are Allowed |  Questions are Encouraged\n: ~5 minutes | Last updated:08/25/2025 20:44:43\n! PLEASE !\nInterrupt me and ask questions or clarifications.\nI’m here to talk with you now, not to these slides.\n\n\nControl tips\n\nMy preferred (FOSS) flavor of slidedecks, revealjs, has intuitive but none the less unconventional PowerPoint presentation controls:\n\nSlides dynamically resize to use the entirety of the browser window, but you can still fullscreen with F.\n\nThis slide has a red border indicating the content extent.\n\nSlide navigation is mode dependent. If there are vertical slides, press space, N, or the down arrow key, not the right arrow to advance slides\nPress M to open to the menu, Press O for the slide deck overview, Press B to black out the presentation screen, Press S for a speaker view.\nYou can use the chalkboard to freemouse/touchpad draw.\nSlides should render as designed1 but you can press Alt/Opt + click on the slide to zoom in. Increase text size with Alt/Opt + +, Alt/Opt + - to decrease, and Alt/Opt + 0 reset to the default scale.\nPress C to declare victory and head home.2\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\n\n\n\n\nSpeaker notes\n\ntested in an updated version of google chromeA favored quote from one of my giants: Dr. David Maidment"
  },
  {
    "objectID": "FlatGrillDeck.html#project-status-and-updates",
    "href": "FlatGrillDeck.html#project-status-and-updates",
    "title": "So long as you get the GIS&T of it",
    "section": "Project Status and Updates",
    "text": "Project Status and Updates\n\n\n\n\n\n\n\n\n\n\n%%---\n%%displayMode: compact | The 7 P's of Project Management (needs a refresh) | Yesterdays weather | I have the estimation prowess of George RR Martin | This week's timeline by Captian Hindsight\n%%---\n%% [link](https://mermaid.live/edit#pako:eNp9Ve9P2zAQ_VdOnpBASqukbWiTb4jChkY31KJJTPmAiS-t1dTOHAfoEP_7Lj9IU1bwp8Tn9-7d-eXywmItkIVsyZW1kQJaglu81GbDLcAdrd5s1ptO65iVNkWo1x3mFo3g2xyekNsVmvoMPsdpITAvn58Q16hEXkeOjuD4_i18DzyOMbM55BnGMpFxlTkHqTppIamUOFDl0QlQnooVjiOWF4q2I3YC2kDE2mTMgYfCgtK2Pq6NaMIlS8T6J60eq2lrxs0aDeTW6DX2nqSwq9DPnp16I_ziJq6jMx5Luw3dvh-pGp5jbKVW8PXsx-0tnK-4sfAdt3XwXG-yFC0KsDxfw7sVCq3QaV6oGZ4DA3fg99xxb-A64LniPUtspJUxT7t0YbnpNFTEMnCAJ3QnDWPLckY6H_GgkJKFV2HnTcuwwzLosFwWtjAfs3RfCDnqsAw7LIfq6LBUFe1Y_A7LqGHZb_6tzmTc-OvXsO_Cx6sttG111fZG2Hx-tlhcX8zhZn4Q3LS5xVJrRg302-VwCoWVKRnkY2g3q98gb35egcFEKtygIrdivFLyT1F_PP8jqQWDBjmVj1IggWOtYsrMq258UuukWyuFyO-pzhAynRXpPvogeOeDqxlkMsOUVFP-vDCmuYtD4EnP7aheVCkTGhellTKdEgn_BOt5uy7XWKHjouzVu3oJ67Qgco23B7LcLJEmDVcCeGH15g0dcq9yWwud7O6mhubkVamWh9xUGZN7H8D3bTpFLsqG1RouHqmCxrTX_IGm15RvPzFuzR_03PKrFsxhG6SpKAUN7peSJWI05zYYsZAeBSa8SG3EIvVKR8t6F1sVs9CaAh1WZOWYnUq-NHzDwoSnOe1mXLHwhT2zsDfqj3x_OPTc0_HYDYLAd9iWhd5k3Pf9seeNxn4wCfzB6avD_mpNFG4_mJz6Hp0duL7rEchhKKTVZlb_XKp_TJXjdwWohRhdLFetgKUpy2leDI1xNOe6UJaF_iR4_QehLNnu)\n%% [Mermaid](https://mermaid.live/edit)\ngantt\n    dateFormat  YYYY-MM-DD\n    title       I have the estimation prowess of George RR Martin\n    excludes    weekends\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    %% todayMarker stroke-width:5px,stroke:#0f0,opacity:0.5\n\n    section GANTT Chart Key\n    Completed task                :done,       des1, 2025-07-20, 10d\n    Completed critical task       :crit,done,  des2, after des1, 10d\n    Active task                   :active,     des3, after des2, 10d\n    Future task                   :            des4, after des3, 10d\n    critical task                 :crit,       des5, after des4, 10d\n\n    section Topics\n    V3.0                           :active,crit, 2025-07-21, 35d\n    River slope population         :done, 2025-07-28, 10d\n    FIM pipeline resurrection      :done, 2025-08-04, 2d\n    Slope feature pollination      :done, 2025-08-11, 4d\n    Sword intigration prototype    :done, 2025-08-14, 2d\n    Slope STATISTICS and DOCUMENTATION            :active,2025-08-21, 4d\n    Slope targets and functionalization   :done,    2025-08-18, 3d\n    STAC surface minting           :active,   2025-08-15, 11d\n    Profile plotting and metrics   :active,   2025-08-25, 5d\n    \n    section Deadlines and Events\n    Labor Day                      :2025-09-01, 1d\n\n\n\n\n\n\n\n\nRisks:\n\n\n\n3: Not enough time in the day.\n\n\n4: Lack of ADCP.\n\n\n5: Unable to afford to live in a van down by the river\n\n\n6: (re 3) Rendering is delayed: solution - do it Friday\n\n\n7: (pre 5) Coffee prices: solution - \n\n\n\n\n\n← likelihood →\n\n\n\n\n\n\n\n\n\n\n\n5\n\n\n3\n\n\nProgress:\n\n\n\nMetric scoping\n\n\nDomain selection\n\n\nDatabase draft\n\n\nDid NOT send sensitive information over group chat\n\n\nCalling it: Coheed covers christmas\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n\n\n\n\n\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n← consequence →\n\n\n\nLast updated: 08/25/2025(A Tentative WIP)\n\n\n\n\nLook at me, I’m project management now! !IMPORTANT!: You should at least double every estimate I give you and nothing is ever done  Risk 1 (The general lack of forethought or concern from empowered blowholes) has evolved into risk 5 5: Retirement - I wasn’t going to make it that far anyways. - removed, risk was realized and no one likes risk"
  },
  {
    "objectID": "FlatGrillDeck.html#slope-tests-domain-context",
    "href": "FlatGrillDeck.html#slope-tests-domain-context",
    "title": "So long as you get the GIS&T of it",
    "section": "Slope tests: domain context",
    "text": "Slope tests: domain context\n\n\ntodo\n\n\n\n\n\n\n\n\n\nI clearly have no idea what I’m doing…"
  },
  {
    "objectID": "FlatGrillDeck.html#slope-methods-todo",
    "href": "FlatGrillDeck.html#slope-methods-todo",
    "title": "So long as you get the GIS&T of it",
    "section": "Slope methods: todo",
    "text": "Slope methods: todo\n\n\nI clearly have no idea what I’m doing…"
  },
  {
    "objectID": "FlatGrillDeck.html#slope-results-model-slope",
    "href": "FlatGrillDeck.html#slope-results-model-slope",
    "title": "So long as you get the GIS&T of it",
    "section": "Slope results: model slope",
    "text": "Slope results: model slope\n\n\n\n\nData Table"
  },
  {
    "objectID": "FlatGrillDeck.html#slope-results-sword-slopes",
    "href": "FlatGrillDeck.html#slope-results-sword-slopes",
    "title": "So long as you get the GIS&T of it",
    "section": "Slope results: SWORD slopes",
    "text": "Slope results: SWORD slopes\n\n\n\n\n\n\n\n\n\nData Table\n\n\n\n\n\n\nRRASSLER_key\npdsd\nmodel_area\nPermanent_Identifier\nFDate\nResolution\nGNIS_ID\nGNIS_Name\nLengthKM\nReachCode.x\nFlowDir\nWBArea_Permanent_Identifier\nFType\nFCode\nMainPath\nInNetwork\nVisibilityFilter\nShape_Length\nNHDPlusID\nEnabled\nStreamLeve\nStreamOrde\nStreamCalc\nFromNode\nToNode\nHydroSeq\nLevelPathI\nPathLength\nTerminalPa\nArbolateSu\nDivergence\nStartFlag\nTerminalFl\nUpLevelPat\nUpHydroSeq\nDnLevel\nDnLevelPat\nDnHydroSeq\nDnMinorHyd\nDnDrainCou\nFromMeas\nToMeas\nReachCode.y\nRtnDiv\nThinner\nVPUIn\nVPUOut\nAreaSqKm\nTotDASqKm\nDivDASqKm\nMaxElevRaw\nMinElevRaw\nMaxElevSmo\nMinElevSmo\nSlope\nSlopeLenKm\nElevFixed\nHWType\nHWNodeSqKm\nStatusFlag\nx\ny\nreach_len\nn_nodes\nwse\nwse_var\nwidth\nwidth_var\nfacc\nn_chan_max\nn_chan_mod\nobstr_type\ngrod_id\nhfalls_id\nslope\ndist_out\nlakeflag\nmax_width\nn_rch_up\nn_rch_dn\nrch_id_up\nrch_id_dn\nswot_orbit\nswot_obs\ntype\nriver_name\nedit_flag\ntrib_flag\npath_freq\npath_order\npath_segs\nmain_side\nstrm_order\nend_reach\nnetwork\ngeometry\n\n\n\n\n243\n1115819_2360_ras_g01_1712630157\n1115819\n2398746646\n104744344\n2014-06-06T00:00:00\n2\n01375103\nNeches River\n1.69464\n12020003008679\n1\n120028141\n558\n55800\n0\n1\n0\n0.0170621\n3.00008e+13\n1\n4\n8\n8\n3.00008e+13\n3.00008e+13\n3.00008e+13\n3.00008e+13\n186.7668\n3.00008e+13\n34124.71\n1\n0\n0\n3.00008e+13\n3.00008e+13\n4\n3.00008e+13\n3.00008e+13\n0\n1\n0\n100\n12020003008679\n0\nNA\n0\n0\n0.5837\n18828.37\n18823.73\n-9998\n2508\n2530\n2530\n1e-05\n1.69464\n0\nNA\nNA\nA\n-94.40931\n31.02562\n11542.81\n58\n27.75\n1.137485\n42\n141.1555\n9544.698\n1\n1\n0\n0\n0\n0.2835481\n279073.9\n0\n330\n1\n1\n75120400446\n75120400421\n\n0\n1\nNeches River\nNaN\n0\n1\n136\n178\n0\n1\n0\n8\nLINESTRING (-95.74099 32.50..."
  },
  {
    "objectID": "FlatGrillDeck.html#slope-results-statistics",
    "href": "FlatGrillDeck.html#slope-results-statistics",
    "title": "So long as you get the GIS&T of it",
    "section": "Slope results: statistics",
    "text": "Slope results: statistics\n\n\n\n\n\n\n\n\n\n\nI clearly have no idea what I’m doing…"
  },
  {
    "objectID": "FlatGrillDeck.html#next-steps",
    "href": "FlatGrillDeck.html#next-steps",
    "title": "So long as you get the GIS&T of it",
    "section": "Next Steps",
    "text": "Next Steps\nEarly Findings\n\nSWORD is too sparse for our application scale\n\nbut could make for excellent POI’s!\nEarly draft here: link_todo\n\nSpatial conflation ; Network conflation \nEmphasis on the full representation of the ‘vector’ of the measurement\n\nslope is not “grade”, but is conceptually conflated too frequently\n\n\nNext Tasks\n\nMore polish and reporting\nmulti-surface slope continuity tests\nV3 creation\nFigure out how to make a better map (reveal fights with everything, leafpop is breaking things)…\nUpdate RRASSLER with sample data from M3\n\n\n\nI like going outside"
  },
  {
    "objectID": "FlatGrillDeck.html#a-statement",
    "href": "FlatGrillDeck.html#a-statement",
    "title": "So long as you get the GIS&T of it",
    "section": "A Statement",
    "text": "A Statement\nI believe a competent modeler can use variations in modeling surface and the uncertainty in parameterizations to reach a suitable equifinality prediction.\nTherefore, I find it very hard to find something useful to say in the face of competing objectives and value judgments, and a shameful lack of observation and geographic context.\n\n\nI used to go outside…"
  },
  {
    "objectID": "FlatGrillDeck.html#a-contribution",
    "href": "FlatGrillDeck.html#a-contribution",
    "title": "So long as you get the GIS&T of it",
    "section": "A Contribution",
    "text": "A Contribution\nMy contribution to the science is a public, reproducible demonstration of technical and methodological excellence (ha). In order to communicate the nuances surroundings the quantification of accuracy and equifinality of FIM, I introduce concepts including mapping and modeling surfaces, and library creation and access patterns.\n\n\n\n\nI used to go outside…"
  },
  {
    "objectID": "FlatGrillDeck.html#making-sense-around-wickedly-fun-problems",
    "href": "FlatGrillDeck.html#making-sense-around-wickedly-fun-problems",
    "title": "So long as you get the GIS&T of it",
    "section": "Making Sense Around Wicked(ly fun!) Problems",
    "text": "Making Sense Around Wicked(ly fun!) Problems\n\n\n\n\n\n\n\nmindmap\n  Wicked Problem\n    Every problem is unique\n      E.G.: Water is not the same as a biological outbreak\n    Every problem is connected to others\n    E.G.: FEWS\n    There is no clear problem definition\n    Are multi-causal, multi-scaler, and interconnected.\n    Include multiple, invested stakeholders with different values, goals, and objectives.\n    Straddle organizational and disciplinary boundaries\n    Solutions to an aspect have implications/ramifications across the system\n    Solutions are not right or wrong, but good and bad\n    Can be difficult to measure or evaluate effects of implemented solutions\n    Problems are never completely solved\n      They are wicked problems, not wicked puzzles\n\n\n\n\n\n\n\nI demonstrate that these are critical factors in the execution of FIModeling, and that with this new framing we can try and make a little more sense around our wicked problem.\n\n\nI used to go outside…"
  },
  {
    "objectID": "FlatGrillDeck.html#interested-in-learning-more",
    "href": "FlatGrillDeck.html#interested-in-learning-more",
    "title": "So long as you get the GIS&T of it",
    "section": "Interested in learning more?",
    "text": "Interested in learning more?\n\nIf the contents of these pages somehow resonate with you, you’re exactly who I want to talk to! Please reach out and don’t be afraid to bump your email if I don’t respond in a timely fashion. I am oversaturated, keep track of too many inboxes, and appreciate the reminder.\n\n\nThank you for entertaining my advanced idiocy. Now I should get back to work [[20240325082848]] _creating knowledge\n\n\n\n\n\nChow, Ven Te. 1988. Open-Channel Hydraulics. Reissued. McGraw-Hill Classic Textbook Reissue Series. Boston, Mass.: McGraw-Hill."
  },
  {
    "objectID": "FIM_primer.html",
    "href": "FIM_primer.html",
    "title": "FIM with Jim",
    "section": "",
    "text": "Flood Inundation Mapping, or FIM, is the act of mapping what area will be under water for a specified set of conditions. I’ve been fortunate enough to learn from and help River Forecast Centers and the Office of Water Prediction as we undertake the monumental task of generating and operationalizing CONUS scale FIM, pushing the boundaries of Big Data and tackling one of our most pressing wicked problems.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM"
    ]
  },
  {
    "objectID": "FIM_primer.html#abstract",
    "href": "FIM_primer.html#abstract",
    "title": "FIM with Jim",
    "section": "Abstract",
    "text": "Abstract\nThe computational efficiency of geospatial flood modeling, as opposed to those that solve shallow water flow equations, lend these approaches to real time regional and national scaling. However, the validation of these models remains sparse across both space and time, and no consistent validation metric has been used to quantifiably compare model skill. Furthermore, the pathways from streamflow forecast to flood inundation mapping are an incredibly active area of research. This work provides a high-level overview of several open-source geospatial and low complexity flood inundation mapping techniques, their strengths, costs, and tradeoffs, and the hurdles experienced while performing these tasks.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM"
    ]
  },
  {
    "objectID": "FIM_primer.html#background",
    "href": "FIM_primer.html#background",
    "title": "FIM with Jim",
    "section": "Background",
    "text": "Background\nFlood inundation mapping, or the act of delineating the area under water, is a complex topic that probes deep gaps in our collective knowledge of physical laws governing reality, wrestles us with the technical friction of implementing that theory into operations, and antagonizes us with the with the need for decisive action in situations where every (in)decision has some degree of negative consequence to it. Of these pain points, the technical friction a user experiences when generating a flood inundation map, has recently received a near exponential level-up in difficulty over the last decade with the small renaissance of geospatial flood modeling methods. Geospatial Flood Modeling, as opposed to hydraulic flood modeling, utilizes local, focal, and zonal operations commonly introduced in map algebra curriculum, whereas hydraulic flood modeling typically solves some form of the shallow water flow equations in open channels and can be found in many engineering programs. Although once reserved for large scale analysis, with the trailblazer of the NWM - HAND - Synthetic rating curve demonstrations from the NOAA Office of Water Prediction lead to a dramatic increase in research efforts which formalized and published a number of methods to map a flood, These methods may occasionally emerge from the same organization, but more commonly are created for slightly different use cases, use slightly different terminology, take different pathways towards technical realization, and perhaps most frustratingly, all deploy their drivers in different forms. It is this last point in particular, that each method currently requires it’s own dedicated input stream, that makes intercomparisions between models particularly challenging. To overcome that friction, we propose a framework for interoperable flood mapping efforts which unify these into a set of common input choices which can be easily parsed and passed between methods.\nThere’s a lot of requisite background knowledge, friction, and occasionally conflicting trade-offs that need to be made between FIM methodology, FIM creation, and FIM operations. FrankenFIM is that bridge (hopefully). To talk about FrankenFIM though, it’s useful to first set the stage for what FIM is and how we use them before we start our interrogation. The first step in that will be to set up some core terminology, most from the field and some from my own making.\nAs noted, there are a wide variety of FIM libraries and flood modeling algorithms and their origins and linage are not particularly clear. While not meaningful in terms of implementation or prediction skill, it can be useful to compare and contrast development directions and so to place a few of the key methods I deploy into context, I’ll take a step back and describe the history and linage of a few of the more popular tools and libraries we use to map flooding.\n\nHistory of FIM\n\nI’m a poor excuse for a historian, but if I timeline out the different programs and techniques that have been used across the FIM domain, it would look something like this.\nSince 2015, efforts towards producing real-time and future inundation forecasts for the US have resulted in the compilation of a 10 m resolution Height Above Nearest Drainage (HAND) layer for the continental United States (CONUS; Liu et al., 2018). The methodology for coupling streamflow predictions from the National Water Model (NWM) and HAND was initiated as part of the National Flood Interoperability Experiment (NFIE) (Maidment (2017)). The methodology has since been enhanced and added to the National Water Center (NWC) US flood forecasting framework (NOAA National Water Center, 2018).\nThe first iteration of geospatial flood mapping techniques arouse as part of the National Flood Interoperablilty Experiment hosted in the National Water centers summer Institute This iteration represented the first time the HAND/SRC was Run Country scale and operational forecasts were being released for every attachment represented in the network from there the inundation library generation software was written for every and then returned to open source software methods and what is publicly available now as Cahaba known internally as FIM3.\n\n\nWhat is FIM\nFIM, most commonly used as the acronym for Flood Inundation Mapping, is used to refer to activities which comprise the creation of a map which displays the presence or absence of water on the land surface. This is usually represented as a raster of binary values encoding the presence or absence of water, rasters of depth values, or as polygonized representations of contours of those surfaces. One could certainly make the argument that it is not Flood Inundation Mapping but Flood Impact Mapping that is the true end goal, but we’ll avoid that scope creep for now in favor of another, that FIM ought to stand for Flood Inundation Modeling. Many efforts to tease apart the skill and contributions of the techniques end up conflating processes, drivers, and representations, making it impossible to reach a definitive conclusion about what is and is not a “better” approach. Those efforts are further stymied by the different considerations and concerns of the validation metrics and processes deployed that define what that “better” actually is. So, in order to carry out a validation of FIM (A “model bakeoff” in cruder terms), one must construct the FIM libraries using the same model inputs. But what is a FIM Library?\n\n\nWhat is a library\nI’ve adopted a functional definition for the term FIM Library, which is used to denote a given database. Each of these databases are composed are parsed out in their own particular way, which I refer to as volumes. For example, the CFIM library has volumes which are distributed in a huc6 encoding pattern. The RAS2FIM tool creates libraries whose volumes are distributed in a huc8 and crs pattern. FLDPLN libraries are composed of volumes called segments. These somewhat arbitrary definitions and terms might at first seem a little unwieldy, but are absolutely critical in order to be able to communicate how we are able to make these methods mesh seamlessly and communicate how the different methods perform.\n\n\nAside: cleaning up terms\nThese terms often crop up when talking about “libraries”, and while I view #### Precanned? While it’s use is #### CATFIM #### In a box?\n\n\nFIM Design Considerations\nThere are a number of design considerations that one should take into account, and these considerations extend beyond the creation of FIM data to questions like scale and cadence of output generation, fault tolerance, and personally identifiable information (PII) and data security concerns. These considerations need to be addressed before you can start attempting to solve a problem",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM"
    ]
  },
  {
    "objectID": "FIM_primer.html#methods-overview",
    "href": "FIM_primer.html#methods-overview",
    "title": "FIM with Jim",
    "section": "Methods Overview",
    "text": "Methods Overview\nThere are far too many models and frameworks to enumerate and compare, but to set the stage for the intercompairison, it’s useful to distinguish the different ways a library is constructed.\n\nDEM Subtraction\nOne of the simplest forms of geospatial FIM is a simplistic cost distance fill, often referred to as a Bathtub fill. These usually require an input DEM, and a gridded set of source cells which serve as the stage of that initial distance operation. Any cell with a value lower than the stage is considered inundated, and anything that is not is considered safe.\n\n\nHAND\nHAND is a type of relative elevation model FIM libraries can come in many forms but the general consensus as it pertains to operational flood mapping is that a viable FIM library contains both the data nessassary to translate a discharge into a stage, and a way to place that stage on a map. These most often come in the form of singular rasters of depth assigned a stage, and tied to a river gage as is commonly found in the ahps fim libraries. More recently, that form was altered to accommodate the increased resolution of the river forecast system (In the form of the national water model) by disseminating the rating curves as tables, and geometries as a raster alongside associated depth grids.\n\nCFIM\nThe first? publicly available, NIFE consistent FIM dataset, this database uses the synthetic rating curve and HAND methodology to transform the output of the National Water Model Channel Routing Outputs to transform a discharge value into a stage and then subtract that stage from the Height Above Nearest Drainage.\n\n\n\n_c79e7106aded02b9872f976475e4fb1e.png\n\n\n\n\nFIM 3 FR/MS and FIM 4\nThe evolution of the CFIM workflow from a methodological standpoint, lead by NOAA, resulted in a suite of methodologies which by in large were show to improve the accuracy of the flood map. The first, FIM 3 Full Res (FR) is in essence the CFIM methodology. The second evolution of methodology, FIM 3 Main Stems (MS), addresses inaccuracies at boundaries by refactoring and aggregating the input stream network along a dominant pathway in the watershed. FIM 4, occasionally referenced as Generalized Main Stems (GMS) hallmark improvement was to generalize that refactoring to each branch of the input network. All of these methods share a similar access pattern as well, where the discharge is first converted to stage and then that stage value is used in the HAND subtraction.\n\n\n\n\nGeospatial FIM\n\nFLDPLN\nA per pixel representation of libraries which represent both backfill and spillover flooding using geospatial approximations.\n\n\n\nHydraulically derrived FIM\n\nUSGS and AHPS FIM\nThe United States Geologic Survey and the Advance Hydrologic Predition Station Flood Inundation Maps are a series of stage relative depth maps for each river gage that has a survey. These depth grids are model outputs from contractors who developed hydralic models and standardised the output. Users select the stage they wish to view at the gage, and the requisite map is accessed. The outputs of this database compose what is collouquially refered to as “the skittles” map.\nThere are two main sources of inundation maps at a federal level. The first is the USGS Flood Inundation Mapping Program (“Flood Inundation Mapping (FIM) Program  U.S. Geological Survey” n.d.). The second, The Advanced Hydrologic Prediction Service Flood Inundation Mapping services through the National Weather Service (“NOAA - National Weather Service - Water” n.d.). These two separate databases both reference the same standards document (“Guidelines for the Development of Advanced Hydrologic Prediction Service Flood Inundation Mapping” 2011), models are constructed following (A statement about the overlap between the two databases)\n\n\n\nRAS2FIM\nRAS2FIM automatically formats and runs compliant HEC-RAS 1D models using a densified series of boundary conditions to generate a series of depth grids similar to the AHPS maps while also extracting a representative rating curve for that model. Given the technologic and computational hurdles the software is working around, there are several pain points both before and after the actual execution of the “RAS2FIM” code that make it difficult to elegantly deploy beyond a single domain or for inputs which have not already been pre-canned. To address the first, we introduce RRASSLER, a HEC-RAS model catalog generation and extraction tool to wrestle model data into a standardized and spatially accessible format. To address the latter, RAS2REM was created, which reformats RAS2FIM outputs into a geospatially accessible flood libraries.\n\n\nSCHISM\nSCHISM (Semi-implicit Cross-scale Hydroscience Integrated System Model) is an unstructured mesh which forecasts a water surface elevation for a set of boundary conditions across large scales. Past iterations of the modal have nodes which can be closer than 5 meters or as far away as 2000 meters. The current iteration has more than 10 million nodes across the Atlantic and Gulf domains\n\n\n\nFLDPLN and Operational Flood Inundation Mapping in Kansas\nFlood inundation maps are needed for emergency response and planning. We present an operational flood inundation mapping (FIM) system which covers the eastern two-thirds of the State of Kansas which and provides both current and forecasted flood inundation depth maps using the National Weather Service (NWS) Advanced Hydrologic Prediction Service (AHPS) and USGS observed gage stages. The system is based on the low-complexity FLDPLN FIM model which considers both backfill and spillover mechanisms to and generates flood inundation relations between stream channel cells and flood plain cells. We present the assessment on the FLDPLN model and compare it with the Height Above Nearest Drainage (HAND) FIM model used for nation-wide operational flood inundation mapping at the Office of Water Prediction (OWP)National Water Center. The mapping component of the system organizes the flood inundation relations as tiles in space, and takes the advantage of parallel computing to speed up inundation mapping. In addition, the system provides an example of computing maps using spatial relationships, which is a new geospatial data analysis framework currently under development.\n\n\nHealing a map/model surface\nOne of the goals of a FIMap is to communicate the area inundated across the terrain. Depending on how familiar you are with reading a FIMap or you’re reading the map for a particular purpose, you may find yourself wondering if a particular asset is impacted by the flood. One of the most common ways we want to make a map more reflective of the conditions and uses is to place bridge representations back into the model surface in order to represent whether or not the bridge was expected to be impacted.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM"
    ]
  },
  {
    "objectID": "FIM4_workflow_slides.html#the-fim4-algorithm-via-codebase-decomposition",
    "href": "FIM4_workflow_slides.html#the-fim4-algorithm-via-codebase-decomposition",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM4 Algorithm (via Codebase Decomposition)",
    "text": "The FIM4 Algorithm (via Codebase Decomposition)\n```{{r, setup, warning = FALSE, echo = FALSE, results=‘hide’}} library(magrittr) library(rgl, quietly=T) knitr::knit_hooks$set(webgl = hook_webgl)\nunit_to_map &lt;- ‘01050002’\npath_to_output &lt;- file.path(“C:/Users/stick/root/Dropbox/root/projects/floodmapping/tools/FrankenFIM/FrankenFIM/vignettes/vis”,fsep=.Platform\\(file.sep)\npath_to_tmp_output &lt;- file.path(path_to_output,\"tmp\",fsep = .Platform\\)file.sep) path_to_maine_stock &lt;- file.path(“C:/Users/stick/root//Dropbox/root/projects/floodmapping/methods/OWP_FIM/outputs/FIM4_maine/”,fsep = .Platform$file.sep)\npath_to_fim4_unit_outputs &lt;- path_to_maine_stock\nall_hucs &lt;- sf::st_read(file.path(“C:/Users/stick/root//Dropbox/root/database/hosted/water/HUC8.fgb”,fsep = .Platform$file.sep))\nwbd8_clp_inputs &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,“wbd8_clp.gpkg”,fsep=.Platform\\(file.sep))\nlevelpaths &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"nwm_subset_streams_levelPaths.gpkg\",fsep = .Platform\\)file.sep)) nearby_hucs &lt;- sf::st_transform(all_hucs,sf::st_crs(wbd8_clp_inputs))[wbd8_clp_inputs %&gt;% sf::st_buffer(1000),]\ntest_pkg &lt;- file.path(“~/data/temp/test.gpkg”,fsep = .Platform$file.sep) hf_flowpaths &lt;- sf::read_sf(test_pkg, “flowpaths”) hf_divides &lt;- sf::read_sf(test_pkg, “divides”) hf_flowlines &lt;- sf::read_sf(test_pkg, “flowlines”) hf_network &lt;- sf::read_sf(test_pkg, “network”)\nhuc_flowpaths &lt;- hf_flowpaths[wbd8_clp_inputs[4,],] huc_divides &lt;- hf_divides[hf_divides\\(id %in% huc_flowpaths\\)divide_id,] huc_network &lt;- hf_network[hf_network\\(divide_id %in% huc_flowpaths\\)divide_id,]\nhuc_bbox &lt;- sf::st_bbox(sf::st_transform(huc_divides,sf::st_crs(‘EPSG:4326’))) %&gt;% unlist() %&gt;% unname()\nhf_bounds &lt;- sf::st_bbox(sf::st_transform(huc_divides,sf::st_crs(‘EPSG:4326’))) %&gt;% unlist() %&gt;% unname()\nbranch_polys &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,“branch_polygons.gpkg”,fsep = .Platform$file.sep)) branch_polys_selction &lt;- branch_polys[25,] #2,11, 19 aoi_bbox &lt;- sf::st_bbox(sf::st_transform(branch_polys_selction,sf::st_crs(‘EPSG:4326’))) %&gt;% unlist() %&gt;% unname()\n\n## Reminder: Last time {data-background-image=\"Water_bw.png\"}\n\n* Flowpath: The HY_Features (OGC-16-032r2) are defined such that we use the term to describe \"whatever exists as part of a network of waterbody and surface depressions and surfance channels\" and we represent as a linear centerline\n* Divide: Divides are the polygon partitoning of the landscape\n* Flowline: A flowline is a one-dimensional (linear) feature that represents a flowing body of water and is functional similar to a flowpath but does not realize the catchment concept.\n\n![](./vis/flowline_flowpath.png){fig-align=\"center\" height=\"80%\"}\n\n&gt; From: Hydrologic Modeling and River Corridor Applications of HY_Features Concepts. https://docs.ogc.org/per/22-040.html. and Figure 9 reproduced.\n\n## Hydrofabric for VPU 1\n\n```{r, VPU, warning = FALSE, echo = FALSE}\nif(!file.exists(file.path(path_to_output,glue::glue(\"VPU1_HF.png\"),fsep = .Platform$file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n  \n  hf_bounds &lt;- sf::st_bbox(sf::st_transform(hf_divides,sf::st_crs('EPSG:4326'))) %&gt;% unlist() %&gt;% unname()\n  hf_divide_values &lt;- unique(hf_divides$divide_id)\n  # hf_divide_pal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(hf_divide_values)),hf_divide_values,na.color = \"transparent\")\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(hf_divides,sf::st_crs('EPSG:4326')),stroke = TRUE, opacity = 0.8,fillOpacity = 0,fill = FALSE,weight = 1) %&gt;%\n    leafem::addFeatures(data = sf::st_transform(hf_flowpaths,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"blue\") %&gt;%\n    # leaflet::addLabelOnlyMarkers(data=sf::st_transform(forecast_basins %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n    #                              label = ~`ID`,\n    #                              labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n    #                              group=\"Index Labels\") %&gt;%\n    leaflet::fitBounds(hf_bounds[3],hf_bounds[2],hf_bounds[1],hf_bounds[4])\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"VPU1_HF.png\"),fsep = .Platform$file.sep))\n}"
  },
  {
    "objectID": "FIM4_workflow_slides.html#focus-in-on-huc01050002",
    "href": "FIM4_workflow_slides.html#focus-in-on-huc01050002",
    "title": "So long as you get the GIS&T of it",
    "section": "Focus in on HUC:01050002",
    "text": "Focus in on HUC:01050002\n```{{r, tVPU, warning = FALSE, echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“VPU1_HF_HUC.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;% leaflet::addProviderTiles(“CartoDB.Positron”,group = “CartoDB.Positron”) %&gt;% leafem::addFeatures(data = sf::st_transform(hf_flowpaths,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=“blue”) %&gt;% leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs[4,],sf::st_crs(‘EPSG:4326’)),stroke = TRUE,opacity = 1,fillOpacity = 0,weight = 0.2,color=“black”) %&gt;% leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=“#000000”) %&gt;% leaflet::addLabelOnlyMarkers(data=sf::st_transform(nearby_hucs %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs(‘EPSG:4326’)), label = ~name, labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T), group=“Index Labels”) %&gt;% leaflet::fitBounds(hf_bounds[3],hf_bounds[2],hf_bounds[1],hf_bounds[4]) mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep), border_size = 4,just = “center”) ),file.path(path_to_output,glue::glue(“VPU1_HF_HUC.png”),fsep = .Platform$file.sep)) }\n\n![](./vis/VPU1_HF_HUC.png)\n\n## Focus in on HUC:01050002\n\n```{r, HUC, warning = FALSE, echo = FALSE}\nif(!file.exists(file.path(path_to_output,glue::glue(\"HUC01050002.png\"),fsep = .Platform$file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n  \n  huc_flowpaths &lt;- hf_flowpaths[wbd8_clp_inputs[4,],]\n  huc_divides &lt;- hf_divides[hf_divides$id %in% huc_flowpaths$divide_id,]\n  huc_network &lt;- hf_network[hf_network$divide_id %in% huc_flowpaths$divide_id,]\n  \n  hf_bounds &lt;- sf::st_bbox(sf::st_transform(huc_divides,sf::st_crs('EPSG:4326'))) %&gt;% unlist() %&gt;% unname()\n  hf_divide_values &lt;- unique(huc_divides$divide_id)\n  hf_divide_pal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(hf_divide_values)),hf_divide_values,na.color = \"transparent\")\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(huc_divides,sf::st_crs('EPSG:4326')),stroke = TRUE, opacity = 0.8,fillOpacity = 0,fill = FALSE,weight = 1) %&gt;%\n    leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs[4,],sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 0,weight = 0.2,color=\"black\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(huc_flowpaths,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"blue\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    leaflet::addLabelOnlyMarkers(data=sf::st_transform(nearby_hucs %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n                                 label = ~`name`,\n                                 labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n                                 group=\"Index Labels\") %&gt;%\n    leaflet::fitBounds(hf_bounds[3],hf_bounds[2],hf_bounds[1],hf_bounds[4])\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"HUC01050002.png\"),fsep = .Platform$file.sep))\n}"
  },
  {
    "objectID": "FIM4_workflow_slides.html#and-by-divide",
    "href": "FIM4_workflow_slides.html#and-by-divide",
    "title": "So long as you get the GIS&T of it",
    "section": "And By Divide",
    "text": "And By Divide\n```{{r,divide, warning = FALSE, echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“HUC01050002_divide.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\nhuc_flowpaths &lt;- hf_flowpaths[wbd8_clp_inputs[4,],] huc_divides &lt;- hf_divides[hf_divides\\(id %in% huc_flowpaths\\)divide_id,] huc_network &lt;- hf_network[hf_network\\(divide_id %in% huc_flowpaths\\)divide_id,]\nhf_bounds &lt;- sf::st_bbox(sf::st_transform(huc_divides,sf::st_crs(‘EPSG:4326’))) %&gt;% unlist() %&gt;% unname() hf_divide_values &lt;- unique(huc_divides\\(divide_id)\n  hf_divide_pal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(hf_divide_values)),hf_divide_values,na.color = \"transparent\")\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(huc_divides,sf::st_crs('EPSG:4326')),stroke = TRUE, opacity = 0.8,fillOpacity = 1,fill = TRUE,weight = 1,fillColor=hf_divide_pal(huc_divides\\)divide_id)) %&gt;% leafem::addFeatures(data = sf::st_transform(huc_flowpaths,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=“blue”) %&gt;% leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=“#000000”) %&gt;% leaflet::addLabelOnlyMarkers(data=sf::st_transform(nearby_hucs %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs(‘EPSG:4326’)), label = ~name, labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T), group=“Index Labels”) %&gt;% leaflet::fitBounds(hf_bounds[3],hf_bounds[2],hf_bounds[1],hf_bounds[4]) mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep), border_size = 4,just = “center”) ),file.path(path_to_output,glue::glue(“HUC01050002_divide.png”),fsep = .Platform$file.sep)) }\n\n![](./vis/HUC01050002_divide.png)\n\n## What does this network look like?\n\n```{r, visNetwork1, echo = FALSE}\nhuc_flowpaths &lt;- hf_flowpaths[wbd8_clp_inputs[4,],]\nhuc_divides &lt;- hf_divides[hf_divides$id %in% huc_flowpaths$divide_id,]\nhuc_network &lt;- hf_network[hf_network$divide_id %in% huc_flowpaths$divide_id,]\ng = dplyr::select(huc_network, id, toid) %&gt;%\n  dplyr::distinct() %&gt;%\n  igraph::graph_from_data_frame(directed = TRUE) \nvisNetwork::visIgraph(g) %&gt;%\n  visNetwork::visOptions(highlightNearest = list(enabled = T, hover = T), nodesIdSelection = T)"
  },
  {
    "objectID": "FIM4_workflow_slides.html#fim-pipeline",
    "href": "FIM4_workflow_slides.html#fim-pipeline",
    "title": "So long as you get the GIS&T of it",
    "section": "FIM Pipeline",
    "text": "FIM Pipeline"
  },
  {
    "objectID": "FIM4_workflow_slides.html#fim-pipeline-1",
    "href": "FIM4_workflow_slides.html#fim-pipeline-1",
    "title": "So long as you get the GIS&T of it",
    "section": "FIM Pipeline",
    "text": "FIM Pipeline"
  },
  {
    "objectID": "FIM4_workflow_slides.html#stepping-through-the-pipeline",
    "href": "FIM4_workflow_slides.html#stepping-through-the-pipeline",
    "title": "So long as you get the GIS&T of it",
    "section": "Stepping through the pipeline",
    "text": "Stepping through the pipeline"
  },
  {
    "objectID": "FIM4_workflow_slides.html#the-fim-pipeline-derive-levelpaths",
    "href": "FIM4_workflow_slides.html#the-fim-pipeline-derive-levelpaths",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Pipeline: Derive Levelpaths",
    "text": "The FIM Pipeline: Derive Levelpaths\n```{{r, lelvepaths, echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“HUC01050002_levels.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\nbranch_polys &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,“branch_polygons.gpkg”,fsep = .Platform$file.sep)) map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;% leaflet::addProviderTiles(“CartoDB.Positron”,group = “CartoDB.Positron”) %&gt;% # leafem::addFeatures(data = sf::st_transform(schism_pts,sf::st_crs(‘EPSG:4326’)),stroke = FALSE,opacity = 1,fillOpacity = 1,weight = 0.02,color=“black”) %&gt;% leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs(‘EPSG:4326’)),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=“#0327cd”) %&gt;% leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(branch_polys,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=“#000000”) %&gt;% leaflet::addLabelOnlyMarkers(data=sf::st_transform(branch_polys %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs(‘EPSG:4326’)), label = ~levpa_id, labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T), group=“Index Labels”) %&gt;% leaflet::fitBounds(hf_bounds[3],hf_bounds[2],hf_bounds[1],hf_bounds[4])\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep), border_size = 4,just = “center”) ),file.path(path_to_output,glue::glue(“HUC01050002_levels.png”),fsep = .Platform$file.sep)) }\n\n![](./vis/HUC01050002_levels.png)\n\n## Aside: What does SCHISM see?\n\n```{r, bs, echo = FALSE}\nif(!file.exists(file.path(path_to_output,glue::glue(\"HUC01050002_schism.png\"),fsep = .Platform$file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n  \n  basedir &lt;- \"~/data/raw/nohrsc/owp_files/nwm/nwm_parameters/NWM_coastal_parameters.tar/coastal_parameters/coastal/atlgulf/\"\n  hgrid.gr3_file &lt;- file.path(basedir,\"hgrid.gr3\")\n  hgrid.gr3 &lt;- data.table::fread(hgrid.gr3_file,skip = 2,nrows = 10537611-2)\n  hgrid.gr3_sf &lt;- sfheaders::sf_point(\n    obj = hgrid.gr3\n    , x = \"V2\"\n    , y = \"V3\"\n    , z = \"V4\"\n    , keep = TRUE\n  ) |&gt; sf::st_set_crs(sf::st_crs(\"EPSG:6349\")) %&gt;% \n    sf::st_transform(sf::st_crs(\"EPSG:4326\"))\n  schism_pts &lt;- hgrid.gr3_sf[sf::st_transform(wbd8_clp_inputs,sf::st_crs(hgrid.gr3_sf)) %&gt;% sf::st_make_valid(),]\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(schism_pts,sf::st_crs('EPSG:4326')),stroke = FALSE,opacity = 1,fillOpacity = 1,weight = 0.1,color=\"black\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    leaflet::addLabelOnlyMarkers(data=sf::st_transform(nearby_hucs %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n                                 label = ~`name`,\n                                 labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n                                 group=\"Index Labels\") %&gt;%\n    leaflet::fitBounds(hf_bounds[3],hf_bounds[2],hf_bounds[1],hf_bounds[4])\n\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"HUC01050002_schism.png\"),fsep = .Platform$file.sep))\n}"
  },
  {
    "objectID": "FIM4_workflow_slides.html#the-fim-pipeline-target-branch",
    "href": "FIM4_workflow_slides.html#the-fim-pipeline-target-branch",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Pipeline: Target branch",
    "text": "The FIM Pipeline: Target branch\n```{{r, tlevelpath, echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“HUC01050002_levels_select.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\nbranch_polys &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,“branch_polygons.gpkg”,fsep = .Platform$file.sep)) branch_polys_selction &lt;- branch_polys[25,] #2,11, 19 map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;% leaflet::addProviderTiles(“CartoDB.Positron”,group = “CartoDB.Positron”) %&gt;% # leafem::addFeatures(data = sf::st_transform(schism_pts,sf::st_crs(‘EPSG:4326’)),stroke = FALSE,opacity = 1,fillOpacity = 1,weight = 0.02,color=“black”) %&gt;% leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs(‘EPSG:4326’)),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=“#0327cd”) %&gt;% leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(branch_polys,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=“yellow”) %&gt;% leaflet::addLabelOnlyMarkers(data=sf::st_transform(branch_polys %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs(‘EPSG:4326’)), label = ~levpa_id, labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T), group=“Index Labels”) %&gt;% leaflet::fitBounds(hf_bounds[3],hf_bounds[2],hf_bounds[1],hf_bounds[4])\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep), border_size = 4,just = “center”) ),file.path(path_to_output,glue::glue(“HUC01050002_levels_select.png”),fsep = .Platform$file.sep)) }\n\n![](./vis/HUC01050002_levels_select.png)\n\n## The FIM Pipeline: Target branch callout\n\n```{r, slevelpath, echo = FALSE}\nif(!file.exists(file.path(path_to_output,glue::glue(\"HUC01050002_levels_select_lone.png\"),fsep = .Platform$file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n  \n  branch_polys &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branch_polygons.gpkg\",fsep = .Platform$file.sep))\n  branch_polys_selction &lt;- branch_polys[25,] #2,11, 19\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(schism_pts,sf::st_crs('EPSG:4326')),stroke = FALSE,opacity = 1,fillOpacity = 1,weight = 0.02,color=\"black\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(branch_polys,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=\"yellow\") %&gt;%\n    # leaflet::addLabelOnlyMarkers(data=sf::st_transform(branch_polys %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n    #                              label = ~`levpa_id`,\n    #                              labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n    #                              group=\"Index Labels\") %&gt;%\n    leaflet::fitBounds(hf_bounds[3],hf_bounds[2],hf_bounds[1],hf_bounds[4])\n\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"HUC01050002_levels_select_lone.png\"),fsep = .Platform$file.sep))\n}"
  },
  {
    "objectID": "FIM4_workflow_slides.html#the-fim-pipeline-target-branch-1",
    "href": "FIM4_workflow_slides.html#the-fim-pipeline-target-branch-1",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Pipeline: Target branch",
    "text": "The FIM Pipeline: Target branch\n```{{r, tbranch, echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“HUC01050002_selectlevel.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\nbranch_polys &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,“branch_polygons.gpkg”,fsep = .Platform\\(file.sep))\n  branch_polys_selction &lt;- branch_polys[25,] #2,11, 19\n  aoi_bbox &lt;- sf::st_bbox(sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326'))) %&gt;% unlist() %&gt;% unname()\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(schism_pts,sf::st_crs('EPSG:4326')),stroke = FALSE,opacity = 1,fillOpacity = 1,weight = 0.02,color=\"black\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths\\)levpa_id==branch_polys_selction$levpa_id,],sf::st_crs(‘EPSG:4326’)),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=“#0327cd”) %&gt;% leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=“#000000”) %&gt;% # leafem::addFeatures(data = sf::st_transform(branch_polys,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=“yellow”) %&gt;% leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 0.2,weight = 1,color=“#000000”) %&gt;% leaflet::addLabelOnlyMarkers(data=sf::st_transform(branch_polys_selction %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs(‘EPSG:4326’)), label = ~levpa_id, labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T), group=“Index Labels”) %&gt;% # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4]) leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep), border_size = 4,just = “center”) ),file.path(path_to_output,glue::glue(“HUC01050002_selectlevel.png”),fsep = .Platform$file.sep)) }\n\n![](./vis/HUC01050002_selectlevel.png)\n\n## The FIM Pipeline: DEM\n\n```{r,dem, echo = FALSE}\nif(!file.exists(file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_DEM.png\"),fsep = .Platform$file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n  \n  dem_meters &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"dem_meters_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n\n  gg_df &lt;- as.data.frame(dem_meters,xy = T)\n  gglegend &lt;- ggplot2::ggplot() +\n    ggplot2::geom_raster(data = gg_df, ggplot2::aes(x = x, y = y, fill = dem_meters_2678000065)) +\n    ggplot2::scale_fill_stepsn(name = \"3DEP 10m (m)\", \n                               colors =c(\"forestgreen\",\"yellow\",\"tan\",\"brown\"),\n                               breaks = round(seq(plyr::round_any(min(na.omit(gg_df$dem_meters_2678000065)), 10, f = floor),\n                                            plyr::round_any(max(na.omit(gg_df$dem_meters_2678000065)), 10, f = ceiling), length = 7), 1),\n                               values = scales::rescale(\n                                 seq(plyr::round_any(min(na.omit(gg_df$dem_meters_2678000065)), 10, f = floor),\n                                     plyr::round_any(max(na.omit(gg_df$dem_meters_2678000065)), 10, f = ceiling), length = 6)))\n  llegend &lt;- cowplot::get_legend(gglegend)\n  cowplot::save_plot(plot=cowplot::plot_grid(llegend),filename=file.path(path_to_tmp_output,glue::glue(\"test_legend.png\"),fsep = .Platform$file.sep))\n  pal &lt;- leaflet::colorNumeric(c(\"forestgreen\",\"yellow\",\"tan\",\"brown\"), terra::values(dem_meters),na.color = \"transparent\")\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths$levpa_id==branch_polys_selction$levpa_id,],sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    # leafem::addGeoRaster(dem_meters,colorOptions = leafem:::colorOptions(palette = pal,\n    #                                                                      # breaks = as.numeric(c(0:xx$max_ffreq)),\n    #                                                                      domain = c(dem_meters_values_min,dem_meters_values_max))) %&gt;%\n    # leafem::addGeoRaster(dem_meters,color= pal) %&gt;%\n    leaflet::addRasterImage(dem_meters, colors = pal, opacity = 0.9,maxBytes=42*1024*1024) %&gt;%\n    # leaflet::addLegend(\"bottomleft\",title = \"3DEP 10m (m)\", pal = pal, values = terra::values(dem_meters)) %&gt;% \n    leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\n  map &lt;- magick::image_read(file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\n  legend &lt;- magick::image_read(file.path(path_to_tmp_output,glue::glue(\"test_legend.png\"),fsep = .Platform$file.sep)) %&gt;% \n    magick::image_trim() %&gt;% \n    magick::image_fill(\n      color = \"transparent\", \n      refcolor = \"white\", \n      fuzz = 4,\n      point = \"+1+1\"\n    ) %&gt;% \n    magick::image_scale(\"200\")\n  magick::image_write(image = magick::image_mosaic(c(map, legend)),path=file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_DEM.png\"),fsep = .Platform$file.sep))\n}"
  },
  {
    "objectID": "FIM4_workflow_slides.html#the-fim-pipeline-flow-girds-boolean",
    "href": "FIM4_workflow_slides.html#the-fim-pipeline-flow-girds-boolean",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Pipeline: Flow Girds (boolean)",
    "text": "The FIM Pipeline: Flow Girds (boolean)\n```{{r, fgrids,echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“HUC01050002_selectlevel_flows_grid_boolean.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\nflows_grid_boolean &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,“branches”,branch_polys_selction\\(levpa_id,glue::glue(\"flows_grid_boolean_{branch_polys_selction\\)levpa_id}.tif”),fsep = .Platform\\(file.sep)) %&gt;%\n  terra::classify(cbind(-Inf, 0, NA), right=TRUE)\n  pal &lt;- leaflet::colorFactor(c(\"black\",\"blue\"), c(0,1),na.color = \"transparent\")\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    leaflet::addRasterImage(flows_grid_boolean, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n    leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n  # addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\n  map1\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep),vwidth = 1080,vheight = 1080) magick::image_write( magick::image_read( cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_flows_grid_boolean.png\"),fsep = .Platform\\)file.sep)) }\n\n![](./vis/HUC01050002_selectlevel_flows_grid_boolean.png)\n\n## The FIM Pipeline: Headwaters\n\n```{r, heads,echo = FALSE}\nif(!file.exists(file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_headwaters.png\"),fsep = .Platform$file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n  \n  headwaters &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"nwm_headwater_points_subset.gpkg\",fsep = .Platform$file.sep))\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(headwaters,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,radius = 2,color=\"#FF0000\") %&gt;%\n    # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n    leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n  # addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\n  map1\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_headwaters.png\"),fsep = .Platform$file.sep))\n}"
  },
  {
    "objectID": "FIM4_workflow_slides.html#the-fim-pipeline-flow-direction",
    "href": "FIM4_workflow_slides.html#the-fim-pipeline-flow-direction",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Pipeline: Flow Direction",
    "text": "The FIM Pipeline: Flow Direction\n```{{r,fd, echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“HUC01050002_selectlevel_flowdir.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\n# flowdir_d8_burned_filled &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,“flowdir_d8_burned_filled.tif”,fsep = .Platform\\(file.sep))\n  flowdir_d8_burned_filled &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction\\)levpa_id,glue::glue(“flowdir_d8_burned_filled_{branch_polys_selction\\(levpa_id}.tif\"),fsep = .Platform\\)file.sep)) pal &lt;- leaflet::colorFactor(c(”#862627”,“#26207a”,“#6563b7”,“#6563b7”,“#cbc6c0”,“#fee08a”,“#f4a429”,“#c2631f”),c(1,2,3,4,5,6,7,8),na.color = “transparent”) map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;% leaflet::addProviderTiles(“CartoDB.Positron”,group = “CartoDB.Positron”) %&gt;% # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs(‘EPSG:4326’)),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=“#0327cd”) %&gt;% leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths\\(levpa_id==branch_polys_selction\\)levpa_id,],sf::st_crs(‘EPSG:4326’)),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=“#0327cd”) %&gt;% # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=“#000000”) %&gt;% leaflet::addRasterImage(flowdir_d8_burned_filled, colors = pal, opacity = 1,maxBytes=4210241024) %&gt;% leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=“black”) %&gt;% # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4]) leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4]) # addLegend(pal = pal, values = terra::values(dem_meters), title = “Elevation data for Santander (mts)”) map1 mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep), border_size = 4,just = “center”) ),file.path(path_to_output,glue::glue(“HUC01050002_selectlevel_flowdir.png”),fsep = .Platform$file.sep))\n}\n\n![](./vis/HUC01050002_selectlevel_flowdir.png)\n\n## The FIM Pipeline: Burned & Filled Flow Direction\n\n```{r, bf,echo = FALSE}\nif(!file.exists(file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_BFflowdir.png\"),fsep = .Platform$file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n  \n  # flowdir_d8_burned_filled &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"flowdir_d8_burned_filled.tif\",fsep = .Platform$file.sep))\n  flowdir_d8_burned_filled &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"flowdir_d8_burned_filled_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n  pal &lt;- leaflet::colorFactor(c(\"#862627\",\"#26207a\",\"#6563b7\",\"#6563b7\",\"#cbc6c0\",\"#fee08a\",\"#f4a429\",\"#c2631f\"),c(1,2,3,4,5,6,7,8),na.color = \"transparent\")\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths$levpa_id==branch_polys_selction$levpa_id,],sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    leaflet::addRasterImage(flowdir_d8_burned_filled, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n    leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n    # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n    leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n  # addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\n  map1\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_BFflowdir.png\"),fsep = .Platform$file.sep))\n\n}"
  },
  {
    "objectID": "FIM4_workflow_slides.html#the-fim-pipeline-flow-accumulation",
    "href": "FIM4_workflow_slides.html#the-fim-pipeline-flow-accumulation",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Pipeline: Flow Accumulation",
    "text": "The FIM Pipeline: Flow Accumulation\n```{{r, fa, echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“HUC01050002_selectlevel_faBFflowdir.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\nflowaccum_d8_burned_filled &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,“branches”,“0”,“flowaccum_d8_burned_filled_0.tif”,fsep = .Platform\\(file.sep))\n  pal &lt;- leaflet::colorFactor(c(\"#862627\",\"#26207a\",\"#6563b7\",\"#6563b7\",\"#cbc6c0\",\"#fee08a\",\"#f4a429\",\"#c2631f\"),c(1,2,3,4,5,6,7,8),na.color = \"transparent\")\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths\\)levpa_id==branch_polys_selction\\(levpa_id,],sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    leaflet::addRasterImage(flowaccum_d8_burned_filled, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n    leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n  # addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\n  map1\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep),vwidth = 1080,vheight = 1080) magick::image_write( magick::image_read( cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_faBFflowdir.png\"),fsep = .Platform\\)file.sep))\n}\n\n![](./vis/HUC01050002_selectlevel_faBFflowdir.png)\n\n## The FIM Pipeline: Slope\n\n```{r,slope, echo = FALSE}\nif(!file.exists(file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_Slope.png\"),fsep = .Platform$file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n\n  # slopes_d8_dem_meters &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",\"0\",\"slopes_d8_dem_meters_masked_0.tif\",fsep = .Platform$file.sep))\n  slopes_d8_dem_meters &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"slopes_d8_dem_meters_masked_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n  pal &lt;- leaflet::colorFactor(c(\"#862627\",\"#26207a\",\"#6563b7\",\"#6563b7\",\"#cbc6c0\",\"#fee08a\",\"#f4a429\",\"#c2631f\"),c(1,2,3,4,5,6,7,8),na.color = \"transparent\")\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths$levpa_id==branch_polys_selction$levpa_id,],sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    leaflet::addRasterImage(slopes_d8_dem_meters, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n    leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n  # addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\n  map1\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_Slope.png\"),fsep = .Platform$file.sep))\n\n}\n\ntest"
  },
  {
    "objectID": "FIM4_workflow_slides.html#the-fim-pipeline-dem-derived-reaches",
    "href": "FIM4_workflow_slides.html#the-fim-pipeline-dem-derived-reaches",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Pipeline: DEM Derived Reaches",
    "text": "The FIM Pipeline: DEM Derived Reaches\n```{{r, demreach, echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“HUC01050002_selectlevel_snreach.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\n# sn_demDerived_reaches &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,“branches”,“0”,“demDerived_reaches_0.shp”,fsep = .Platform\\(file.sep))\n  # sn_catchemnt_reaches &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",\"0\",\"sn_catchments_reaches_0.tif\",fsep = .Platform\\)file.sep)) sn_demDerived_reaches &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,“branches”,branch_polys_selction\\(levpa_id,glue::glue(\"demDerived_reaches_{branch_polys_selction\\)levpa_id}.shp”),fsep = .Platform\\(file.sep))\n  sn_catchemnt_reaches &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction\\)levpa_id,glue::glue(“sn_catchments_reaches_{branch_polys_selction\\(levpa_id}.tif\"),fsep = .Platform\\)file.sep))\ncatch_values &lt;- unique(terra::values(sn_catchemnt_reaches)) pal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(catch_values)),catch_values,na.color = “transparent”) map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;% leaflet::addProviderTiles(“CartoDB.Positron”,group = “CartoDB.Positron”) %&gt;% leafem::addFeatures(data = sf::st_transform(sn_demDerived_reaches,sf::st_crs(‘EPSG:4326’)),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=“#0327cd”) %&gt;% # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=“#000000”) %&gt;% leaflet::addRasterImage(sn_catchemnt_reaches, colors = pal, opacity = 1,maxBytes=4210241024) %&gt;% leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=“black”) %&gt;% # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4]) leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4]) # addLegend(pal = pal, values = terra::values(dem_meters), title = “Elevation data for Santander (mts)”) mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep), border_size = 4,just = “center”) ),file.path(path_to_output,glue::glue(“HUC01050002_selectlevel_snreach.png”),fsep = .Platform$file.sep))\n}\n\n![](./vis/HUC01050002_selectlevel_snreach.png)\n\n## The FIM Pipeline: Gage Weighted (gw) Reaches\n\n```{r, gwreach, echo = FALSE}\nif(!file.exists(file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_gwreach.png\"),fsep = .Platform$file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n  \n  # gw_catchments_reaches &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",\"0\",\"gw_catchments_reaches_0.tif\",fsep = .Platform$file.sep))\n  gw_catchments_reaches &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"gw_catchments_reaches_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n  \n  catch_values &lt;- unique(terra::values(gw_catchments_reaches))\n  pal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(catch_values)),catch_values,na.color = \"transparent\")\n  map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n    leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n    # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n    leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n    leaflet::addRasterImage(gw_catchments_reaches, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n    leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n    # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n    leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n  # addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\n  mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n    ),file.path(path_to_output,glue::glue(\"HUC01050002_selectlevel_gwreach.png\"),fsep = .Platform$file.sep))\n\n}"
  },
  {
    "objectID": "FIM4_workflow_slides.html#the-fim-pipeline-hand",
    "href": "FIM4_workflow_slides.html#the-fim-pipeline-hand",
    "title": "So long as you get the GIS&T of it",
    "section": "The FIM Pipeline: HAND",
    "text": "The FIM Pipeline: HAND\n```{{r, HAND,echo = FALSE}} if(!file.exists(file.path(path_to_output,glue::glue(“HUC01050002_selectlevel_HAND.png”),fsep = .Platform\\(file.sep))) {\n  unlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform\\)file.sep))\n# HAND &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,“branches”,“0”,“rem_zeroed_masked_0.tif”,fsep = .Platform\\(file.sep))\n  HAND &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction\\)levpa_id,glue::glue(“rem_zeroed_masked_{branch_polys_selction\\(levpa_id}.tif\"),fsep = .Platform\\)file.sep))\nHAND_values &lt;- terra::values(HAND) pal &lt;- leaflet::colorNumeric(RColorBrewer::brewer.pal(5, “Blues”), c(min(HAND_values,na.rm = TRUE),max(HAND_values,na.rm = TRUE)),na.color = “transparent”) map1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;% leaflet::addProviderTiles(“CartoDB.Positron”,group = “CartoDB.Positron”) %&gt;% # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=“#000000”) %&gt;% leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=“#000000”) %&gt;% leaflet::addRasterImage(HAND, colors = pal, opacity = 1,maxBytes=5010241024) %&gt;% leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs(‘EPSG:4326’)),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=“black”) %&gt;% # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4]) leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4]) # addLegend(pal = pal, values = terra::values(dem_meters), title = “Elevation data for Santander (mts)”) mapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(“test.png”),fsep = .Platform\\(file.sep),vwidth = 1080,vheight = 1080)\n  magick::image_write(\n    magick::image_read(\n      cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform\\)file.sep), border_size = 4,just = “center”) ),file.path(path_to_tmp_output,glue::glue(“test_cropped.png”),fsep = .Platform$file.sep))\ngg_df &lt;- as.data.frame(HAND,xy = T) gglegend &lt;- ggplot2::ggplot() + ggplot2::geom_raster(data = gg_df, ggplot2::aes(x = x, y = y, fill = rem_zeroed_masked_2678000065)) + ggplot2::scale_fill_stepsn(name = “HAND (m)”, colors =RColorBrewer::brewer.pal(5, “Blues”), breaks = round(seq(plyr::round_any(min(na.omit(gg_df\\(rem_zeroed_masked_2678000065)), 10, f = floor),\n                                                  plyr::round_any(max(na.omit(gg_df\\)rem_zeroed_masked_2678000065)), 10, f = ceiling), length = 7), 1), values = scales::rescale( seq(plyr::round_any(min(na.omit(gg_df\\(rem_zeroed_masked_2678000065)), 10, f = floor),\n                                     plyr::round_any(max(na.omit(gg_df\\)rem_zeroed_masked_2678000065)), 10, f = ceiling), length = 6))) llegend &lt;- cowplot::get_legend(gglegend) cowplot::save_plot(plot=cowplot::plot_grid(llegend),filename=file.path(path_to_tmp_output,glue::glue(“test_legend.png”),fsep = .Platform$file.sep))\nmap &lt;- magick::image_read(file.path(path_to_tmp_output,glue::glue(“test_cropped.png”),fsep = .Platform\\(file.sep))\n  legend &lt;- magick::image_read(file.path(path_to_tmp_output,glue::glue(\"test_legend.png\"),fsep = .Platform\\)file.sep)) %&gt;% magick::image_trim() %&gt;% magick::image_fill( color = “transparent”, refcolor = “white”, fuzz = 4, point = “+1+1” ) %&gt;% magick::image_scale(“200”) magick::image_write(image = magick::image_mosaic(c(map, legend)),path=file.path(path_to_output,glue::glue(“HUC01050002_selectlevel_HAND.png”),fsep = .Platform$file.sep))\n}\n![](./vis/HUC01050002_selectlevel_HAND.png)\n\n## These are all interactive too!\n\n```{r, HANDleaf,echo = FALSE}\nHAND &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"rem_zeroed_masked_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\nHAND_values &lt;- terra::values(HAND)\npal &lt;- leaflet::colorNumeric(RColorBrewer::brewer.pal(5, \"Blues\"), c(min(HAND_values,na.rm = TRUE),max(HAND_values,na.rm = TRUE)),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths$levpa_id==branch_polys_selction$levpa_id,],sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(HAND, colors = pal, opacity = 1,maxBytes=50*1024*1024) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\nmap1"
  },
  {
    "objectID": "FIM4_workflow_slides.html#images-for-squirrels",
    "href": "FIM4_workflow_slides.html#images-for-squirrels",
    "title": "So long as you get the GIS&T of it",
    "section": "Images for squirrels",
    "text": "Images for squirrels\n{r, gif,echo = FALSE} if(!file.exists(file.path(path_to_output,glue::glue(\"FIM4_gif\"),fsep = .Platform$file.sep))) {   m &lt;- magick::image_animate(c(magick::image_read(file.path(path_to_output,\"VPU1_HF_HUC.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_divide.png\",fsep = .Platform$file.sep)),   # magick::image_read(file.path(path_to_output,\"HUC01050002_schism.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_levels.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_levels_select.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_levels_select_lone.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_selectlevel.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_selectlevel_DEM.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_selectlevel_flows_grid_boolean.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_selectlevel_headwaters.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_selectlevel_flowdir.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_selectlevel_BFflowdir.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_selectlevel_faBFflowdir.png\",fsep = .Platform$file.sep)),   # magick::image_read(file.path(path_to_output,\"HUC01050002_selectlevel_Slope.png\",fsep = .Platform$file.sep)),   # magick::image_read(file.path(path_to_output,HUC01050002_selectlevel_snreach.png,fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,\"HUC01050002_selectlevel_gwreach.png\",fsep = .Platform$file.sep)),   magick::image_read(file.path(path_to_output,HUC01050002_selectlevel_HAND.png,fsep = .Platform$file.sep))   ), fps = 1, loop = 1, dispose = \"previous\")   magick::image_write(m,file.path(path_to_output,glue::glue(\"FIM4_gif\"),fsep = .Platform$file.sep)) }"
  },
  {
    "objectID": "FIM4_workflow_slides.html#early-improvements",
    "href": "FIM4_workflow_slides.html#early-improvements",
    "title": "So long as you get the GIS&T of it",
    "section": "Early improvements",
    "text": "Early improvements\n\nSane(er) base data handling\n10 fewer steps\n68 seconds off pre-processing and 55 seconds off a branch"
  },
  {
    "objectID": "FIM4_workflow_slides.html#too-long",
    "href": "FIM4_workflow_slides.html#too-long",
    "title": "So long as you get the GIS&T of it",
    "section": "Too long",
    "text": "Too long\n```{r}\n# ===========================\n# FIM4\n# ===========================\nlibrary(magrittr)\n# library(leaflet)\n# library(leafgl)\n# remotes::install_github(\"doehm/cropcircles\")\n\nunit_to_map &lt;- '01050002'\n\npath_to_current &lt;- file.path(\"~/data/output/FIM4_stock/\",fsep = .Platform$file.sep)\npath_to_new &lt;- file.path(\"~/data/output/FIM4_refrepl/\",fsep = .Platform$file.sep)\npath_to_output &lt;- file.path(\"~/data/output/vis\",fsep = .Platform$file.sep)\npath_to_tmp_output &lt;- file.path(path_to_output,\"tmp\",fsep = .Platform$file.sep)\npath_to_maine_stock &lt;- file.path(\"~/Dropbox/root/projects/floodmapping/methods/OWP_FIM/outputs/FIM4_maine/\",fsep = .Platform$file.sep)\n\npath_to_fim4_unit_outputs &lt;- path_to_maine_stock\n\nall_hucs &lt;- sf::st_read(file.path(\"~/Dropbox/root/database/hosted/water/HUC8.fgb\",fsep = .Platform$file.sep))\n\nwbd8_clp_inputs &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"wbd8_clp.gpkg\",fsep=.Platform$file.sep))\nlevelpaths &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"nwm_subset_streams_levelPaths.gpkg\",fsep = .Platform$file.sep))\nnearby_hucs &lt;- sf::st_transform(all_hucs,sf::st_crs(wbd8_clp_inputs))[wbd8_clp_inputs %&gt;% sf::st_buffer(1000),]\n\nhf_flowlines &lt;- arrow::open_dataset(\"~/data/raw/lynker-spatial/hydrofabric/v2.2/final/conus/conus_flowpaths/\") %&gt;% dplyr::collect() %&gt;% sf::st_as_sf()\nsf::st_crs(hf_flowlines) &lt;- sf::st_crs(\"EPSG:5070\")\nhf_divides &lt;- arrow::open_dataset(\"~/data/raw/lynker-spatial/hydrofabric/v2.2/final/conus/conus_divides/\") %&gt;% dplyr::collect() %&gt;% sf::st_as_sf()\nsf::st_crs(hf_divides) &lt;- sf::st_crs(\"EPSG:5070\")\n\nselect_divides &lt;- hf_divides[sf::st_transform(wbd8_clp_inputs,sf::st_crs(hf_divides)),]\nselect_flowlines &lt;- hf_flowlines[hf_flowlines$divide_id %in% select_divides$divide_id,]\n\n# Hydrofrabric maps (boundary conditions)\n# ===========================\nforecast_basins &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"nwm_catchments_proj_subset.gpkg\",fsep = .Platform$file.sep))\nforecast_basins_values &lt;- unique(forecast_basins$ID)\nforecast_basins_pal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(forecast_basins_values)),forecast_basins_values,na.color = \"transparent\")\nforecast_network &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"nwm_subset_streams.gpkg\",fsep = .Platform$file.sep))\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(forecast_basins,sf::st_crs('EPSG:4326')),stroke = FALSE, opacity = 0.8,fillOpacity = 1,fill = TRUE,weight = 1,fillColor=forecast_basins_pal(forecast_basins$ID)) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(forecast_network,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"blue\") %&gt;%\n  # leaflet::addLabelOnlyMarkers(data=sf::st_transform(forecast_basins %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n  #                              label = ~`ID`,\n  #                              labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n  #                              group=\"Index Labels\") %&gt;%\n  leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n\n# Hydrofabric\nhydrofabric_divides_pal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(select_divides$divide_id)),select_divides$divide_id,na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(select_divides,sf::st_crs('EPSG:4326')),stroke = FALSE, opacity = 0.8,fillOpacity = 1,fill = TRUE,weight = 1,fillColor=hydrofabric_divides_pal(select_divides$divide_id)) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(select_flowlines,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"blue\") %&gt;%\n  # leaflet::addLabelOnlyMarkers(data=sf::st_transform(forecast_basins %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n  #                              label = ~`ID`,\n  #                              labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n  #                              group=\"Index Labels\") %&gt;%\n  leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n\n# Test Flowlines\nhydrofabric_flowlpaths &lt;- sf::st_read(\"~/data/temp/test.gpkg\",layer = 'flowpaths')\nhydrofabric_divides &lt;- sf::st_read(\"~/data/temp/test.gpkg\",layer = 'divides')\nhydrofabric_flowlines &lt;- sf::st_read(\"~/data/temp/test.gpkg\",layer = 'flowlines')\nhydrofabric_divides_pal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(select_divides$divide_id)),select_divides$divide_id,na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(select_divides,sf::st_crs('EPSG:4326')),stroke = FALSE, opacity = 0.8,fillOpacity = 1,fill = TRUE,weight = 1,fillColor=hydrofabric_divides_pal(select_divides$divide_id)) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(select_flowlines,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"blue\") %&gt;%\n  # leaflet::addLabelOnlyMarkers(data=sf::st_transform(forecast_basins %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n  #                              label = ~`ID`,\n  #                              labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n  #                              group=\"Index Labels\") %&gt;%\n  leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n# ===============\n\n\n# Coastal\nbasedir &lt;- \"~/data/raw/nohrsc/owp_files/nwm/nwm_parameters/NWM_coastal_parameters.tar/coastal_parameters/coastal/atlgulf/\"\nhgrid.gr3_file &lt;- file.path(basedir,\"hgrid.gr3\")\nhgrid.gr3 &lt;- data.table::fread(hgrid.gr3_file,skip = 2,nrows = 10537611-2)\nhgrid.gr3_sf &lt;- sfheaders::sf_point(\n  obj = hgrid.gr3\n  , x = \"V2\"\n  , y = \"V3\"\n  , z = \"V4\"\n  , keep = TRUE\n) |&gt; sf::st_set_crs(sf::st_crs(\"EPSG:6349\")) %&gt;% \n  sf::st_transform(sf::st_crs(\"EPSG:4326\"))\nschism_pts &lt;- hgrid.gr3_sf[sf::st_transform(wbd8_clp_inputs,sf::st_crs(hgrid.gr3_sf)) %&gt;% sf::st_make_valid(),]\n\n# Branch 0\n# ===========================\nwbd_buffered_inputs &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"wbd_buffered.gpkg\",fsep=.Platform$file.sep))\nbranch_0_bbox &lt;- sf::st_bbox(sf::st_transform(wbd_buffered_inputs,sf::st_crs('EPSG:4326'))) %&gt;% unlist() %&gt;% unname()\n\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(schism_pts,sf::st_crs('EPSG:4326')),stroke = FALSE,opacity = 1,fillOpacity = 1,weight = 0.02,color=\"black\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addLabelOnlyMarkers(data=sf::st_transform(nearby_hucs %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n                               label = ~`name`,\n                               labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n                               group=\"Index Labels\") %&gt;%\n  leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n# ===========================\nbranch_polys &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branch_polygons.gpkg\",fsep = .Platform$file.sep))\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(schism_pts,sf::st_crs('EPSG:4326')),stroke = FALSE,opacity = 1,fillOpacity = 1,weight = 0.02,color=\"black\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addLabelOnlyMarkers(data=sf::st_transform(branch_polys %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n                               label = ~`levpa_id`,\n                               labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n                               group=\"Index Labels\") %&gt;%\n  leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n# BranchPolySelector===========================\nbranch_polys_selction &lt;- branch_polys[25,] #2,11, 19\naoi_bbox &lt;- sf::st_bbox(sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326'))) %&gt;% unlist() %&gt;% unname()\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(schism_pts,sf::st_crs('EPSG:4326')),stroke = FALSE,opacity = 1,fillOpacity = 1,weight = 0.02,color=\"black\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(branch_polys,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=TRUE,opacity = 1,fillOpacity = 0.2,weight = 1,color=\"yellow\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 0.2,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addLabelOnlyMarkers(data=sf::st_transform(branch_polys_selction %&gt;% sf::st_centroid(quiet = TRUE),sf::st_crs('EPSG:4326')),\n                               label = ~`levpa_id`,\n                               labelOptions = leaflet::labelOptions(noHide = T, sticky = T,textOnly = T),\n                               group=\"Index Labels\") %&gt;%\n  # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n\n# ===========================\ndem_meters &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"dem_meters.tif\",fsep = .Platform$file.sep))\ndem_meters &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"dem_meters_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n\ngg_df &lt;- as.data.frame(dem_meters,xy = T)\ngglegend &lt;- ggplot2::ggplot() +\n  ggplot2::geom_raster(data = gg_df, ggplot2::aes(x = x, y = y, fill = dem_meters_2678000065)) +\n  ggplot2::scale_fill_stepsn(name = \"3DEP 10m (m)\", \n                             colors =c(\"forestgreen\",\"yellow\",\"tan\",\"brown\"),\n                             breaks = seq(plyr::round_any(min(na.omit(gg_df$dem_meters_2678000065)), 10, f = floor),\n                                          plyr::round_any(max(na.omit(gg_df$dem_meters_2678000065)), 10, f = ceiling), length = 7),\n                             values = scales::rescale(\n                               seq(plyr::round_any(min(na.omit(gg_df$dem_meters_2678000065)), 10, f = floor),\n                                   plyr::round_any(max(na.omit(gg_df$dem_meters_2678000065)), 10, f = ceiling), length = 6))) \nllegend &lt;- cowplot::get_legend(gglegend)\n\npal &lt;- leaflet::colorNumeric(c(\"forestgreen\",\"yellow\",\"tan\",\"brown\"), terra::values(dem_meters),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths[levelpaths$levpa_id==branch_polys_selction$levpa_id,],sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  # leafem::addGeoRaster(dem_meters,colorOptions = leafem:::colorOptions(palette = pal,\n  #                                                                      # breaks = as.numeric(c(0:xx$max_ffreq)),\n  #                                                                      domain = c(dem_meters_values_min,dem_meters_values_max))) %&gt;%\n  # leafem::addGeoRaster(dem_meters,color= pal) %&gt;%\n  leaflet::addRasterImage(dem_meters, colors = pal, opacity = 0.9,maxBytes=42*1024*1024) %&gt;%\n  # leaflet::addLegend(\"bottomleft\",title = \"3DEP 10m (m)\", pal = pal, values = terra::values(dem_meters)) %&gt;% \n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep),vwidth = input$dimension[1])\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\n\nggplot2::ggsave(cowplot::plot_grid(llegend, rel_widths = c(1)),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep),)\nholepuch_map &lt;- cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\nmagick::image_display(holepuch_map)\n\nmagick::image_read(holepuch_map)\nmagick::image_read(test)\n\nmagick::image_mosaic(c(bigdata, logo, frink))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n\n# ===========================\nflows_grid_boolean &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"flows_grid_boolean.tif\",fsep = .Platform$file.sep))\nflows_grid_boolean &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"flows_grid_boolean_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep)) %&gt;%\n  terra::classify(cbind(-Inf, 0, NA), right=TRUE)\npal &lt;- leaflet::colorFactor(c(\"black\",\"blue\"), c(0,1),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(flows_grid_boolean, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n# ===========================\nheadwaters &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"nwm_headwater_points_subset.gpkg\",fsep = .Platform$file.sep))\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(headwaters,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,radius = 2,color=\"#FF0000\") %&gt;%\n  # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n# ===========================\nflowdir_d8_burned_filled &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"flowdir_d8_burned_filled.tif\",fsep = .Platform$file.sep))\nflowdir_d8_burned_filled &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"flowdir_d8_burned_filled_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\npal &lt;- leaflet::colorFactor(c(\"#862627\",\"#26207a\",\"#6563b7\",\"#6563b7\",\"#cbc6c0\",\"#fee08a\",\"#f4a429\",\"#c2631f\"),c(1,2,3,4,5,6,7,8),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(flowdir_d8_burned_filled, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n# ===========================\nflowaccum_d8_burned_filled &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",\"0\",\"flowaccum_d8_burned_filled_0.tif\",fsep = .Platform$file.sep))\npal &lt;- leaflet::colorFactor(c(\"#862627\",\"#26207a\",\"#6563b7\",\"#6563b7\",\"#cbc6c0\",\"#fee08a\",\"#f4a429\",\"#c2631f\"),c(1,2,3,4,5,6,7,8),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(flowaccum_d8_burned_filled, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n  leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n# ===========================\nslopes_d8_dem_meters &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",\"0\",\"slopes_d8_dem_meters_masked_0.tif\",fsep = .Platform$file.sep))\nslopes_d8_dem_meters &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"slopes_d8_dem_meters_masked_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\npal &lt;- leaflet::colorFactor(c(\"#862627\",\"#26207a\",\"#6563b7\",\"#6563b7\",\"#cbc6c0\",\"#fee08a\",\"#f4a429\",\"#c2631f\"),c(1,2,3,4,5,6,7,8),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(levelpaths,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(flowaccum_d8_burned_filled, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n  leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n\n# ===========================\nsn_demDerived_reaches &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",\"0\",\"demDerived_reaches_0.shp\",fsep = .Platform$file.sep))\nsn_catchemnt_reaches &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",\"0\",\"sn_catchments_reaches_0.tif\",fsep = .Platform$file.sep))\nsn_demDerived_reaches &lt;- sf::st_read(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"demDerived_reaches_{branch_polys_selction$levpa_id}.shp\"),fsep = .Platform$file.sep))\nsn_catchemnt_reaches &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"sn_catchments_reaches_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n\ncatch_values &lt;- unique(terra::values(sn_catchemnt_reaches))\npal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(catch_values)),catch_values,na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(sn_demDerived_reaches,sf::st_crs('EPSG:4326')),stroke = TRUE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#0327cd\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(sn_catchemnt_reaches, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n\n# ===========================\ngw_catchments_reaches &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",\"0\",\"gw_catchments_reaches_0.tif\",fsep = .Platform$file.sep))\ngw_catchments_reaches &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"gw_catchments_reaches_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n\ncatch_values &lt;- unique(terra::values(gw_catchments_reaches))\npal &lt;- leaflet::colorFactor(randomcoloR::distinctColorPalette(length(catch_values)),catch_values,na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(gw_catchments_reaches, colors = pal, opacity = 1,maxBytes=42*1024*1024) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n# ===========================\nHAND &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",\"0\",\"rem_zeroed_masked_0.tif\",fsep = .Platform$file.sep))\nHAND &lt;- terra::rast(file.path(path_to_fim4_unit_outputs,unit_to_map,\"branches\",branch_polys_selction$levpa_id,glue::glue(\"rem_zeroed_masked_{branch_polys_selction$levpa_id}.tif\"),fsep = .Platform$file.sep))\n\nHAND_values &lt;- terra::values(HAND)\npal &lt;- leaflet::colorNumeric(RColorBrewer::brewer.pal(5, \"Blues\"), c(min(HAND_values,na.rm = TRUE),max(HAND_values,na.rm = TRUE)),na.color = \"transparent\")\nmap1 &lt;- leaflet::leaflet(options = leaflet::leafletOptions(preferCanvas = TRUE)) %&gt;%\n  leaflet::addProviderTiles(\"CartoDB.Positron\",group = \"CartoDB.Positron\") %&gt;%\n  # leafem::addFeatures(data = sf::st_transform(wbd8_clp_inputs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1.2,color=\"#000000\") %&gt;%\n  leafem::addFeatures(data = sf::st_transform(nearby_hucs,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 1,color=\"#000000\") %&gt;%\n  leaflet::addRasterImage(HAND, colors = pal, opacity = 1,maxBytes=50*1024*1024) %&gt;%\n  leafem::addFeatures(data = sf::st_transform(branch_polys_selction,sf::st_crs('EPSG:4326')),stroke=TRUE,fill=FALSE,opacity = 1,fillOpacity = 1,weight = 2,color=\"black\") %&gt;%\n  # leaflet::fitBounds(branch_0_bbox[3],branch_0_bbox[2],branch_0_bbox[1],branch_0_bbox[4])\n  leaflet::fitBounds(aoi_bbox[3],aoi_bbox[2],aoi_bbox[1],aoi_bbox[4])\n# addLegend(pal = pal, values = terra::values(dem_meters), title = \"Elevation data for Santander (mts)\")\nmap1\nmapview::mapshot2(map1,file=file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep))\nmagick::image_write(\n  magick::image_read(\n    cropcircles::crop_circle(file.path(path_to_tmp_output,glue::glue(\"test.png\"),fsep = .Platform$file.sep), border_size = 4,just = \"center\")\n  ),file.path(path_to_tmp_output,glue::glue(\"test_cropped.png\"),fsep = .Platform$file.sep))\nunlink(file.path(path_to_tmp_output,\"*\",fsep = .Platform$file.sep))\n\n```"
  },
  {
    "objectID": "DS.html#how-to",
    "href": "DS.html#how-to",
    "title": "Digital Statistics",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "DM.html",
    "href": "DM.html",
    "title": "Data Management",
    "section": "",
    "text": "My FILE I/O page has seen quite a bit of scope creep, but the goal there was really to demonstrate that getting to that table/space data is really not that hard. However, the competent practitioner has a few larger standardized bites they typically encounter. Those are what you’ll find here.\n\n\n\n\n\n\n\n```{r}\npotential_files &lt;- file.path(path_to_fim_inputs,\"osm\",\"bridges\",glue::glue('huc_{dplyr::filter(hucs, grepl(vpu_unit, substr(hucs$huc8, 1, 2)))$huc8}_osm_bridges.gpkg'),fsep = .Platform$file.sep)\nfound_files &lt;- list.files(file.path(path_to_fim_inputs,\"osm\",\"bridges\",fsep = .Platform$file.sep),pattern = \"*gpkg$\",full.names = T)\nroadlines &lt;- do.call(dplyr::bind_rows, lapply(potential_files[basename(potential_files) %in% basename(found_files)],\n                                            sf::st_read)) %&gt;%\nsf::st_transform(sf::st_crs(\"EPSG:5070\"))\n```",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Data Management"
    ]
  },
  {
    "objectID": "DM.html#how-to",
    "href": "DM.html#how-to",
    "title": "Data Management",
    "section": "",
    "text": "```{r}\npotential_files &lt;- file.path(path_to_fim_inputs,\"osm\",\"bridges\",glue::glue('huc_{dplyr::filter(hucs, grepl(vpu_unit, substr(hucs$huc8, 1, 2)))$huc8}_osm_bridges.gpkg'),fsep = .Platform$file.sep)\nfound_files &lt;- list.files(file.path(path_to_fim_inputs,\"osm\",\"bridges\",fsep = .Platform$file.sep),pattern = \"*gpkg$\",full.names = T)\nroadlines &lt;- do.call(dplyr::bind_rows, lapply(potential_files[basename(potential_files) %in% basename(found_files)],\n                                            sf::st_read)) %&gt;%\nsf::st_transform(sf::st_crs(\"EPSG:5070\"))\n```",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Data Management"
    ]
  },
  {
    "objectID": "DEM.html",
    "href": "DEM.html",
    "title": "DEM",
    "section": "",
    "text": "Here I’ll demonstrate a few of the different access patterns I deploy to get DEM data. I’ll caveat that with the note that, if you’re looking to start with the LIDAR to get that DEM data you’re looking for the Point Cloud page."
  },
  {
    "objectID": "DEM.html#dem-1",
    "href": "DEM.html#dem-1",
    "title": "DEM",
    "section": "DEM",
    "text": "DEM\nLoading and pulling one in. Related: [[CA_Brushes.qmd]] CA_Brushes.qmd"
  },
  {
    "objectID": "DDT.html",
    "href": "DDT.html",
    "title": "Data Driven Thoughts",
    "section": "",
    "text": "There are three degrees of falsehoods faff, lies data, damn lies big data, and statistics cloud computing\n- ~~LEONARD COURTNEY~~ Jim\n\n(COURTNEY 1895)\nI hate computers[ [[20230917192549]] I hate computers], I hate standards, and I hate timelines. I would also be worthless without this magic rectangle and the digital footprint I’ve accumulated with it, the regular form and formatting of our world that makes what would otherwise be unpredictable, more certain; and you wouldn’t be reading this if there were no timelines. So here we are.\nHaving worked in many different interdisciplinary teams and with wizards of their respective crafts, it’s become apparent that one of the largest hurdles in really getting traction on a problem is that we are often happy to talk in the abstract and the theoretical until the rubber meets the road and suddenly data pipeines, semantics, and versioning become the largest monsters in the room. Moreover, all that theoretical progress we thought me made comes unraveled as we realize that, although we thought we were talking about the same concepts and using the same verbiage, what we were attempting to communicate got lost in translation somewhere.\n\n\n\n\nReferences\n\nCOURTNEY, LEONARD. 1895. “To My Fellow-Disciples at Saratoga Springs.” The National Review."
  },
  {
    "objectID": "CV.html#how-to",
    "href": "CV.html#how-to",
    "title": "Coverage",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Analysis",
      "Network",
      "Coverage"
    ]
  },
  {
    "objectID": "CO.html#how-to",
    "href": "CO.html#how-to",
    "title": "Coordinate Geometry",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Math",
      "Coordinate Geometry"
    ]
  },
  {
    "objectID": "CFIM.html",
    "href": "CFIM.html",
    "title": "The continental flood inundation mapping (CFIM) framework",
    "section": "",
    "text": "_c79e7106aded02b9872f976475e4fb1e.png\n\n\n\nExecutive Summary\nBest summerized by the developers themselves: “The continental flood inundation mapping (CFIM) framework is a high-performance computing (HPC)-based computational framework for the Height Above Nearest Drainage (HAND)-based inundation mapping methodology. Using the 10m Digital Elevation Model (DEM) data produced by U.S. Geological Survey (USGS) 3DEP (the 3-D Elevation Program) and the NHDPlus hydrography dataset produced by USGS and the U.S. Environmental Protection Agency (EPA), a hydrological terrain raster called HAND is computed for HUC6 units in the conterminous U.S. (CONUS). The value of each raster cell in HAND is an approximation of the relative elevation between the cell and its nearest water stream. Derived from HAND, a hydraulic property table is established to calculate river geometry properties for each of the 2.7 million river reaches covered by NHDPlus (5.5 million kilometers in total length). This table is a lookup table for water depth given an input stream flow value. Such lookup is available between water depth 0m and 25m at 1-foot interval. The flood inundation map can then be computed by using HAND and this lookup table based on the near real-time water forecast from the National Water Model (NWM) at the National Oceanic and Atmospheric Administration (NOAA).” - from https://cfim.ornl.gov/data/\n\n\nOther resources\n\nFOSSFlood",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "The continental flood inundation mapping (CFIM) framework"
    ]
  },
  {
    "objectID": "CBP.html",
    "href": "CBP.html",
    "title": "Dissecting a bar plot",
    "section": "",
    "text": "Affectionately called a Carson Plot in honor of Carson Pruitt as the fim developer who popularized it, the nided stacked bar plot provides a dense visual assessment of most major values in a confusion matrix.  The form also makes it quite direct to make visual comparisons between two different versions.  These plots are frequently used in the FIM team to help make versioning decisions and process improvements."
  },
  {
    "objectID": "CA_Brushes.html",
    "href": "CA_Brushes.html",
    "title": "Tools of mapping",
    "section": "",
    "text": "Following my Bob Ross “Paint the landscape” metaphor, there are a few choice brushes I reach for when you ask me to paint a picture.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Tools of mapping"
    ]
  },
  {
    "objectID": "CA_Brushes.html#dem",
    "href": "CA_Brushes.html#dem",
    "title": "Tools of mapping",
    "section": "DEM",
    "text": "DEM\nSource Files from data sources",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Tools of mapping"
    ]
  },
  {
    "objectID": "CA_Brushes.html#qgis",
    "href": "CA_Brushes.html#qgis",
    "title": "Tools of mapping",
    "section": "QGIS",
    "text": "QGIS\nwkp/country/wiki-scotland \n\nState Fitlers\n\"GEOID\" &lt; '57%' AND \"GEOID\"  NOT IN  ('02%','15%')\nstates &lt;- sf::st_read(file.path(base_data,“admin/tiger/TIGER2022/STATE/tl_2022_us_state/tl_2022_us_state.fgb”,fsep = .Platform$file.sep), query = glue::glue(“SELECT * FROM tl_2022_us_state WHERE GEOID &lt; ‘57%’ AND GEOID NOT LIKE ‘02%’ AND GEOID NOT LIKE ‘15%’”), quiet=!is_verbose)",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Tools of mapping"
    ]
  },
  {
    "objectID": "CA_Brushes.html#tidyterra",
    "href": "CA_Brushes.html#tidyterra",
    "title": "Tools of mapping",
    "section": "tidyterra",
    "text": "tidyterra",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Tools of mapping"
    ]
  },
  {
    "objectID": "CA_Brushes.html#leaflet",
    "href": "CA_Brushes.html#leaflet",
    "title": "Tools of mapping",
    "section": "Leaflet",
    "text": "Leaflet",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Tools of mapping"
    ]
  },
  {
    "objectID": "CA_Brushes.html#tmap",
    "href": "CA_Brushes.html#tmap",
    "title": "Tools of mapping",
    "section": "TMAP",
    "text": "TMAP\nhttps://rpubs.com/ials2un/dem_analysis",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Tools of mapping"
    ]
  },
  {
    "objectID": "CA_Brushes.html#earth-engine",
    "href": "CA_Brushes.html#earth-engine",
    "title": "Tools of mapping",
    "section": "Earth Engine",
    "text": "Earth Engine\n```{javascript}\n// https://www.youtube.com/watch?v=1ZkXZnw-tsc\n// https://helpx.adobe.com/photoshop/using/blending-modes.html\n\n//var elevation_data = ee.Image('CGIAR/SRTM90_V4').select('elevation');\nvar elevation_data = ee.Image('USGS/3DEP/10m').select('elevation');\n\nfunction radians(img) {\n  return img.toFloat().multiply(Math.PI).divide(180);\n}\nfunction hillshade(az, ze, slope, aspect) {\n  var azimuth = radians(ee.Image(az));\n  var zenith = radians(ee.Image(ze));\n  return azimuth.subtract(aspect).cos()\n    .multiply(slope.sin())\n    .multiply(zenith.sin())\n    .add(\n      zenith.cos().multiply(slope.cos()));\n}\n\nvar elevation_data_blur0 = elevation_data;\nvar elevation_data_blur7 = elevation_data.focalMean(7, \"circle\", \"pixels\", 1, null);\nvar elevation_data_blur15 = elevation_data.focalMean(15, \"circle\", \"pixels\", 1, null);\nvar elevation_data_blur30 = elevation_data.focalMean(30, \"circle\", \"pixels\", 1, null);\n\nvar slope_blur0 = ee.Terrain.slope(elevation_data_blur0);\nvar slope_blur7 = ee.Terrain.slope(elevation_data_blur7);\nvar slope_blur15 = ee.Terrain.slope(elevation_data_blur15);\nvar slope_blur30 = ee.Terrain.slope(elevation_data_blur30);\n\nvar aspect_blur0 = ee.Terrain.aspect(elevation_data_blur0);\nvar aspect_blur7 = ee.Terrain.aspect(elevation_data_blur7);\nvar aspect_blur15 = ee.Terrain.aspect(elevation_data_blur15);\nvar aspect_blur30 = ee.Terrain.aspect(elevation_data_blur30);\n\nvar hillshade_azimuth = 320; //270\nvar hillshade_elevation = 45;\nvar max_opacity_percent = 0.40;\nvar top_brush = 0.8;\nvar bottom_brush = 6;\n\nvar elevation_data_blur0_hs = ee.Terrain.hillshade(elevation_data_blur0,hillshade_azimuth,hillshade_elevation);\nelevation_data_blur0_hs = elevation_data_blur0_hs.mask(\n  (elevation_data_blur0_hs.subtract((1-max_opacity_percent)*255))\n  .divide(255).clamp(0,1)\n  );\n\nvar elevation_data_blur7_hs = ee.Terrain.hillshade(elevation_data_blur7,hillshade_azimuth,hillshade_elevation);\nelevation_data_blur7_hs = elevation_data_blur7_hs.mask(\n  (elevation_data_blur7_hs.subtract((1-max_opacity_percent)*255))\n  .divide(255).clamp(0,1)\n  );\n\nvar elevation_data_blur15_hs = ee.Terrain.hillshade(elevation_data_blur15,hillshade_azimuth,hillshade_elevation);\nelevation_data_blur15_hs = elevation_data_blur15_hs.mask(\n  (elevation_data_blur15_hs.subtract((1-max_opacity_percent)*255))\n  .divide(255).clamp(0,1)\n  );\n  \nvar elevation_data_blur30_hs = ee.Terrain.hillshade(elevation_data_blur30,hillshade_azimuth,hillshade_elevation);\nelevation_data_blur30_hs = elevation_data_blur30_hs.mask(\n  (elevation_data_blur30_hs.subtract((1-max_opacity_percent)*255))\n  .divide(255).clamp(0,1)\n  );\n\nMap.addLayer(ee.Image(1),{palette:['ffffff']});\nMap.addLayer(elevation_data_blur0,{palette:['000000','000000']},'raw dem',false);\nMap.addLayer(elevation_data_blur0_hs,{palette:['000000','000000']},'blur0 HS');\nMap.addLayer(elevation_data_blur7_hs,{palette:['000000','000000']},'blur7 HS');\nMap.addLayer(elevation_data_blur15_hs,{palette:['000000','000000']},'blur15 HS');\nMap.addLayer(elevation_data_blur30_hs,{palette:['000000','000000']},'blur30 HS');\n\nslope_blur0 = slope_blur0.mask(slope_blur0.divide(90).clamp(0,1));\nMap.addLayer(slope_blur0,{palette:['000000','000000']},'slope0 HS');\n\nvar elevation_data_white_highlights_hs = ee.Terrain.hillshade(elevation_data_blur0,hillshade_azimuth,hillshade_elevation);\nelevation_data_white_highlights_hs = elevation_data_white_highlights_hs.mask(\n  elevation_data_white_highlights_hs.divide(255));\nMap.addLayer(elevation_data_white_highlights_hs,{min:255*top_brush, max:255, palette:['ffffff','ffffff']},'slope highlights');\n\nslope_blur7 = slope_blur7.mask(\n  (slope_blur7.subtract((1-max_opacity_percent)*90))\n  .divide(90).clamp(0,1));\nMap.addLayer(slope_blur7,{palette:['000000','000000']},'slope7');\nslope_blur15 = slope_blur15.mask(\n  (slope_blur15.subtract((1-max_opacity_percent)*90))\n  .divide(90).clamp(0,1));\nMap.addLayer(slope_blur0,{palette:['000000','000000']},'slope15');\nslope_blur30 = slope_blur30.mask(\n  (slope_blur30.subtract((1-max_opacity_percent)*90))\n  .divide(90).clamp(0,1));\nMap.addLayer(slope_blur30,{palette:['000000','000000']},'slope30');\n\nvar slope_blur0_hs = ee.Terrain.hillshade(ee.Terrain.slope(elevation_data_blur0),hillshade_azimuth,hillshade_elevation);\nvar slope_blur0_hs_mask = ee.Image(slope_blur0_hs.divide(255)).interpolate(ee.List([0,0.25,0.75,1]),ee.List([1,0,0,1]),'mask');\nslope_blur0_hs = slope_blur0_hs.updateMask(slope_blur0_hs_mask);\nMap.addLayer(slope_blur0_hs,{palette:['000000','ffffff']},'slope_blur0_hs');\n\nvar slope_blur7_hs = ee.Terrain.hillshade(ee.Terrain.slope(elevation_data_blur7),hillshade_azimuth,hillshade_elevation);\nvar slope_blur7_hs_mask = ee.Image(slope_blur7_hs.divide(255)).interpolate(ee.List([0,0.25,0.75,1]),ee.List([1,0,0,1]),'mask');\nslope_blur7_hs = slope_blur7_hs.updateMask(slope_blur7_hs_mask);\nMap.addLayer(slope_blur7_hs,{palette:['000000','ffffff']},'slope_blur7_hs');\n\nvar low_shadows_mask = elevation_data_blur0.interpolate(ee.List([20,70]),ee.List([0.4,0]),'clamp');\nMap.addLayer(elevation_data_blur0.updateMask(low_shadows_mask),{palette:['000000','ffffff']},'low lands');\n```\n\n\n\n_8e7ee7efe11d80763ed7086757b34621.png\n\n\n```{javascript}\n// Define a function to convert from degrees to radians.\nfunction radians(img) {\n  return img.toFloat().multiply(Math.PI).divide(180);\n}\n\n// Define a function to compute a hillshade from terrain data\n// for the given sun azimuth and elevation.\nfunction hillshade(az, ze, slope, aspect) {\n  // Convert angles to radians.\n  var azimuth = radians(ee.Image(az));\n  var zenith = radians(ee.Image(ze));\n  // Note that methods on images are needed to do the computation.\n  // i.e. JavaScript operators (e.g. +, -, /, *) do not work on images.\n  // The following implements:\n  // Hillshade = cos(Azimuth - Aspect) * sin(Slope) * sin(Zenith) +\n  //     cos(Zenith) * cos(Slope)\n  return azimuth.subtract(aspect).cos()\n    .multiply(slope.sin())\n    .multiply(zenith.sin())\n    .add(\n      zenith.cos().multiply(slope.cos()));\n}\n\nvar elevation = ee.Image(\"USGS/SRTMGL1_003\");\n// var slopeDegrees = ee.Algorithms.Terrain(elevation).select('slope').abs();\nvar slopeDegrees = ee.Terrain.slope(elevation).select('slope');\nvar slopePercent = ee.Image(slopeDegrees.tan()).multiply(100).abs();\nvar aspect = ee.Terrain.aspect(elevation).select('aspect');\n\nvar slopereclass = ee.Image(slopePercent)\n  .where(slopePercent.gt(0).and(slopePercent.lte(5)), 10)\n  .where(slopePercent.gt(5).and(slopePercent.lte(20)), 20)\n  .where(slopePercent.gt(20).and(slopePercent.lte(40)), 30)\n  .where(slopePercent.gt(40), 40);\nvar aspectreclass = ee.Image(aspect)\n  .where(aspect.gt(0).and(aspect.lte(22.5)), 1)\n  .where(aspect.gt(22.5).and(aspect.lte(67.5)), 2)\n  .where(aspect.gt(67.5).and(aspect.lte(112.5)), 3)\n  .where(aspect.gt(112.5).and(aspect.lte(157.5)), 4)\n  .where(aspect.gt(157.5).and(aspect.lte(202.5)), 5)\n  .where(aspect.gt(202.5).and(aspect.lte(247.5)), 6)\n  .where(aspect.gt(247.5).and(aspect.lte(292.5)), 7)\n  .where(aspect.gt(292.5).and(aspect.lte(337.5)), 8)\n  .where(aspect.gt(337.5).and(aspect.lte(360)), 1);\nvar colorTerrain = aspectreclass.add(slopereclass);\n\n// Map.addLayer(slopeDegrees,{},'slope (degrees)',false);\n// Map.addLayer(slopePercent,{},'slope (percent)',false);\n// Map.addLayer(slopereclass,{},'slope reclass',false);\n// Map.addLayer(aspect,{},'aspect',false);\n// Map.addLayer(aspectreclass,{},'aspect reclass',false);\nMap.addLayer(colorTerrain,{\n  min:19,\n  max:48,\n  palette:['#a1a1a1','#98b581','#72a890','#7c8ead','#8c75a0','#b47ba1',\n  '#cb8b8f','#c5a58a','#bdbf89','#8dc458','#3dab71','#5078b6','#77479d',\n  '#c04d9c','#e76f7a','#e2a66c','#d6db5e','#84d600','#84d600','#0068c0',\n  '#6c00a3','#ca009c','#ff5568','#ffab47','#f4fa00']});\nvar x = 0.1;\n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 1))),{color: \"#2f6ecc\", width: 0.002}); \n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 2))),{color: \"#2f6ecc\", width: 0.002}); \n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 3))),{color: \"#2f6ecc\", width: 0.002});\n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 4))),{color: \"#2f6ecc\", width: 0.002}); \n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 5))),{color: \"#2f6ecc\", width: 0.002});\n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 6))),{color: \"#2f6ecc\", width: 0.002}); \n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 7))),{color: \"#2f6ecc\", width: 0.002});\n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 8))),{color: \"#2f6ecc\", width: 0.002}); \n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 9))),{color: \"#2f6ecc\", width: 0.002});\n// Map.addLayer(ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 10))),{color: \"#2f6ecc\", width: 0.002});\n\nvar riv1 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 1))).style({color: \"#2f6ecc\", width: 1}); \nvar riv2 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 2))).style({color: \"#2f6ecc\", width: 1.2}); \nvar riv3 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 3))).style({color: \"#2f6ecc\", width: 1.4});\nvar riv4 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 4))).style({color: \"#2f6ecc\", width: 1.6}); \nvar riv5 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 5))).style({color: \"#2f6ecc\", width: 1.8});\nvar riv6 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 6))).style({color: \"#2f6ecc\", width: 2}); \nvar riv7 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 7))).style({color: \"#2f6ecc\", width: 2.2});\nvar riv8 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 8))).style({color: \"#2f6ecc\", width: 2.4}); \nvar riv9 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 9))).style({color: \"#2f6ecc\", width: 2.6});\nvar riv10 = ee.FeatureCollection(Rivers.filter(ee.Filter.eq('RIV_ORD', 10))).style({color: \"#2f6ecc\", width: 2.8});\nMap.addLayer(riv1); \nMap.addLayer(riv2); \nMap.addLayer(riv3); \nMap.addLayer(riv4); \nMap.addLayer(riv5); \nMap.addLayer(riv6); \nMap.addLayer(riv7); \nMap.addLayer(riv8); \nMap.addLayer(riv9); \nMap.addLayer(riv10); \n\nMap.addLayer(BIA.union().style({color: '#8a5b19', fillColor: '#8a5b1935',width:2}));\nMap.addLayer(Aqu.union().style({color: '#1f4bab', fillColor: '#1f4bab45',width:4}));\nMap.addLayer(Cities.style({color: '#ecf230'}));\nMap.addLayer(SDWells.style({color: '#22e352'}));\nMap.addLayer(Uran.style({color: '#cf3221'}));\nvar State = ee.FeatureCollection(\"TIGER/2018/States\");\nvar filter = State.filter(ee.Filter.inList(\"NAME\",['South Dakota']).not());\nMap.addLayer(filter.geometry().dissolve(), {opacity:0.3}, 'State');\nvar legend = ui.Panel({\n  style: {\n    position: 'bottom-right',\n    padding: '8px 15px'\n  }\n});\n\n// Create legend title\nvar legendTitle = ui.Label({\n  value: 'Uranium mines, Wells, Waterways, Terrain, and Populations in the Black Hills of South Dakota',\n  style: {\n    fontWeight: 'bold',\n    fontSize: '18px',\n    margin: '0 0 4px 0',\n    padding: '0',\n    maxWidth:'390px'\n  }\n});\n \n// Add the title to the panel\nlegend.add(legendTitle);\n \n// Creates and styles 1 row of the legend.\nvar makeRow = function(color, name) {\n \n      // Create the label that is actually the colored box.\n      var colorBox = ui.Label({\n        style: {\n          backgroundColor: '#' + color,\n          // Use padding to give the box height and width.\n          padding: '8px',\n          margin: '0 0 4px 0'\n        }\n      });\n \n      // Create the label filled with the description text.\n      var description = ui.Label({\n        value: name,\n        style: {margin: '0 0 4px 6px'}\n      });\n \n      // return the panel\n      return ui.Panel({\n        widgets: [colorBox, description],\n        layout: ui.Panel.Layout.Flow('horizontal')\n      });\n};\n//  Palette with the colors\nvar palette =['cf3221', '22e352','ecf230','1f4bab','8a5b19'];\n \n// name of the legend\nvar names = ['Uranium mines (Surficial)','Water wells','Major cities','Inyan Kara Group aquifers','American Indian Land Area Representation'];\n \n// Add color and and names\nfor (var i = 0; i &lt; 5; i++) {\n  legend.add(makeRow(palette[i], names[i]));\n}  \nvar legendSources = ui.Label({\n  value: 'Sources: The BIA, USGS, SRTM, and the South Dakota Department of Agriculture & Natural Resources',\n  style: {\n    fontWeight: 'normal',\n    fontSize: '12px',\n    margin: '0 0 4px 0',\n    padding: '0',\n    maxWidth:'390px'\n  }\n});\n\n// add legend to map (alternatively you can also print the legend to the console)\n\n\n\nvar logo = ee.Image('users/JamesMColl/LargeSlopeAspect').visualize({\n    bands:  ['b1', 'b2', 'b3'],\n    min: 0,\n    max: 255\n    });\n\nvar thumb = ui.Thumbnail({\n    image: logo,\n    params: {\n        dimensions: '398x382',\n        format: 'png'\n        },\n    style: {height: '398px', width: '382px',padding :'0'}\n    });\nlegend.add(thumb);\nMap.add(legend);\n// var toolPanel = ui.Panel(thumb, 'flow', {width: '300px'});\n// ui.root.widgets().add(toolPanel);\nlegend.add(legendSources);\nMap.setCenter(-103.061848427201,43.811618572829055, 9);\n```\n\n\n\n_b7fc57afd097034b03732421b557206b.png",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Tools of mapping"
    ]
  },
  {
    "objectID": "BLE.html",
    "href": "BLE.html",
    "title": "BLE",
    "section": "",
    "text": "Probably delete?\nBase level engineering, as the name implies, is a baseline solution to understanding the risk of a location to flood recurrence intervals. To do this, the Federal Emergency Management Agency contracts out engineering firms to develop what were traditionally rather coarse one-dimensional HEC-RAS modeling efforts with a few simple water surface profiles which are intended to represent common high flow magnitudes. These modeling efforts have since evolved into more robust one and two dimensional HEC-RAS rain-on-grids with multiple water surface profiles. These resulting maps offer a solid credible engineering analysis and modeling foundation for communities to start planning and enforcing development and insurance codes through the establishment of a Base Flood Elevation (a 100 year water surface elevation) which forms the basis for Flood Insurance Rate Maps as provided by the National Flood Insurance Program. In short, Base Level Engineering provides the modeling surface off which the elevations are pulled to create the mapping surface off which insurance enforcement operates.\nhttps://www.fema.gov/sites/default/files/documents/fema_state-quick-guide-base-level-engineering.pdf\nWhile BLE represents a standard, there are variations on that standard that make some models more robust than others. The “baseline standard” typically involves a series of hydrolic and hydraulic analyses that follow the rough following colloquilal style: - [ ] The input DEM data for the terrain must meet vertical accuracy standards (3DEP is appropriate) - [ ] Cell size no greater than 10 meters - [ ] Full (at least 90%) spatial coverage of the domain of interest - [ ] Hydrology using NRCS rainfall-runoff methods to define upstream hydrograph - [ ] Calibrated to stream gage where appropriate - [ ] Typically identify 6 storm amounts (volumes) corresponding to… - [ ] 10%: (once every 10 years) - [ ] 4%: (once every 25 years) - [ ] 2%: (once every 50 years) - [ ] 1%: (once every 100 years) - [ ] 0.2%: (once every 500 years) - [ ] Hydraulics from one (and since 2020 two dimensional HEC-RAS) models - [ ] roughness from landcover reclassification1 - [ ] Automated hydraulic feature placement followed by manual engineering judgment tweaks. - [ ] Calibrated where feasible - [ ] Deliverable and Mapped outputs - [ ] Hydraulic models - [ ] Mapped surfaces representing floodplanedepth grids - [ ] ancillary analyses, relevant metadata, and reports as prescribed\n\n\nWhat BLE offers: * Solid Credible engineering analysus and modeling for local comminities * Both a map and a model * Can be adopted bt BLE is not a FIRM (BLE is a non-regulatory product) Can be though… \nUsing BLE data as regulatory sdata for floodplain management Use BLE as a higher standard\nSpesific to Arkansas… We have to include surveyors because they are wery of turnnig a WSE into depth More senerio planning BLE &gt; FEMA Effective Maps Arkansa minimum: “Alert residence to its presence”\nWe capitalize on all the hard work from across several databases but most formally FEMA region 7 to make there archival HEC-RAS models more immediatly accountable and useable across a range of workflows using the RRASSLER software."
  },
  {
    "objectID": "BLE.html#footnotes",
    "href": "BLE.html#footnotes",
    "title": "BLE",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n https://www.hec.usace.army.mil/confluence/rasdocs/r2dum/latest/developing-a-terrain-model-and-geospatial-layers/creating-land-cover-mannings-n-values-and-impervious-layers↩︎"
  },
  {
    "objectID": "BA.html#how-to",
    "href": "BA.html#how-to",
    "title": "Band Index",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing",
      "Band Index"
    ]
  },
  {
    "objectID": "AR.html#how-to",
    "href": "AR.html#how-to",
    "title": "Augmented Reality",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Applications",
      "Grab bag",
      "Augmented Reality"
    ]
  },
  {
    "objectID": "AC.html#how-to",
    "href": "AC.html#how-to",
    "title": "Atmosphere Correction",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing",
      "Atmosphere Correction"
    ]
  },
  {
    "objectID": "3D.html#how-to",
    "href": "3D.html#how-to",
    "title": "3D Analysis",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "3D Analysis"
    ]
  },
  {
    "objectID": "AJ.html#how-to",
    "href": "AJ.html#how-to",
    "title": "Spatial Adjustment",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Math",
      "Spatial Adjustment"
    ]
  },
  {
    "objectID": "ARCO.html",
    "href": "ARCO.html",
    "title": "Analysis-ready Cloud Optimized",
    "section": "",
    "text": "How many people really need to wrangle 1TB of data on a daily basis? I feel like the word-count / hype-calories expended on “cloud native geospatial” dramatically exceeds the number of organizations that can get value from the architecture. Which is perhaps why ESRI just keeps beavering along with incremental improvements to their desktop oriented, cloud immigrant architecture. - Paul Ramsey\nWhat is architecture but a way to deliver data? I’d argue why ESRI is able to beaver along is because the number of bodies able to productively contribute to the dam is dwindling and keeping the dam maintained is a foundational skillset that needs more accessible pathways.\nEveryone can/could/should/will benefit from analysis ready, cloud optimized data. It’s the perfect position to be the one deciding what that analysis is.\n“Analysis-ready Cloud Optimized” (ARCO) is an acronym describing the (re)structuring of data for on-the-fly (web based) access to (geo)spatial data. This formatting . As the Cloud-Native Geospatial Foundation accurately points out however, “There is no one-size-fits-all approach to cloud-optimized data.” h intended to advance that but the one-two punch"
  },
  {
    "objectID": "ARCO.html#loose-reflections-on-where-i-think-this-is-going",
    "href": "ARCO.html#loose-reflections-on-where-i-think-this-is-going",
    "title": "Analysis-ready Cloud Optimized",
    "section": "Loose reflections on where I think this is going?",
    "text": "Loose reflections on where I think this is going?"
  },
  {
    "objectID": "Atlas14.html",
    "href": "Atlas14.html",
    "title": "NOAA Atlas 14",
    "section": "",
    "text": "NOAA Atlas 14"
  },
  {
    "objectID": "BD.html",
    "href": "BD.html",
    "title": "Big Data",
    "section": "",
    "text": "Big data is a tough term to define in concrete terms because of the ever evolving landscape of technology and how pinned to personal skillsets the term is. My favored definition of the term is “data that is too large to process given the tools at your disposal” as it recognizes that that aspect of skill and the platform you are working with. After all, if we look at the most common tools deployed today, shapefiles might be able to hold no more than 2 gigabytes of data, and Excel tops out at a little more than 2 million rows. While that might seem feasible to you, that’s tiny to someone who is manipulating video files winch can reach 100’s of gigs when stored raw, and the point clouds of measurements top out at terabytes for a single collection, and the archive of public satellite data accrued by Google Earth Engine exceeds petabytes of data."
  },
  {
    "objectID": "BD.html#defining",
    "href": "BD.html#defining",
    "title": "Big Data",
    "section": "Defining",
    "text": "Defining\nVolume Velocity Veracity Variety/Variability Value Validity Volatility Visualization"
  },
  {
    "objectID": "BD.html#example",
    "href": "BD.html#example",
    "title": "Big Data",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "BD.html#how-to",
    "href": "BD.html#how-to",
    "title": "Big Data",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "CB.html#how-to",
    "href": "CB.html#how-to",
    "title": "Composite Bands",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing",
      "Composite Bands"
    ]
  },
  {
    "objectID": "CF.html#how-to",
    "href": "CF.html#how-to",
    "title": "Conflation",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Conflation"
    ]
  },
  {
    "objectID": "DDD.html",
    "href": "DDD.html",
    "title": "Data Driven Documentation",
    "section": "",
    "text": "The world is a loud place, filled with a lot of garbage. In the battle for quality vs. quantity, quality seems to have lost and that is unfortunate. Not that the “publish or perish” paradigm we have constructed doesn’t have some positives, or that the speed of iteration foisted by Silicon Valley hasn’t improved the general condition of humanity (Computers are useful and time saving), but in general it would be phenomenal if we could move that needle back towards quality a bit. I find this to have been a losing battle for a number of reasons.\nHaving spent the time struggling and soul searching, I’ve assembled the monstrosity of a working system you’ve found here. Blending zettlekasten concepts, a Diataxis-styled documentation style, the self-sold confidence of a modified PARA method, and my years of experience creating file bloat to generate this; the newest iteration of my digital footprint. Hopefully working within this framing will reduce the friction I experience with some of those pain points, and provide me with a landscape against which I can have some sort of opinion. Ideally re-framing these opinions into useful output is equally as seamless. Having that knowledge in a synthesized and authoritative form should also allow me to more responsively provide answers and meaningful assistance. If not, at the very least my notes look fancy and I have a consistent starting point to begin [[20230908085243]] The hellscape that is my day :)",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Data Driven Documentation"
    ]
  },
  {
    "objectID": "DDD.html#executable-writing",
    "href": "DDD.html#executable-writing",
    "title": "Data Driven Documentation",
    "section": "Executable writing",
    "text": "Executable writing\nThere’s a lot of dark corners in the maze between head work and hand work, and the best way I’ve found of truly overcoming that (as opposed to stopgap band-aids and unreplicable solutions), is to make the entire process data driven and executable. In short this means:\n\nThe friction from source to citation is non-existent: Zotero and Zettlr accomplish most of this very nicely/seamlessly as outlined in Note taking.\nThere’s a place for virtually every word I need to write. This means that any thought temporarily thrown into [[20220630144912]] 02 Now will eventually have to percolate through the rest of my system.\nComputational reproducibility is a function of the amount of abstraction I’m willing to do, but is generally very high. See:\n\nHow I install GIS.\nHow I install and use docker.\n\nCommunicating those results is also highly reproducible since this site is based on Quarto, and so is a data driven, fully contained static site which capitalizes on cloud-first patterns and puts documentation in line.\nAnd is almost entirely scaffolded, constructed, and reliant on FOSS!\n\nFollowing this pattern, so long as my notes are up to date with what I am currently working on, small changes trickle out immediately to accrete on top of my knowledge base. That base can be easily tweaked, remixed, and expressed as a webpage or publication-ready format. Pushing this site is a trivia (1 hour long) step. Great success! Now if only it were really that easy…",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Data Driven Documentation"
    ]
  },
  {
    "objectID": "DDD.html#finding-knowledge",
    "href": "DDD.html#finding-knowledge",
    "title": "Data Driven Documentation",
    "section": "Finding Knowledge",
    "text": "Finding Knowledge\n\nFollowing the standard “literature_notes” ingest process outlined in Installing Zotero.\nTake a Question/Evidence/Conclusion framing to the literature_note. Taking the concept handle (a memorable noun phrase representing a more complex idea, probably the conclusion), frame the resource as a “zettle”.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Data Driven Documentation"
    ]
  },
  {
    "objectID": "DDD.html#adding-knowledge",
    "href": "DDD.html#adding-knowledge",
    "title": "Data Driven Documentation",
    "section": "Adding Knowledge",
    "text": "Adding Knowledge\n\nMinimally extending content gratefully pilfered from Andyʼs working notes\n\nZettles should be “atomic” in that they address or concern themselves with with a single aspect of a concept handle:\n\nZettle notes, in increasingly complex stages of development:\n\nStubs implicitly defined through backlinks\nBridge notes narrowly relate two adjacent terms\nZettles: precise, narrow declarative notes\n\nPrefer note titles with complete phrases to sharpen claims (e.g. Human channel capacity increases with stimulus dimensionality)\nPrefer sharp titles (Prefer note titles with complete phrases to sharpen claims), and positive framings (Prefer positive note titles to promote systematic theory).\nSometimes these are framed as questions, when evidence is too inconclusive to frame sharply (e.g. To what extent is exceptional ability heritable?, To what extent can application prompts supplant recall prompts in the mnemonic medium?)\n\n\nhigher-level APIs (Concept handle note titles are like APIs)\nnotes abstracting over many other notes, (e.g. Educational games are a doomed approach to creating enabling environments, Reading texts on computers is unpleasant)\n“Outline notes”, e.g. MOC\nConcept handle API’s can be loosely structured following a Diataxis framing",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Data Driven Documentation"
    ]
  },
  {
    "objectID": "DDD.html#who-am-i-writing-to",
    "href": "DDD.html#who-am-i-writing-to",
    "title": "Data Driven Documentation",
    "section": "Who am I writing to?",
    "text": "Who am I writing to?\n[[20230923033512]] Know your audience.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Data Driven Documentation"
    ]
  },
  {
    "objectID": "DDD.html#assembling-a-moc-or-paper",
    "href": "DDD.html#assembling-a-moc-or-paper",
    "title": "Data Driven Documentation",
    "section": "Assembling a MOC or Paper",
    "text": "Assembling a MOC or Paper\n```{md}\nhome                      &lt;- landing page\n├── tutorial              &lt;- landing page\n│   └── part 1            &lt;- landing page\n├── how-to guides         &lt;- landing page\n│   ├── install\n│   └── deploy\n├── reference             &lt;- landing page\n│   ├── commandline tool\n│   └── available endpoints\n└── explanation           &lt;- landing page\n    ├── best practice recommendations\n    └── performance\nPaper\n├── Introduction\n├── Background       &lt;- literature_notes,reference,explanation\n├── Methods          &lt;- how-to guides,reference,explanation \n├── Results\n├── Discussion       &lt;- reference,explanation \n└── Conclutions      \n```\n\nUndirected\n\nWrite durable notes continuously while reading and thinking. (Evergreen note-writing as fundamental unit of knowledge work)\nEach time you add a note, add a link to it to an outline, creating one if necessary (Create speculative outlines while you write).\nEventually, you’ll feel excited about fleshing out one of those outlines. (Let ideas and beliefs emerge organically)\nWrite new notes to fill in missing pieces of the outline.\nConcatenate all the note texts together to get an initial manuscript\nRewrite it.\n\n\n\nDirected\n\nReview notes related to your topic (and a step or two beyond those—Notes should surprise you)\nWrite an outline\nAttach existing notes to each point in the outline; write new notes as needed.\nConcatenate all the note texts together to get an initial manuscript\nRewrite it.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Data Driven Documentation"
    ]
  },
  {
    "objectID": "DDD.html#footnotes",
    "href": "DDD.html#footnotes",
    "title": "Data Driven Documentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“we are our own worst enemies”↩︎\nsome call this “learning exhaust”, a phrasing I agree with because who wants to suck a tailpipe?↩︎",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Data Driven Documentation"
    ]
  },
  {
    "objectID": "DE.html#how-to",
    "href": "DE.html#how-to",
    "title": "Data Engineering",
    "section": "How to",
    "text": "How to\n[[20240603032529]] RASSTAC paper"
  },
  {
    "objectID": "DF.html",
    "href": "DF.html",
    "title": "Data Flow Diagram",
    "section": "",
    "text": "Visual tracing of a workflow or program can add a lot of understanding to your knowledge and a robust representation of the inner machinations of the process (so that they are less of an enigma), and more importantly provides a concrete and explainable method for describing the process.  Although there are a lot of facets that might be of interest, one of the most universal concerns is what data goes whereas a primarily data-focused team we’d all benefit from a more explainable process with regard to the data as opposed to\nCall out steps in the process\nAlthough there are a number of ways we could diagram out a workflow, having a standardized and agreed upon format will give the team a more distinct visual language and brand to call back to. We’ll lean on others who have put a bit more thought to the metacognitive aspects of this and follow the game and sarsen format with the light tweaks Representing a data source Representing a process Other considerations Design size and flow Many of the workflows we deploy are complex or involved\nAs a core data science team and the foundation for many workflows it’s critical that we have a solid, authoratative, and united view of where what data can be found (e:g: Lynker Spatial Data), who is using that data, and how it is being used. To aid the objectives of the last pieces, we as a team ought to adopt a standardized pattern and language to describe the data flow, or a data flow diagram design language."
  },
  {
    "objectID": "DF.html#theory",
    "href": "DF.html#theory",
    "title": "Data Flow Diagram",
    "section": "Theory",
    "text": "Theory\nAfter a cursory search and discussion, we’ve decided to follow the Gane and Sarson Data Flow Diagram stylings (named after Chris Gane and Trish Sarson). These leaders developed a simplistic and comprehensible system to diagramming out processes that is intuitive, quick to grasp, and composed of only 4 main points. To place these resources in a consolidated place, we’ll gratefully pilfer the critical bits from (“Data Flow Diagram Symbols” n.d.)\n\nAll data flow diagrams include four main elements: entity, process, data store and data flow. External Entity – Also known as actors, sources or sinks, and terminators, external entities produce and consume data that flows between the entity and the system being diagrammed. These data flows are the inputs and outputs of the DFD. Since they are external to the system being analyzed, these entities are typically placed at the boundaries of the diagram. They can represent another system or indicate a subsystem. Process – An activity that changes or transforms data flows. Since they transform incoming data to outgoing data, all processes must have inputs and outputs on a DFD. This symbol is given a simple name based on its function, such as “Ship Order,” rather than being labeled “process” on a diagram. In Gane-Sarson notation, a rectangular box is used and may be labeled with a reference number, location of where in the system the process occurs and a short title that describes its function. Processes are typically oriented from top to bottom and left to right on a data flow diagram. Data Store – A data store does not generate any operations but simply holds data for later access. Data stores could consist of files held long term or a batch of documents stored briefly while they wait to be processed. Input flows to a data store include information or operations that change the stored data. Output flows would be data retrieved from the store. Data Flow – Movement of data between external entities, processes and data stores is represented with an arrow symbol, which indicates the direction of flow. This data could be electronic, written or verbal. Input and output data flows are labeled based on the type of data or its associated process or data store, and this name is written alongside the arrow.\n\nWhile you can replicate these shapes in almost any software, we’ll endorse draw.io as it includes both a freely accessible web interface or a downloadable desktop application"
  },
  {
    "objectID": "DF.html#practice",
    "href": "DF.html#practice",
    "title": "Data Flow Diagram",
    "section": "Practice",
    "text": "Practice\nTry to stick to the 4 elements listed above. Not that you can’t change the template up should you choose to, but a good template helps scaffold thinking out and gives your output a more consistent form which aids the design language.\nStylistic notes: - [ ] No acronyms - [ ] We prefer curvy arrows. Those can be toggled on here:\n\n\n\n_e6905e338e7eb4fb8d5e85fb123904d9.png"
  },
  {
    "objectID": "DF.html#sources",
    "href": "DF.html#sources",
    "title": "Data Flow Diagram",
    "section": "Sources",
    "text": "Sources\n(“Data Flow Diagram Symbols” n.d.)"
  },
  {
    "objectID": "DR.html#how-to",
    "href": "DR.html#how-to",
    "title": "Directions",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Analysis",
      "Network",
      "Directions"
    ]
  },
  {
    "objectID": "ED.html#how-to",
    "href": "ED.html#how-to",
    "title": "Editing",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Editing"
    ]
  },
  {
    "objectID": "FAA.html",
    "href": "FAA.html",
    "title": "FAA licensure",
    "section": "",
    "text": "Drone law might be the only thing moving faster than technology, and while the state of the field might be up in the air, there are a few constants we can latch on to…",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "FAA licensure"
    ]
  },
  {
    "objectID": "FIM_history.html#the-history-of-fim-according-to-jim",
    "href": "FIM_history.html#the-history-of-fim-according-to-jim",
    "title": "So long as you get the GIS&T of it",
    "section": "The History of FIM according to Jim",
    "text": "The History of FIM according to Jim\n\n\n\n\n\n\n\nGoals:\n\ndescribe the history and linage of a few of the more popular tools and libraries used to map flooding.\nProvide a high level overview of the methodologies.\n\nOutcomes:\n\nA better understanding of why we use the tools and methods we use\nMore context on the approaches we take to FIM\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\nSlide layout:  default     Items linked/bordered in green are cited in the tooltip on hover.\n: interactive elements     Items linked/bordered in blue are hyperlinked to relevant resources.\n\n\nAlthough I was neither there nor the victor, I still get to write the history."
  },
  {
    "objectID": "FIM_history.html#presentation-navigation-tips",
    "href": "FIM_history.html#presentation-navigation-tips",
    "title": "So long as you get the GIS&T of it",
    "section": "Presentation Navigation Tips",
    "text": "Presentation Navigation Tips\n\n\n\nSlide layout : This deck has one axis, use any key to advance.\n: This deck is narrative-oriented\n\nItems linked/bordered in green are cited in the tooltip on hover.\nItems linked/bordered in blue are hyperlinked to relevant resources.\n\n Photos are Allowed |  Questions are Encouraged\n: ~5 minutes | Last updated:08/25/2025 20:44:34\n! PLEASE !\nInterrupt me and ask questions or clarifications.\nI’m here to talk with you now, not to these slides.\n\n\nControl tips\n\nMy preferred (FOSS) flavor of slidedecks, revealjs, has intuitive but none the less unconventional PowerPoint presentation controls:\n\nSlides dynamically resize to use the entirety of the browser window, but you can still fullscreen with F.\n\nThis slide has a red border indicating the content extent.\n\nSlide navigation is mode dependent. If there are vertical slides, press space, N, or the down arrow key, not the right arrow to advance slides\nPress M to open to the menu, Press O for the slide deck overview, Press B to black out the presentation screen, Press S for a speaker view.\nYou can use the chalkboard to freemouse/touchpad draw.\nSlides should render as designed1 but you can press Alt/Opt + click on the slide to zoom in. Increase text size with Alt/Opt + +, Alt/Opt + - to decrease, and Alt/Opt + 0 reset to the default scale.\nPress C to declare victory and head home.2\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\n\n\n\n\nSpeaker notes\n\ntested in an updated version of google chromeA favored quote from one of my giants: Dr. David Maidment"
  },
  {
    "objectID": "FIM_history.html#we-write-the-next-steps",
    "href": "FIM_history.html#we-write-the-next-steps",
    "title": "So long as you get the GIS&T of it",
    "section": "We write the next steps!",
    "text": "We write the next steps!\n\n\n\nGoals:\n\nDescribe the history and linage of a few of the more popular tools and libraries used to map flooding.\nProvide a high level overview of the methodologies.\n\nOutcomes :\n\nA better understanding of why we use the tools and methods we use.\nMore context on the approaches we take to FIM.\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\nSuper cool."
  },
  {
    "objectID": "FOSSFlood.html",
    "href": "FOSSFlood.html",
    "title": "FOSSFlood",
    "section": "",
    "text": "Executive Summary\nAn accessible Free and Open Source Software solution for Flood Impact Mapping based on the CFIM data. FOSSFlood is a Free & Open Source, GUI driven application built on a combination of R, Leaflet, and Shiny that removes the barriers of entry and allows anyone to take National Water Model forecasts and transform them into actionable intelligence.\n\n\nInstallation\n\ndownload this repository\nunzip it\ndouble click the RunMe.hta,\nspecify an area (via zip code or other common flood units), and\nexplore the outputs!\n\n\n\nTutorials\nFind the FOSSFlood User manual here\n\n\nOther resources:\n\nCFIM"
  },
  {
    "objectID": "FrankenFIM.html",
    "href": "FrankenFIM.html",
    "title": "FrankenFIM",
    "section": "",
    "text": "FrankenFIM\n#   ______               _             ______ ________  ___\n#   |  ___|             | |            |  ___|_   _|  \\/  |\n#   | |_ _ __ __ _ _ __ | | _____ _ __ | |_    | | | .  . |\n#   |  _| '__/ _` | '_ \\| |/ / _ \\ '_ \\|  _|   | | | |\\/| |\n#   | | | | | (_| | | | |   &lt;  __/ | | | |    _| |_| |  | |\n#   \\_| |_|  \\__,_|_| |_|_|\\_\\___|_| |_\\_|    \\___/\\_|  |_/\n#\n#⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n#⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⢁⡈⠻⣦⣀⠺⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀\n#⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡴⠋⣀⣴⣿⣿⣦⣀⠙⢦⡈⠀⠀⠀⠀⠀⠀⠀⠀⠀\n#⠀⠀⠀⠀⠀⠀⠀⣠⠴⠋⣠⣾⣿⣿⣿⣿⣿⣿⣷⣄⠙⠦⣄⠀⠀⠀⠀⠀⠀⠀\n#⠀⠀⠀⠀⠀⠐⠚⠁⣤⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣤⠈⠓⠂⠀⠀⠀⠀⠀\n#⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿           O__,     O__,⠀⠀⠀⠀⠀⠀⠀⠀\n#⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⠉⣉⡉⢹⣿⣿⣿⣿⣿⣿⣿⣿       /'._|\\/______|\\_/.'\\⠀⠀⠀⠀⠀⠀⠀⠀\n#⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⠀⡿⠇⠘⠿⣿⣿⣿⣿⠿⠛⠻       \\____/_____________/⠀⠀⠀⠀⠀⠀⠀⠀\n#⠀⠀⣀⣤⠶⠟⢷⣦⡉⠛⢀⣴⠾⠻⣶⣌⠙⠋⣡⡶⠟⢷⣦⡀⢀⣴⠾⠻⣶⣌⠙⠋⣡⡶⠟⢷⣦⣌⠙⠋⣡⡴⠻⣶⣌⠙⠋⣡⡶⠟⢷⣦⡀⠀⢀⣠⡴⠀\n#⠀⠈⠋⠁⠀⠀⠀⠈⠛⠛⠛⠁⠀⠀⠀⠙⠛⠛⠋⠀⠀⠀⠈⠛⠛⠛⠁⠀⠀⠀⠙⠛⠛⠋⠀⠀⠀⠈⠛⠛⠛⠁⠀ ⠀⠀⠙⠛⠛⠋⠀⠀⠀⠈⠛⠛⠛⠁⠀⠀ \n#⠀⠀⣠⡶⠟⠛⠳⣦⣀⢀⣠⡶⠟⠛⢶⣄⡀⣀⣴⠾⠛⠳⣦⣀⣠⡶⠟⠛⢶⣄⡀⣀⣴⠾⠛⠳⣦⣀⠀⣀⣴⠾⠛⢶⣄⡀⣀⣴⠾⠛⠳⣦⣀⠀⣀⣴⠾⠀\n#⠀⠀⠁⠀⠀⠀⠀⠈⠉⠉⠉⠀⠀⠀⠀⠉⠉⠉⠁⠀⠀⠀⠈⠉⠉⠉⠀⠀⠀⠀⠉⠉⠉⠁⠀⠀⠀⠈⠉⠉⠉⠀⠀⠀⠀⠀⠉⠉⠉⠁⠀⠀⠀⠈⠉⠉⠉⠀⠀⠀\n# By: Jim Coll, Mike Johnson, & the extended OWP and hydrologic communities\n\n\n\n_022592d47fcbe0185e447b9e4894a987.png"
  },
  {
    "objectID": "GEOINT_index.html",
    "href": "GEOINT_index.html",
    "title": "Introduction to GEOINT",
    "section": "",
    "text": "Note\n\n\n\nThis is a living document. Changes will be announced in class.\n\n\n\n\nHello all!\nMy name is Jim Coll and I am your instructor for the semester. A few notes for you pertain to how I run this course. Although the KU blackboard site will be the “official” site for this class and the place you submit all work to, I use this site here for the benefit of all and my own selfish desire to streamline my digital footprint. I will keep both sites as identical as possible when double posting material (e.g. the syllabus and course schedule), but in case of a conflict treat this version as the most recent. Use the navigation table to the left to find the syllabus, labs, and other documents. I have made myself as available to you as I can so feel free to find me via:\n\nEmail\nClass slack channel\nCornering me in the hallway\n\nBelow you’ll find the course outline and slides as appropriate. I look forward to an exciting, productive, and stimulating semester with you all. Best, Jim\n\n\n\n\n\n\n_8bd8b33e8ba641fb2c30c4c60813d8c5.png",
    "crumbs": [
      "Introduction to GEOINT"
    ]
  },
  {
    "objectID": "GEOINT_index.html#announcements",
    "href": "GEOINT_index.html#announcements",
    "title": "Introduction to GEOINT",
    "section": "",
    "text": "Hello all!\nMy name is Jim Coll and I am your instructor for the semester. A few notes for you pertain to how I run this course. Although the KU blackboard site will be the “official” site for this class and the place you submit all work to, I use this site here for the benefit of all and my own selfish desire to streamline my digital footprint. I will keep both sites as identical as possible when double posting material (e.g. the syllabus and course schedule), but in case of a conflict treat this version as the most recent. Use the navigation table to the left to find the syllabus, labs, and other documents. I have made myself as available to you as I can so feel free to find me via:\n\nEmail\nClass slack channel\nCornering me in the hallway\n\nBelow you’ll find the course outline and slides as appropriate. I look forward to an exciting, productive, and stimulating semester with you all. Best, Jim",
    "crumbs": [
      "Introduction to GEOINT"
    ]
  },
  {
    "objectID": "GEOINT_index.html#tentative-course-outline",
    "href": "GEOINT_index.html#tentative-course-outline",
    "title": "Introduction to GEOINT",
    "section": "",
    "text": "_8bd8b33e8ba641fb2c30c4c60813d8c5.png",
    "crumbs": [
      "Introduction to GEOINT"
    ]
  },
  {
    "objectID": "GEOINT_lab02.html",
    "href": "GEOINT_lab02.html",
    "title": "GEOINT Lab 02",
    "section": "",
    "text": "Lab 02 - Introduction to Google Earth Pro\nThis lab is a gratefully modified version of lab 2 from Bradley A. Shellito’s Introduction to Geospatial Technologies\nLearning Objective This lab provides an introduction on how to use Google Earth Pro and will help familiarize you with many of its features. Although we’ll touch on several more advanced software as the class moves on, Google Earth Pro is a really fast and useful arrow to have in your quiver and we’ll be back to use it more than once. The steps and analyses we’ll do in this introductory lab are pretty basic but foundational, and we’ll build on these as we move forward. The goals for you to take away from this lab are:\nFamiliarize yourself with the Google Earth Pro (GEP) environment, basic functionality, and navigation using the software Use different GEP layers and features Outline:\nLearning Objective Submission requirements Tutorial Obtaining software Setting up GEP Navigating in GEP Exploring landscapes in GEP Using GEP to Save Imagery Using GEP to Make Placemarks and Measurements Exploring landscapes in GEP Other Features of Google Earth Pro Google Mars Google Sky Flight Simulator Wrapping up Submission requirements Data Name Description GEOG111_Lab2Questions.docx Handout to turn in You are answering the questions (laid out in the word doc above and also included in the tutorial below) as you work through the lab. Use full sentences as necessary to answer the prompts, and submit it to blackboard when done. Tutorial Obtaining software Google Earth can be found in many forms. You are more than likely familiar with Google Maps, an application centered on driving directions and location finding, but there is also Google Earth on the web, a Google Earth for for your phone, and Google Earth Pro for desktop (GEP from here on out), a simplistic but fully fledged geospatial software. The lab computers already have this installed, but if you want to do this on your own PC you will need to download and install it.\nSetting up GEP If this is your first time opening GEP, the window will look like so:\nimcenter\nFeel free to take the tour to get a head start on the rest of this lab if you’ve never used Google Earth before, but we’ll walk though the most important steps below.\nOne of the first things to do in any software is to look at the options. To find these, go to Tools &gt; Options. Take a second and read though the visualization options in the 3D View tab. Note that the option to change both the Show Lat/Long format and Units of Measurement is in this tab. In the Navigation tab, a useful behavior you may want to turn off is the tilt on zoom option, the “Do not automatically tilt while zooming” is the most intuitive zoom behavior and will fix the need to habitually reorient yourself when using GEP to explore. Back on the main application page, if you click View, you should make sure both the Toolbar and Sidebar boxes are checked. There are also several other map options here you might want to explore. Common ones to keep on are the Overview map and Scale Legend.\nBefore we start exploring the planet, lets take a look at the options and set some of the default behaviors Use the mouse to change your perspective and explore different areas of the globe.\nNavigating in GEP Being able to adeptly navigate around is critical to any geographic software. Nothing is more frustrating that knowing how you want to move around but not being able to do so. Below are the navigation steps, read them and take the time to become proficient with them.\nTo pan in any direction: Left-click and hold. Then, drag the cursor until you see the view you want. To return to the default view (reorient yourself so north is up and the camera angle is pointed straight down) - Click the map and press “r”. You can zoom in and out to see more or less of a map area. Use the scroll wheel on your mouse or mouse touchpad to zoom in and out. The map controls on the upper right hand side of the map can also be used to pivot, pan, and zoom using just the mouse. Finally, there is a search bar on the toolbar on the left that works just like the search bar on Google Maps. Move the mouse to the upper-right side of the screen, and a set of controls appear, should you want to use those instead. These controls fade out when not in use and reappear when the mouse rolls over them.\nimcenter\nThe first button, an eye ball surrounded by four arrow marks with ‘N’ marked above the upward arrow, representing the ‘Look around’ function. Grab the N with the mouse (by placing the pointer on N and holding down the left mouse button) and drag the N (that is, slowly rotate it) around the ring (this is commonly called “grab and drag”). You see the view change (north will still be the direction in which the N is pointed). Clicking on the N returns the imagery so that the north direction is facing the top of the screen. The second button, a palm shaped symbol in the center surrounded by four arrow marks, representing the ‘Move function. By selecting one of the directional arrows and clicking it with the mouse, you can tilt your view around the terrain as if you were standing still and looking about. You can also grab the control and move it (by holding down the left button on your mouse) to simulate looking around in various directions. Recall from above that you can reset your view with the “r” key. You’ll find the Street View peg man button below that. This icon appears when Google Street View imagery is available to see on the ground. To use Street View, you would grab the icon with the mouse and drag it to a street that’s visible in the view to enter the Street View mode. This control is visible only if there are streets in the view through which you can enter Street View mode Below that, there is a zoom slider with ‘plus’ marked at the top and ‘minus’ marked at the bottom. The zoom slider is marked at the center of the slide. In the search box, type in University of Kansas. As GEP rotates zooms in, look to the Layers box. Make sure that both Photos, 3D Buildings, and Terrain are selected. This enables linking of locations on the ground to photos that users have taken. The locations that have linked photos have small blue and brown circles over the imagery. Click on the photo symbols on the imagery to see some photos.\nSee if you can locate the building you are currently in by panning and zooming around the campus. If you got lost or aren’t on campus but are playing along, type in Lindley hall, KU into the search bar. Grab the Street View icon from the controls and drag it to the street right in front of the building. You’ll see large areas of the campus pathways and roads turn blue (which indicates that Street View imagery is available for this street). Release the icon on the street, and the view shifts from an aerial view to imagery that looks like you’re actually standing in that spot. Use a combination of the Street View and imagery to answer question 1:\nQuestion 1 From viewing Lindley Hall from above and in front, what details from the aerial view can help identify what the building is and how to properly orient yourself? From the GEP Layers panel, make sure Roads is checked. You’ll see major roads (interstates, state highways) identifiable as their labels appear, and local roads will also be identifiable when you zoom in close. We’re going to plan a lunch trip to the greatest restaurant in Lawrence, Thai Diner. In the Search box, click on the Get Directions option. In the A option (this is where you’re traveling from), type in Lindley Hall. In the B option (this is where you’re traveling to), type in Thai Diner. Finally, click the Get Directions button. GEP zooms out to show you the path it calculated for driving distance between the two points, and the Search box fills with directions featuring stops and turns along the way. imcenter\nQuestion 2 By viewing the road network between our destinations, you’ll realize there are many possible routes between the two. Why do you suppose GEP chose this particular route? Question 3 Based on the route that GEP calculated, what is the driving distance (and approximate time equivalent) to get to our destination? The capabilities to take a bunch of letters and numbers and turn them into a mapped location and to calculate the shortest driving distance between points are some of the core functions of GIS, and we will develop these later in the course. For now, you can click the X at the bottom of the Search box to clear the directions and remove the route\nExploring landscapes in GEP Kansas has some truly gorgeous landscapes, but it is mathematically flatter than a pancake, so let’s go explore a slightly more topographically diverse landscape. The closest National Park to Lawrence, the Badlands national park, is just to the northwest.\nTo see the boundaries of Badlands National Park, go to the Layers box and expand the option for More (click on the triangle to the left of the name). In the options that appear under the expanded heading, put a checkmark in the Parks/Recreation Areas option. Pan and Zoom out until you see the northernmost boundary of Badlands National Park highlighted in green. If you get lost remember you can use the search function. Once the park is centered in your view new icons for the locations of Visitors Centers and other features should appear. Pan over to the eastern edge of the park, and you see a large Question Mark icon indicating a park entrance as well as a green arrow indicating an overlook. imcenter\nMake sure that the option for Terrain in the Layers box is turned on. Zoom into the point representing the overlook. At the bottom of the main map you’ll see numbers representing the latitude and longitude of the point, as well as the real-world elevation of that spot.\nQuestion 4 What is the elevation of this particular overlook in Badlands National Park? The imagery in GEP is placed on top of a model of Earth’s terrain. To get a better view of this extra dimension, use the Zoom Slider to tilt the view down (remember you can also hold down the Ctrl key and move the mouse to change your perspective) so that you can look around as if you were standing at the overlook point (in a perspective view of the planet). Once you’ve tilted all the way down, use the Look controls to examine the landscape. From here, use the Move controls to fly over the Badlands from this overlook point. Once you feel like you have a decent feeling of what the Badlands looks like, answer Question 5.\nQuestion 5 How does the terrain modeling (with the tilt function) aid in the visualization of the Badlands? This ability to model the peaks and valleys of the landscape with aerial imagery “draped” or “stretched” over the terrain for a more realistic appearance is often used with many aspects of geospatial technology, and we’ll explore some of these analyses in more depth later in the semester.\nUsing GEP to Save Imagery It’s time to continue to the next leg of our journey by heading to Mount Rushmore. Carved out of the side of a mountain in the Black Hills, Mount Rushmore National Memorial features the faces of presidents George Washington, Thomas Jefferson, Theodore Roosevelt, and Abraham Lincoln. For more information about Mount Rushmore.\nIn GEP’s Search box, type “Mount Rushmore”. GEP zooms around to an overhead view of the monument. Like the Badlands, Mount Rushmore is overseen by the National Park Service. Zoom out a little bit and you’ll see the extent of the memorial’s boundaries (still outlined in green). Let’s save an image of what’s being shown in the view. Although we could use the snip tool, GEP can take a “snapshot” and save it as a JPEG (.jpg) file. Position the view to see the full outlined extent of Mount Rushmore, select the File &gt; Save &gt; Save Image. You’ll see the two most common map elements appear, a box with a legend and a title box with Untitled Map in it. Click on that to give your image a name. If your text gets too long, you can enter a new line by pressing Shift-Enter. Once done, you’ll see at the top of the map window there are options including map options, image resolution, and he save button. Once you are happy with your setting hit the save button and save the image to your PC. Minimize GEP go to the location on your computer where you saved the image and open it (using a simple image viewer like Microsoft OfficePhotos). imcenter\nQuestion 6 Note that even though the graphic contains the locations of Mount Rushmore, the outline of the park, and information concerning location (including latitude, longitude, and elevation) at the bottom, it doesn’t have any spatial reference for measurements. Why is this? Even though the saved image doesn’t have any spatial reference data, we’ll cover how to add that back in later in the class. For now, you can turn off the Parks/Recreation layer.\nUsing GEP to Make Placemarks and Measurements While you’re examining Mount Rushmore, you can set up some points of reference to which you can return. GEP allows you to create points of reference as placemarks. Let’s set up three points on the map: the top of the mountain, the amphitheater, and the parking area.\nFrom the GEP toolbar, select the Add Placemark button (1) A yellow pushpin (labeled “Untitled Placemark”) appears on the screen. Using your mouse, click on the Pushpin and drag it to the rear of the amphitheater so that the pin of the placemark is where the path meets the amphitheater (2) In the Placemark dialog box, type “Mount Rushmore Amphitheater” (3) Click the Placemark icon button next to where you typed the name, you can select an icon other than the yellow pushpin. Choose something more distinctive (4) When finished, click OK to close the dialog box. imcenter\nPut a second placemark at the top of the mountain by repeating this process. Name this new placemark “Top of the Memorial”. When done, position the two as tightly within the view as possible. Click on the Ruler tool on the toolbar. In the Ruler dialog box that appears, select Feet from the Map Length pull-down menu Use the options on the Line tab, which allow you compute the distance between two points. If you wanted to measure multiple points, you could do so from the Path tab. Using this tool, measure the distance between your two placemarks to answer the question below: imcenter\nQuestion 7 What is the measured distance between the rear of the amphitheater and the top of the memorial? (Keep in mind that this is the ground distance, not a straight line between the two points.) When you’re done, click on Clear in the Ruler dialog box to remove the drawn line from the screen and then close the Ruler dialog box. These abilities to create points of reference (as well as lines and area shapes) and then compute the distances between them might seem trivial, but these process level functions form the heart almost every GIS tool.\nFinally, let’s use the Tilt functions of GEP to get a perspective view on Mount Rushmore (as we did in the Badlands). Make sure that the option for Terrain in the Layers box is turned on and 3D Buildings option is turned off. Note that although you can see the height and dimensions of the mountain, the famous four presidents’ faces can’t been seen. Question 8 Even with Terrain turned on and the view switching over to a perspective, why can’t the presidents’ faces on the side of the memorial be seen? Now turn 3D Buildings on, can you see them now?\nExploring landscapes in GEP Notice that next to the elevation value at the bottom of the GEP screen, there’s a set of coordinates for lat (latitude) and long (longitude). Move the mouse around the screen, and you see the coordinates change to reflect the latitude and longitude of whatever the mouse’s current location is.\nZoom in closely on the road that enters the parking structure area of Mount Rushmore. Question 9 What are the latitude and longitude coordinates of the entrance to the Mount Rushmore parking area? You can also reference specific locations on Earth’s surface by their coordinates instead of by name. In the Search box, type the following coordinates: 43.836584, –103.623403. GEP rotates and zooms to this new location. Turn on Photos to obtain more information on what you’ve just flown to. You can also turn on the 3D Buildings layer (if necessary, again using the legacy 3D Buildings option, as you did with the Corn Palace and Mount Rushmore) to look at the location with a 3D version of it there.\nAnswer Questions 10 and 11, and then turn off the Photos (and the 3D Buildings option) when you’re done. Question 10 What is located at the following geographic coordinates: latitude 43.836584, longitude –103.623403? Question 11 What is located at the following geographic coordinates: latitude 43.845709, longitude –103.563499? Other Features of Google Earth Pro Google Earth Pro changes with new updates and features, but some of the more novel features include Google Mars, Google Sky, and Flight Simulator.\nGoogle Mars From the View pull-down menu, select Explore &gt; Mars. The view shifts to the familiar-looking globe, but this time covered with imagery (from NASA and the USGS) from the surface of Mars. The usual controls work the same way as they do with GEP, and there’s a lot of Martian territory to be explored. When you’re ready to return to GEP you use the same View &gt; Explore menus and then select Earth.\nGoogle Sky From the View pull-down menu, select Explore &gt; Sky. GEP’s view changes: Instead of looking down on Earth, you’re looking up to the stars, and instead of seeing remotely sensed aerial or satellite imagery, you’re looking at space telescope (including the Hubble) imagery. There’s plenty of imagery to be explored in this new mode. When you’re ready to return to GEP you use the same View &gt; Explore menus and then select Earth.\nFlight Simulator From the Tools pull-down menu, select Enter Flight Simulator. A new dialog box appears, asking if you want to pilot an F-16 fighter jet or an SR-22 propeller airplane. Select whichever plane you want to fly; GEP switches to the view as seen out the cockpit of your chosen plane, and you can fly across South Dakota (and the rest of GEP). Try not to crash (although you can easily reset if you do).\nWrapping up There is no need to save anything from this lab, so when done you can simply close without saving. Submit your answers to the questions on blackboard.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 02"
    ]
  },
  {
    "objectID": "GEOINT_lab04.html",
    "href": "GEOINT_lab04.html",
    "title": "GEOINT Lab 04",
    "section": "",
    "text": "This exercise provides an introduction to using a Global Positioning System (GPS) receiver to obtain coordinates and create a point shapefile. GPS is a system consisting of a network of satellites that orbit ~11,000 nautical miles from the earth in six different orbital paths. They are continuously monitored by ground stations located worldwide. The satellites transmit signals that can be detected with a GPS receiver. Using the receiver, you can determine your location with great precision through the trilateration (not triangulation!) of signals from at least 3 satellites, getting a distance from the difference between time measurements.\nAlthough the system is very sophisticated, and atomic clocks are used by the satellites, there are multiple sources and types of errors involved in finding your location. As the GPS signal passes through the charged particles of the ionosphere and then through the water vapor in the troposphere, this causes the signal to slow a bit, and this creates the same kind of error as bad clocks. Also, if the satellites that are in your view at a particular moment are close together in the sky, the intersecting circles that define a position will cross at very shallow angles. That increases the error margin around a position. The kind of GPS receivers we will be using provide about 10 meter accuracy (which may be reduced to under 3 m if differential corrections like WAAS are used), depending on the number of satellites available and the geometry of those satellites.\n\n\n\nIn this exercise we are going to collect the coordinates of some set of campus features using either the GPS units or your phones, generate a few shapefiles of those features, and then use the shapefile to make a map. The goals for you to take away from this lab:\n\nHow to use your selected platform to collect data\nHow to export that data\nHow to view that data in GIS software\n\n\n\n\n\n\n\n\n\n\n\n\nRecommended path\n\n\n\nIf you have an Android phone, I highly recommend the Android specific application, it is by far the most robust data collection application I’ve tested, and you will always have your phone on hand. The Apple/cross platform application will work for the objectives and purposes of this lab, but you will likely be underwhelmed by the capabilities. The handheld units are the most traditional (and perhaps most powerful) options, but unless you go out and buy a GPS unit for yourself you will likely never hold a GPS unit in your hands again.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe apps outlined in this lab are all free to use, although many have the option of upgrading to a paid version. You do not need to pay for anything if you don’t want to.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf in the event you get confused as to what your looking at, spamming the back button should always take you to the main menus. Make sure your unit is charged before going out on your adventure!\n\n\n\n\n\nYour goal in this lab is to demonstrate to me that you can collect the three different types of data: points, lines, and polygons. How you go about demonstrating that is up to you. Perhaps you want to go out and collect bus stops, a bus path, and outline the building? You can also go collect tree locations, water paths, and outline grassy areas? Maybe you want to go sample all the coffee on campus and trace your path from your dorm to the class? Whatever you decide to do, go out and collect these data and then view them in Google Earth to make a very simple map.\n\n\n\nUse the following instructions, your intuition as an individual born in the digital age, and trial and error to collect data for the above mission.\n\nAndroidAppleGarminetrex10GarminGPSMap60CS\n\n\n\nIf you are on an android device there are a few options available to you. The one you will need for this lab is called Locus GIS. The advantages of this app (Locus) over the cross platform one is several-fold. Perhaps most critically, the app allows you to export databases of features at once without going through a paywall. This app also seems to forgo ads when collecting data. The forms to add data are also more robust, allowing you to add attributes and pictures to the feature. I also feel the interface is easier to use.\n\n\nLOCUS separates data by projects, so to start a new project, go to the menu and create a new project. Make sure your new project coordinate system is set to 4326.\n\n\n\n\nTo go to a particular point, first add the point using the plus button, and manually enter the coordinates. After the point has been added, you can click on it, and at the top of the application, one of the options will be to navigate to the point using Google Maps.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLocus allows you to record points based on GPS, but also has a rich digital editor that allows you to draw GIS data. We’ll only cover recording data here, but feel free to explore!\n\n\n\n\n\nTo create points, you need to first create a point file and fill out the fields as necessary. These can be edited in app.\n\n\n\n\nYou likewise need to create new files for lines and polygons. To draw a line, start recording and stop or pause recording when you want to stop.\n\n\n\n\nYou are looking for the Fields Area Measure Free app. There is a guided tutorial built right into the app, and your intuitive knowledge as digital generation should carry you the rest of the way.\n\n\n\n\nClick the menu button (upper left corner), and go to saved measures. Click the three buttons next to the entries to delete them.\nTo set the units, go to settings, and make sure the measurement system is set to metric\n\n\n\n\n\nTo go to a particular point, first add the point using the plus button, and manually enter the coordinates. After the point has been added, you can click on it, and at the top of the application, one of the options will be to navigate to the point using Google Maps.\n\n\n\n\nThere are a few caveats to this app. First, you may have to sit though an ad if you drop more than a few features in a session. Finally, the exporting of the data must be done individually (feature by feature), making the unpaid version of this unworkable for larger projects. Sorry apple :(\n\n\n\n\nThe lab has access to the eTrex handheld garmin units. See the website for the users manual. Instructions on how to accomplish some of the most common tasks are included below:\n\n\nFirst, lets make sure the unit is set up properly.\n\nFrom the main menu, go to setup | units and ensure distance is set to metric and elevation is in feet\nfrom the setup menu, go to Position Format and make sure it’s set to hddd.ddddd°, Map Datum and Map Spheroid are both set to WGS 84\nFinally, from setup go to reset and make sure that you delete all waypoints and clear the current track\n\n\n\n\n\nTo go to a point, from the main menu go to where to | coordinates, and enter your coordinates, or choose a previously dropped waypoint.\nWhen done, the map will pop up, directing you to your chosen destination.\n\n\n\n\n\n\nFrom the main menu, go to Mark waypoint and click done\n\n\n\nLines:\n\nGo to Main Menu | Tracks\nTo begin line: Clear | Yes (clear track log) | Track Log on\nTo end line: Track Log off | Save | Yes (save all tracks)\nMake sure you clear the track log between tracks! (This keeps each line separate.)\n\nPolygons:\n\nUse the same steps as lines, except you need to make sure that your line ends where it began.\n\n\n\n\n\n\nThe lab also has Garmin GPSMap 60CS handheld units. See the website for the users manual. Instructions on how to accomplish some of the most common tasks are included below:\n\nNote: Charge the unit with the provided usb cable before going out on your adventure!\n\n\n\n\nMake sure the GPS unit is configured to collect coordinates in decimal degrees and distance is in meters:\nMain Menu (press Menu twice) | Setup | Units | Position format = hddd.ddddd° | Distance/Speed = Metric\nEven though the map datum will be set to WGS 84, this will correlate fine with the NAD projection of our data later on.\n\nNext, lets clear out the old data.\n\nFind | Waypoints | Menu | Delete… | All Symbols | Yes\nMain Menu | Tracks | Clear | Yes | Menu | Delete All Saved | Yes\n\n\n\n\n\n\nPoints:\n\nPress the Mark button. Click OK to save.\nTo view a list of all of the waypoints you have taken, use the Find button (it leads to Waypoints menu) next to the Mark button.\n\n\n\n\nLines:\n\nGo to Main Menu | Tracks\nTo begin line: Clear | Yes (clear track log) | Track Log on\nTo end line: Track Log off | Save | Yes (save all tracks)\nMake sure you clear the track log between tracks! (This keeps each line separate.)\n\nPolygons:\n\nUse the same steps as lines, except you need to make sure that your line ends where it began.\n\n\n\n\n\n\n\n\n\nEach method of acquiring GPS data can come with it’s own headache of data massaging. Our goal is getting the data off of the device and into the KML/KMZ format. Fortunately for us, all of the above methods of data collection facilitate this format natively, so we don’t have to do a weird data conversion dance to arrive at our desired endpoint. Use the following instructions to export your data.\n\nFrom LOCUSFrom HandheldsFrom KML (apple)\n\n\nAs mentioned, LOCUS makes this process painless, from your project simply click the 3 more buttons and then Export as KML.\n\n\n\n\nClick the Start/Windows button, click the little down arrow in the bottom left corner, and then click dnrgps (under #-Programs).\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also download from the Minnesota Department of Natural Resources. However, apparently ftp is on the way out and it can be difficult to get it to download properly. I’ve saved a version here for archival purposes, but to get this to download for yourself (I’m assuming you are using Chrome) you need to:\n\nType “chrome://flags” into the browser bar\nctrl-f “ftp” to find the relevant flag\nenable and refresh the browser\nAttempt to redownload it,\nOn the bottom download bar, an unsecure warning appears. click the arrow and then Keep to finally retrieve the file\n\n\n\n\nConnect the GPS unit to the computer using the provided USB cable. Turn the unit on, press the Page button until you reach the Satellite screen, press the Menu button and select Use with GPS off (this helps save the battery).\nIf the application doesn’t find your GPS unit automatically, select GPS | Find GPS (or Connect to Default GPS).\n\n\n\nDownload your features into the application by clicking the GPS menu and selecting Download All.\n\n\n\nLook at the table of waypoints. If you see any unwanted waypoints in the table, select them and click the red X on the left side of the screen.\nLet’s project our data before we go any further. Click File &gt; Set Projection. On the Projection tab, set the POSC Code to “26915”, the datum to “NAD83”, and the projection to “UTM zone 15N” (or whatever is appropriate for your use case). When finished, click OK.\nSelect all of the waypoints that you want to use and then click Edit &gt; *Project Coordinates.\nClick File &gt; Save To &gt; File…. Save your waypoints as a KML with a sensible naming convention (like “GEOG111_Lab4Waypoints”).\n\n\n\nDownloading tracks is similar to downloading waypoints. Click the Tracks tab and then select all the records (rows) that correspond to your tracks click File &gt; Save To &gt; File…. Save your track as KMZ with a sensible name. When it prompts for Save to Shape/GPS Types, be sure you select the appropriate one. When in doubt, Line is safest.\nWhen you’re done creating your KML’s, close the DNR GPS application, turn the GPS unit off, disconnect the unit from your computer, and put it back.\n\n\n\n\nClick on the Menu button to open the Saved measures.\nLong press on a saved set to open up the options at the top and click the share icon Click on KML and then either save the measure to your device or share with an application you have access to (emailing it to yourself is a common if not long way to do this).\n\n\n\n\n\n\n\n\n\nIn Windows Explorer, double-click the file you just created. (If you don’t see it, look for KMZ File under the Type column.)\nGoogle Earth should open and zoom to the location where your points are. Are your points where they’re supposed to be?\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon errors: “When I open my KML layer adobe pops up.” Adobe can be aggressive with it’s file extensions and may have been set as the default program to handle KML layers as for some reason. You can either right click on the file and open with &gt; find program, or open Google Earth and go to File &gt; Open\n\n\n\nFind a cool view angle and zoom level that fully encompasses your data.\nLet’s export our map for viewing. Go to File &gt; Save &gt; Save Image… This adds common map elements including Title blocks and Legend, as well as a north arrow. Fill out the pertinent information, including a relevant title, your name, and the data source.\nFinally, under Map Options at the top, you have the option of toning down the intensity of the base map. When done, export your map and submit it. Congratulations, you just made your first map!",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 04"
    ]
  },
  {
    "objectID": "GEOINT_lab04.html#background",
    "href": "GEOINT_lab04.html#background",
    "title": "GEOINT Lab 04",
    "section": "",
    "text": "This exercise provides an introduction to using a Global Positioning System (GPS) receiver to obtain coordinates and create a point shapefile. GPS is a system consisting of a network of satellites that orbit ~11,000 nautical miles from the earth in six different orbital paths. They are continuously monitored by ground stations located worldwide. The satellites transmit signals that can be detected with a GPS receiver. Using the receiver, you can determine your location with great precision through the trilateration (not triangulation!) of signals from at least 3 satellites, getting a distance from the difference between time measurements.\nAlthough the system is very sophisticated, and atomic clocks are used by the satellites, there are multiple sources and types of errors involved in finding your location. As the GPS signal passes through the charged particles of the ionosphere and then through the water vapor in the troposphere, this causes the signal to slow a bit, and this creates the same kind of error as bad clocks. Also, if the satellites that are in your view at a particular moment are close together in the sky, the intersecting circles that define a position will cross at very shallow angles. That increases the error margin around a position. The kind of GPS receivers we will be using provide about 10 meter accuracy (which may be reduced to under 3 m if differential corrections like WAAS are used), depending on the number of satellites available and the geometry of those satellites.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 04"
    ]
  },
  {
    "objectID": "GEOINT_lab04.html#learning-objective",
    "href": "GEOINT_lab04.html#learning-objective",
    "title": "GEOINT Lab 04",
    "section": "",
    "text": "In this exercise we are going to collect the coordinates of some set of campus features using either the GPS units or your phones, generate a few shapefiles of those features, and then use the shapefile to make a map. The goals for you to take away from this lab:\n\nHow to use your selected platform to collect data\nHow to export that data\nHow to view that data in GIS software",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 04"
    ]
  },
  {
    "objectID": "GEOINT_lab04.html#tutorial",
    "href": "GEOINT_lab04.html#tutorial",
    "title": "GEOINT Lab 04",
    "section": "",
    "text": "Recommended path\n\n\n\nIf you have an Android phone, I highly recommend the Android specific application, it is by far the most robust data collection application I’ve tested, and you will always have your phone on hand. The Apple/cross platform application will work for the objectives and purposes of this lab, but you will likely be underwhelmed by the capabilities. The handheld units are the most traditional (and perhaps most powerful) options, but unless you go out and buy a GPS unit for yourself you will likely never hold a GPS unit in your hands again.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe apps outlined in this lab are all free to use, although many have the option of upgrading to a paid version. You do not need to pay for anything if you don’t want to.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf in the event you get confused as to what your looking at, spamming the back button should always take you to the main menus. Make sure your unit is charged before going out on your adventure!\n\n\n\n\n\nYour goal in this lab is to demonstrate to me that you can collect the three different types of data: points, lines, and polygons. How you go about demonstrating that is up to you. Perhaps you want to go out and collect bus stops, a bus path, and outline the building? You can also go collect tree locations, water paths, and outline grassy areas? Maybe you want to go sample all the coffee on campus and trace your path from your dorm to the class? Whatever you decide to do, go out and collect these data and then view them in Google Earth to make a very simple map.\n\n\n\nUse the following instructions, your intuition as an individual born in the digital age, and trial and error to collect data for the above mission.\n\nAndroidAppleGarminetrex10GarminGPSMap60CS\n\n\n\nIf you are on an android device there are a few options available to you. The one you will need for this lab is called Locus GIS. The advantages of this app (Locus) over the cross platform one is several-fold. Perhaps most critically, the app allows you to export databases of features at once without going through a paywall. This app also seems to forgo ads when collecting data. The forms to add data are also more robust, allowing you to add attributes and pictures to the feature. I also feel the interface is easier to use.\n\n\nLOCUS separates data by projects, so to start a new project, go to the menu and create a new project. Make sure your new project coordinate system is set to 4326.\n\n\n\n\nTo go to a particular point, first add the point using the plus button, and manually enter the coordinates. After the point has been added, you can click on it, and at the top of the application, one of the options will be to navigate to the point using Google Maps.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLocus allows you to record points based on GPS, but also has a rich digital editor that allows you to draw GIS data. We’ll only cover recording data here, but feel free to explore!\n\n\n\n\n\nTo create points, you need to first create a point file and fill out the fields as necessary. These can be edited in app.\n\n\n\n\nYou likewise need to create new files for lines and polygons. To draw a line, start recording and stop or pause recording when you want to stop.\n\n\n\n\nYou are looking for the Fields Area Measure Free app. There is a guided tutorial built right into the app, and your intuitive knowledge as digital generation should carry you the rest of the way.\n\n\n\n\nClick the menu button (upper left corner), and go to saved measures. Click the three buttons next to the entries to delete them.\nTo set the units, go to settings, and make sure the measurement system is set to metric\n\n\n\n\n\nTo go to a particular point, first add the point using the plus button, and manually enter the coordinates. After the point has been added, you can click on it, and at the top of the application, one of the options will be to navigate to the point using Google Maps.\n\n\n\n\nThere are a few caveats to this app. First, you may have to sit though an ad if you drop more than a few features in a session. Finally, the exporting of the data must be done individually (feature by feature), making the unpaid version of this unworkable for larger projects. Sorry apple :(\n\n\n\n\nThe lab has access to the eTrex handheld garmin units. See the website for the users manual. Instructions on how to accomplish some of the most common tasks are included below:\n\n\nFirst, lets make sure the unit is set up properly.\n\nFrom the main menu, go to setup | units and ensure distance is set to metric and elevation is in feet\nfrom the setup menu, go to Position Format and make sure it’s set to hddd.ddddd°, Map Datum and Map Spheroid are both set to WGS 84\nFinally, from setup go to reset and make sure that you delete all waypoints and clear the current track\n\n\n\n\n\nTo go to a point, from the main menu go to where to | coordinates, and enter your coordinates, or choose a previously dropped waypoint.\nWhen done, the map will pop up, directing you to your chosen destination.\n\n\n\n\n\n\nFrom the main menu, go to Mark waypoint and click done\n\n\n\nLines:\n\nGo to Main Menu | Tracks\nTo begin line: Clear | Yes (clear track log) | Track Log on\nTo end line: Track Log off | Save | Yes (save all tracks)\nMake sure you clear the track log between tracks! (This keeps each line separate.)\n\nPolygons:\n\nUse the same steps as lines, except you need to make sure that your line ends where it began.\n\n\n\n\n\n\nThe lab also has Garmin GPSMap 60CS handheld units. See the website for the users manual. Instructions on how to accomplish some of the most common tasks are included below:\n\nNote: Charge the unit with the provided usb cable before going out on your adventure!\n\n\n\n\nMake sure the GPS unit is configured to collect coordinates in decimal degrees and distance is in meters:\nMain Menu (press Menu twice) | Setup | Units | Position format = hddd.ddddd° | Distance/Speed = Metric\nEven though the map datum will be set to WGS 84, this will correlate fine with the NAD projection of our data later on.\n\nNext, lets clear out the old data.\n\nFind | Waypoints | Menu | Delete… | All Symbols | Yes\nMain Menu | Tracks | Clear | Yes | Menu | Delete All Saved | Yes\n\n\n\n\n\n\nPoints:\n\nPress the Mark button. Click OK to save.\nTo view a list of all of the waypoints you have taken, use the Find button (it leads to Waypoints menu) next to the Mark button.\n\n\n\n\nLines:\n\nGo to Main Menu | Tracks\nTo begin line: Clear | Yes (clear track log) | Track Log on\nTo end line: Track Log off | Save | Yes (save all tracks)\nMake sure you clear the track log between tracks! (This keeps each line separate.)\n\nPolygons:\n\nUse the same steps as lines, except you need to make sure that your line ends where it began.\n\n\n\n\n\n\n\n\n\nEach method of acquiring GPS data can come with it’s own headache of data massaging. Our goal is getting the data off of the device and into the KML/KMZ format. Fortunately for us, all of the above methods of data collection facilitate this format natively, so we don’t have to do a weird data conversion dance to arrive at our desired endpoint. Use the following instructions to export your data.\n\nFrom LOCUSFrom HandheldsFrom KML (apple)\n\n\nAs mentioned, LOCUS makes this process painless, from your project simply click the 3 more buttons and then Export as KML.\n\n\n\n\nClick the Start/Windows button, click the little down arrow in the bottom left corner, and then click dnrgps (under #-Programs).\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also download from the Minnesota Department of Natural Resources. However, apparently ftp is on the way out and it can be difficult to get it to download properly. I’ve saved a version here for archival purposes, but to get this to download for yourself (I’m assuming you are using Chrome) you need to:\n\nType “chrome://flags” into the browser bar\nctrl-f “ftp” to find the relevant flag\nenable and refresh the browser\nAttempt to redownload it,\nOn the bottom download bar, an unsecure warning appears. click the arrow and then Keep to finally retrieve the file\n\n\n\n\nConnect the GPS unit to the computer using the provided USB cable. Turn the unit on, press the Page button until you reach the Satellite screen, press the Menu button and select Use with GPS off (this helps save the battery).\nIf the application doesn’t find your GPS unit automatically, select GPS | Find GPS (or Connect to Default GPS).\n\n\n\nDownload your features into the application by clicking the GPS menu and selecting Download All.\n\n\n\nLook at the table of waypoints. If you see any unwanted waypoints in the table, select them and click the red X on the left side of the screen.\nLet’s project our data before we go any further. Click File &gt; Set Projection. On the Projection tab, set the POSC Code to “26915”, the datum to “NAD83”, and the projection to “UTM zone 15N” (or whatever is appropriate for your use case). When finished, click OK.\nSelect all of the waypoints that you want to use and then click Edit &gt; *Project Coordinates.\nClick File &gt; Save To &gt; File…. Save your waypoints as a KML with a sensible naming convention (like “GEOG111_Lab4Waypoints”).\n\n\n\nDownloading tracks is similar to downloading waypoints. Click the Tracks tab and then select all the records (rows) that correspond to your tracks click File &gt; Save To &gt; File…. Save your track as KMZ with a sensible name. When it prompts for Save to Shape/GPS Types, be sure you select the appropriate one. When in doubt, Line is safest.\nWhen you’re done creating your KML’s, close the DNR GPS application, turn the GPS unit off, disconnect the unit from your computer, and put it back.\n\n\n\n\nClick on the Menu button to open the Saved measures.\nLong press on a saved set to open up the options at the top and click the share icon Click on KML and then either save the measure to your device or share with an application you have access to (emailing it to yourself is a common if not long way to do this).\n\n\n\n\n\n\n\n\n\nIn Windows Explorer, double-click the file you just created. (If you don’t see it, look for KMZ File under the Type column.)\nGoogle Earth should open and zoom to the location where your points are. Are your points where they’re supposed to be?\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon errors: “When I open my KML layer adobe pops up.” Adobe can be aggressive with it’s file extensions and may have been set as the default program to handle KML layers as for some reason. You can either right click on the file and open with &gt; find program, or open Google Earth and go to File &gt; Open\n\n\n\nFind a cool view angle and zoom level that fully encompasses your data.\nLet’s export our map for viewing. Go to File &gt; Save &gt; Save Image… This adds common map elements including Title blocks and Legend, as well as a north arrow. Fill out the pertinent information, including a relevant title, your name, and the data source.\nFinally, under Map Options at the top, you have the option of toning down the intensity of the base map. When done, export your map and submit it. Congratulations, you just made your first map!",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 04"
    ]
  },
  {
    "objectID": "GEOINT_lab06.html",
    "href": "GEOINT_lab06.html",
    "title": "GEOINT Lab 06",
    "section": "",
    "text": "Lab 06 - spatial analysis\nThis lab is a gratefully modified version of lab 6 from Bradley A. Shellito’s Introduction to Geospatial Technologies p442\nLearning Objective The goals for you to aim for in this lab:\nBuild simple and compound database queries Extract the results of a query into separate GIS layers Count the number of features within the boundary of another feature Overlay two features for spatial analysis Create and use buffers around objects for spatial analysis Outline:\nLearning Objective Submission requirements Tutorial import and join data (refresher) Attribute queries Exporting Overlay Buffer Measuring Submission Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab2Questions.docx Handout to turn in Tutorial ArcPRO QGIS import and join data (refresher) Attribute queries Exporting Overlay Buffer Measuring Submission All you have to turn into blackboard for this week is the final image you created above.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 06"
    ]
  },
  {
    "objectID": "GEOINT_lab08.html",
    "href": "GEOINT_lab08.html",
    "title": "GEOINT Lab 08",
    "section": "",
    "text": "This lab is a gratefully modified version of lab 7 from Bradley A. Shellito’s Introduction to Geospatial Technologies 529\n\n\n\nThis lab introduces you to the concept of using GIS data to create a print quality map. This map should contain the following:\nThe population per square kilometer for all counties in California, set up in an appropriate color scheme The data displayed in a projection with units of measurement other than decimal degrees (The default units used by the lab data are meters; make sure the scale bar reflects this information.) An appropriate legend (Make sure your legend items have regular names and that the legend is not called “legend.”) An appropriate title (Make sure your map title doesn’t include the word “map” in it.) A north arrow A scale bar Text information: your name, the date, and the sources of the data Appropriate borders, colors, and design layout (Your map should be well designed and should not look as if the map elements were thrown together randomly.) The goals for you to take away from this lab: Familiarize yourself with the Map Layout functions of QGIS Arrange and print professional-quality maps from geographic data using the various layout elements Outline:\nLearning Objective Submission requirements Tutorial Initial Pre-Mapping Tasks in QGIS Setting Graduated Symbology in QGIS The Map Layout in QGIS Choosing Landscape or Portrait Mode in QGIS Map Elements in QGIS Placing a Map into the Layout in QGIS Placing a Scale Bar into the Layout in QGIS Placing a North Arrow into the Layout in QGIS Placing a Legend into the Layout in QGIS Adding Text to the Layout in QGIS Other Map Elements in QGIS Printing the Layout in QGIS Submission Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab2Questions.docx Handout to turn in Tutorial Initial Pre-Mapping Tasks in QGIS If needed, refer to Geospatial Lab Application 5.1: GIS Introduction: QGIS Version for specifics on how to do the following.\nStart QGIS. Add the CalBounds feature class from the CalMap geodatabase in the C:7QGIS folder to the Layers panel. Leave the CalBounds layer’s symbology alone for now; you’ll change it in the next step. Pan and zoom the Map View so that all of California fills up the Map View. Important note: The data used in this Geospatial Lab Application has already been projected to the U.S. National Atlas Equal Area projection for you to use. However, there are many more projections to choose from if you desire; you can change the projection of a data layer by right-clicking on it, selecting Export then Save Features As, and then choosing a projected coordinate system in which to change the layer.\nSet the properties of the project you’re working with so that QGIS will be able to render some of the map elements you’ll be working with (such as the scale bar) properly. From the Project pull-down menu, choose Properties. Select the CRS tab. In the box next to Filter, type US National Atlas Equal Area to search through all available projections and find the one you need. Under Coordinate reference systems of the world, click on the US National Atlas Equal Area option and then click Apply. You have now set the CRS of the project environment to the chosen projection. Click OK to close the dialog box. imcenter\nSetting Graduated Symbology in QGIS QGIS enables you to change a feature’s symbology from a single symbol to multiple symbols or colors and allows for a variety of different data classification methods. Right-click on CalBounds in the Map Legend and select Properties. Click on the Symbology tab. To display the states as graduated symbols, use the following settings (and leave all the other settings alone): From the pull-down menu to the top of the menu, select Graduated. For Column, select PopDens (each county’s population per square kilometer from the year 2010). Use 5 for the number of classes. Use Quantile (Equal Count) for the mode. For the color ramp options, use the pull-down menu to select an appropriate choice. When you have things arranged as you want them, click Apply to make the changes. Take a look at the map and make any further color changes you think are needed. Click OK to close the dialog box. imcenter\nThe symbology of CalBounds has changed in the Map View, and the values that make up each of the breaks can be seen in the Layers panel. If the breaks and classes are not already displayed, you can show them by clicking the black arrow button to the left of the CalBounds layer in the Layers panel. The Map Layout in QGIS To begin laying out the print-quality version of the map, you need to begin working in the Map Layout (also known as the Map Composer). This mode of QGIS works like a blank canvas, allowing you to construct a map using various elements.\nTo begin, from the Project pull-down menu choose New Print Layout. Before the Map Layout opens, you are prompted to give your layout a title. Type in a descriptive title, such as California Population Density 2010 or something similar. Click OK after you’ve entered the title. A new window, the Map Layout, opens. In the Map Layout, the screen represents the printed page of an 8½ × 11 piece of paper, so be careful when you’re working near the edges of the page and keep all elements of the map within that border. The layout has several toolbars horizontally across its top, with a new set of tools; locate and examine these navigation tools. (Note: Any of the toolbars can be turned on and off by choosing the View pull-down menu, choosing Toolbars, and placing a checkmark in the appropriate box. This section refers to the Layout, Navigation, and Actions toolbars.) Starting at the left and moving right, the tools and their uses are as follows: The blue disk is used to save a layout. The white paper creates a new layout. The white paper over the gray paper is used to create a duplicate layout. The white paper with the wrench opens the Layout Manager. The yellow folder allows you to add a template to the layout. The blue disk with the green bar allows you to save a template. The printer icon is used when you’re printing (see later in the lab).h. The next three icons allow you to export your layout to either (1) an image, (2) SVG format, or (3) PDF format. The two curved arrows allow you to either undo to the last change you made or redo the last change you made (which is useful when you need to back up a step in your map design). The second row of tools is as follows: The plus and minus magnifying glasses are used to zoom in and out of the layout. The magnifying glass with the 1:1 text zooms the map to 100%. The magnifying glass with three arrows zooms to the full extent. The twin curved blue lines icon are used to refresh the view. The lock and unlock icons allow you to fix items in place in the layout (lock) or remove this fix so they can be moved (unlock). The square and circle icons allow you to gather up several items and treat them as a single group (group items) or to turn a group of items back into individual items (ungroup). The last four icons with the blue and yellow boxes are used for raising, aligning, distributing, or resizing map elements to better fit into the layout. The vertical toolbar down the left-hand side of the screen is the Toolbox toolbar, and it provides additional tools: imcenter\n\nThe hand is used for panning and adjusting the content of a particular window—for instance, for moving the position of what’s showing on the map without changing the map itself.\nThe magnifying glass is used to zoom in on elements on the map.\nThe cursor pointing to the box is used to move map elements to different places in the layout.\nThe icon with the blue arrows on a piece of paper is used to move the content of items.\nThe icon with the hammer and the three blue points joined together is used for editing nodes.\nThe other tools are used to add map elements to the layout, and you’ll use them later in the lab. You can use these icons to add a new map, an image, a new label, a new legend, and a new scale bar, as well as shapes, nodes, and arrow shapes for annotating the map, the attribute table of a layer, and an HTML frame for displaying Web content. Choosing Landscape or Portrait Mode in QGIS Before going any further with your map, you have to choose whether the map should be oriented in portrait or landscape mode. Portrait mode aligns the layout vertically (so that it’s longer than it is wide), while landscape mode aligns the layout horizontally (so that it’s wider than it is long). If you were mapping the United States, you’d likely choose landscape mode to place the entire country so it filled the page. However, when mapping California, you’d likely use portrait mode as it better fits the dimension of the state. To select whether you want to use portrait or landscape, right-click on the layout itself (the big blank white area) and from the pop-up menu that appears, choose Page properties. imcenter\n\nA new set of tabs appear on the right-hand side of the screen. Click on the Item Properties tab. Under Page Size, next to Orientation, choose Portrait. You see the layout shift to the vertically oriented Portrait mode. By selecting Landscape or Portrait from this option, you can change the orientation of the printed map. Map Elements in QGIS Again, think of the layout (or composition) as a blank sheet of paper that you can use to construct a map. Numerous map elements can be added, including the map canvas, scale bars, north arrows, and a legend. In the Items tab on the right-hand side of the layout, you can see a list of the elements you have added to the layout. When you choose an element in this Items tab, it is also chosen on the layout. You can also turn on an element by placing a checkmark in the box next to its name in the Items tab, and you can turn off an element by removing the checkmark next to it. Each element has properties (such as borders, fill colors, and size and position) that can be accessed by choosing the Item Properties tab on the right-hand side of the layout. You can move and resize elements by selecting them with the mouse and resizing as you would any other object. You can delete map elements by selecting them and pressing the Delete key on the keyboard. Placing a Map into the Layout in QGIS The first element to add to the layout is the default map—in this case, an element that shows all the visible layers in the Map Legend. Click on the Add New Map icon in the vertical Toolbox toolbar. You can then draw a box on the layout to display the current Map View. Draw a rectangular box on the layout to create the canvas. You can click and drag the canvas around the map or resize it as you see fit (using the four blue tabs at its corners), and you can treat the entire canvas as if it’s a single map element. Keep in mind that you’ll be adding several more map elements (such as a legend or a title) and adjust the size, spacing, and balance of the elements accordingly. You can’t manipulate individual layers (for instance, you can’t click and drag a particular county somewhere else), but by using the Move Item Content icon on the vertical toolbar, you can drag the contents of the map around and alter what’s displayed inside the canvas. To make further adjustments to the map (such as creating a border or altering the color of the border), select the Item Properties tab on the right side of the screen and scroll down to see what choices are available (such as expanding the Frame option to change the appearance of the map’s border or the Background option to change the fill color of the map). Investigate the other choices under Item Properties (such as the options under Main Properties for adjusting the scale of the layers being displayed on the map) to set up the canvas the way you’d like it to appear in the final printed layout. Placing a Scale Bar into the Layout in QGIS To add a scale bar to the map, select the Add New Scalebar icon on the toolbar and draw a box on the layout where you want the scale bar to be added. When the scale bar appears, click on it so that the four blue boxes appear at the corners. In the Item Properties box on the right-hand side of the screen, you can see several options for altering the scale bar. Use the options to change the following properties of the scale bar until it looks appropriate: The number of segments on the left or the right side of the 0 value on the scale bar The size of each segment The number of map units per bar units (to even out the breakpoints on the scale bar) The physical dimensions of the scale bar (height, line, width, etc.) The font and color used imcenter\nPlacing a North Arrow into the Layout in QGIS To add a north arrow to the map, select the Add Picture icon from the toolbar and draw a box on the layout where you want the north arrow to be added and draw a box the size that you want to north arrow symbol to be. An empty box and several different graphics options appear in the Item Properties tab. The Add Picture option allows you to add a graphic of your own to the map or to select from several pre-made graphics, including several different north arrows. Expand the options under Search Directories and scroll partway down, and you see a number of options for north arrows. imcenter\nSelect an appropriate north arrow, and it appears in the empty box on the layout. Use the cursor to position or resize the north arrow. Placing a Legend into the Layout in QGIS Each layer you use in QGIS (such as CalBounds) has a set of layer properties. Anything changed in the layer properties is reflected in changes to the layout. For instance, if you change the symbology of a layer in its properties, its appearance is transferred to a legend in the layout. Similarly, whatever name is given to a layer in the Map Legend carries over to a legend in the layout.\nReturn to the regular QGIS window. Currently, the CalBounds layer is named “CalBounds.” However, you can change this name to something more descriptive before adding a legend to the layout. In the QGIS Layers panel, right-click on the name you want to change, CalBounds, and select Rename Layer. In the Layers panel, you can type a new name for the layer (for example, Population per Square Km) and press the Enter key. Return to the layout. To add a legend to the map, click the Add New Legend icon on the toolbar and draw a box on the layout where you want the legend to be added. A default legend is added to the map, consisting of all the layers in the Map Legend with whatever names are assigned to them. Use the cursor to move and resize the legend as needed. To make changes to the default legend, click on the legend itself, and you see several options available in the Item Properties tab. You can change the name of the legend (don’t just call it “Legend”), its font, symbol width, and appearance, as well as the spacing between items. imcenter\nAdding Text to the Layout in QGIS To add text to the map (such as a title or any other text you might want to add), select the Add New Label icon from the toolbar and then draw a box on the layout where you want the text to be added of the size you want the text to be. Note that when text is added, a small box filled with the words “Lorem ipsum” (which is standard placeholder text) will be added to the layout. The lettering in the box, as well as its font, size, and color, can be changed in the Item Properties tab. In the Item Properties tab, under the Label heading, type the text you want to appear in the text box on the map. To alter the font, color, and other properties of the text, click the Font and Font Color drop-downs and choose from the available options. Note that you can alter each text box separately, create larger fonts for titles and smaller fonts for type, and so on. You can also resize each text box and move it to a new position. Other Map Elements in QGIS Though they’re not used in this lab, other map elements can be added, including: A shape: You can add the outline of a shape (an ellipse, a rectangle, or a triangle) to the layout for additional graphics or annotation. Use the Add Ellipse icon to do this. An arrow: You can add a graphic of an arrow (not a north arrow) so that you can point out or highlight areas to add further annotation to your layout. Use the Add Arrow icon to do this. An attribute table: You can add a graphic of a layer’s attribute table to the map to enable the presentation of additional information. Use the Add Attribute Table icon to do this. An HTML frame: You can add a frame that will contain a Web page or other Web content. In the Item Properties tab, you can specify the URL for what you want to display inside the frame. Use the Add HTML Frame icon to do this. Printing the Layout in QGIS When you have constructed the map the way you want it, choose the Layout pull-down menu, select Page Setup, and specify whether you want the map printed in landscape or portrait format and, if desired, add margins for printing. When the layout is ready, select the Print icon from the toolbar. In the Print dialog box, click Print, and QGIS prints to your computer’s default printer. If you want to convert the layout to a digital format instead of printing it, click on the Layout pull-down menu and choose one of the available options: Export as Image (to convert the layout to a format such as JPG or PNG), Export as PDF, or Export as SVG imcenter\nSubmission All you have to turn into blackboard for this week is the final image you created above.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 08"
    ]
  },
  {
    "objectID": "GEOINT_lab08.html#learning-objective",
    "href": "GEOINT_lab08.html#learning-objective",
    "title": "GEOINT Lab 08",
    "section": "",
    "text": "This lab introduces you to the concept of using GIS data to create a print quality map. This map should contain the following:\nThe population per square kilometer for all counties in California, set up in an appropriate color scheme The data displayed in a projection with units of measurement other than decimal degrees (The default units used by the lab data are meters; make sure the scale bar reflects this information.) An appropriate legend (Make sure your legend items have regular names and that the legend is not called “legend.”) An appropriate title (Make sure your map title doesn’t include the word “map” in it.) A north arrow A scale bar Text information: your name, the date, and the sources of the data Appropriate borders, colors, and design layout (Your map should be well designed and should not look as if the map elements were thrown together randomly.) The goals for you to take away from this lab: Familiarize yourself with the Map Layout functions of QGIS Arrange and print professional-quality maps from geographic data using the various layout elements Outline:\nLearning Objective Submission requirements Tutorial Initial Pre-Mapping Tasks in QGIS Setting Graduated Symbology in QGIS The Map Layout in QGIS Choosing Landscape or Portrait Mode in QGIS Map Elements in QGIS Placing a Map into the Layout in QGIS Placing a Scale Bar into the Layout in QGIS Placing a North Arrow into the Layout in QGIS Placing a Legend into the Layout in QGIS Adding Text to the Layout in QGIS Other Map Elements in QGIS Printing the Layout in QGIS Submission Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab2Questions.docx Handout to turn in Tutorial Initial Pre-Mapping Tasks in QGIS If needed, refer to Geospatial Lab Application 5.1: GIS Introduction: QGIS Version for specifics on how to do the following.\nStart QGIS. Add the CalBounds feature class from the CalMap geodatabase in the C:7QGIS folder to the Layers panel. Leave the CalBounds layer’s symbology alone for now; you’ll change it in the next step. Pan and zoom the Map View so that all of California fills up the Map View. Important note: The data used in this Geospatial Lab Application has already been projected to the U.S. National Atlas Equal Area projection for you to use. However, there are many more projections to choose from if you desire; you can change the projection of a data layer by right-clicking on it, selecting Export then Save Features As, and then choosing a projected coordinate system in which to change the layer.\nSet the properties of the project you’re working with so that QGIS will be able to render some of the map elements you’ll be working with (such as the scale bar) properly. From the Project pull-down menu, choose Properties. Select the CRS tab. In the box next to Filter, type US National Atlas Equal Area to search through all available projections and find the one you need. Under Coordinate reference systems of the world, click on the US National Atlas Equal Area option and then click Apply. You have now set the CRS of the project environment to the chosen projection. Click OK to close the dialog box. imcenter\nSetting Graduated Symbology in QGIS QGIS enables you to change a feature’s symbology from a single symbol to multiple symbols or colors and allows for a variety of different data classification methods. Right-click on CalBounds in the Map Legend and select Properties. Click on the Symbology tab. To display the states as graduated symbols, use the following settings (and leave all the other settings alone): From the pull-down menu to the top of the menu, select Graduated. For Column, select PopDens (each county’s population per square kilometer from the year 2010). Use 5 for the number of classes. Use Quantile (Equal Count) for the mode. For the color ramp options, use the pull-down menu to select an appropriate choice. When you have things arranged as you want them, click Apply to make the changes. Take a look at the map and make any further color changes you think are needed. Click OK to close the dialog box. imcenter\nThe symbology of CalBounds has changed in the Map View, and the values that make up each of the breaks can be seen in the Layers panel. If the breaks and classes are not already displayed, you can show them by clicking the black arrow button to the left of the CalBounds layer in the Layers panel. The Map Layout in QGIS To begin laying out the print-quality version of the map, you need to begin working in the Map Layout (also known as the Map Composer). This mode of QGIS works like a blank canvas, allowing you to construct a map using various elements.\nTo begin, from the Project pull-down menu choose New Print Layout. Before the Map Layout opens, you are prompted to give your layout a title. Type in a descriptive title, such as California Population Density 2010 or something similar. Click OK after you’ve entered the title. A new window, the Map Layout, opens. In the Map Layout, the screen represents the printed page of an 8½ × 11 piece of paper, so be careful when you’re working near the edges of the page and keep all elements of the map within that border. The layout has several toolbars horizontally across its top, with a new set of tools; locate and examine these navigation tools. (Note: Any of the toolbars can be turned on and off by choosing the View pull-down menu, choosing Toolbars, and placing a checkmark in the appropriate box. This section refers to the Layout, Navigation, and Actions toolbars.) Starting at the left and moving right, the tools and their uses are as follows: The blue disk is used to save a layout. The white paper creates a new layout. The white paper over the gray paper is used to create a duplicate layout. The white paper with the wrench opens the Layout Manager. The yellow folder allows you to add a template to the layout. The blue disk with the green bar allows you to save a template. The printer icon is used when you’re printing (see later in the lab).h. The next three icons allow you to export your layout to either (1) an image, (2) SVG format, or (3) PDF format. The two curved arrows allow you to either undo to the last change you made or redo the last change you made (which is useful when you need to back up a step in your map design). The second row of tools is as follows: The plus and minus magnifying glasses are used to zoom in and out of the layout. The magnifying glass with the 1:1 text zooms the map to 100%. The magnifying glass with three arrows zooms to the full extent. The twin curved blue lines icon are used to refresh the view. The lock and unlock icons allow you to fix items in place in the layout (lock) or remove this fix so they can be moved (unlock). The square and circle icons allow you to gather up several items and treat them as a single group (group items) or to turn a group of items back into individual items (ungroup). The last four icons with the blue and yellow boxes are used for raising, aligning, distributing, or resizing map elements to better fit into the layout. The vertical toolbar down the left-hand side of the screen is the Toolbox toolbar, and it provides additional tools: imcenter\n\nThe hand is used for panning and adjusting the content of a particular window—for instance, for moving the position of what’s showing on the map without changing the map itself.\nThe magnifying glass is used to zoom in on elements on the map.\nThe cursor pointing to the box is used to move map elements to different places in the layout.\nThe icon with the blue arrows on a piece of paper is used to move the content of items.\nThe icon with the hammer and the three blue points joined together is used for editing nodes.\nThe other tools are used to add map elements to the layout, and you’ll use them later in the lab. You can use these icons to add a new map, an image, a new label, a new legend, and a new scale bar, as well as shapes, nodes, and arrow shapes for annotating the map, the attribute table of a layer, and an HTML frame for displaying Web content. Choosing Landscape or Portrait Mode in QGIS Before going any further with your map, you have to choose whether the map should be oriented in portrait or landscape mode. Portrait mode aligns the layout vertically (so that it’s longer than it is wide), while landscape mode aligns the layout horizontally (so that it’s wider than it is long). If you were mapping the United States, you’d likely choose landscape mode to place the entire country so it filled the page. However, when mapping California, you’d likely use portrait mode as it better fits the dimension of the state. To select whether you want to use portrait or landscape, right-click on the layout itself (the big blank white area) and from the pop-up menu that appears, choose Page properties. imcenter\n\nA new set of tabs appear on the right-hand side of the screen. Click on the Item Properties tab. Under Page Size, next to Orientation, choose Portrait. You see the layout shift to the vertically oriented Portrait mode. By selecting Landscape or Portrait from this option, you can change the orientation of the printed map. Map Elements in QGIS Again, think of the layout (or composition) as a blank sheet of paper that you can use to construct a map. Numerous map elements can be added, including the map canvas, scale bars, north arrows, and a legend. In the Items tab on the right-hand side of the layout, you can see a list of the elements you have added to the layout. When you choose an element in this Items tab, it is also chosen on the layout. You can also turn on an element by placing a checkmark in the box next to its name in the Items tab, and you can turn off an element by removing the checkmark next to it. Each element has properties (such as borders, fill colors, and size and position) that can be accessed by choosing the Item Properties tab on the right-hand side of the layout. You can move and resize elements by selecting them with the mouse and resizing as you would any other object. You can delete map elements by selecting them and pressing the Delete key on the keyboard. Placing a Map into the Layout in QGIS The first element to add to the layout is the default map—in this case, an element that shows all the visible layers in the Map Legend. Click on the Add New Map icon in the vertical Toolbox toolbar. You can then draw a box on the layout to display the current Map View. Draw a rectangular box on the layout to create the canvas. You can click and drag the canvas around the map or resize it as you see fit (using the four blue tabs at its corners), and you can treat the entire canvas as if it’s a single map element. Keep in mind that you’ll be adding several more map elements (such as a legend or a title) and adjust the size, spacing, and balance of the elements accordingly. You can’t manipulate individual layers (for instance, you can’t click and drag a particular county somewhere else), but by using the Move Item Content icon on the vertical toolbar, you can drag the contents of the map around and alter what’s displayed inside the canvas. To make further adjustments to the map (such as creating a border or altering the color of the border), select the Item Properties tab on the right side of the screen and scroll down to see what choices are available (such as expanding the Frame option to change the appearance of the map’s border or the Background option to change the fill color of the map). Investigate the other choices under Item Properties (such as the options under Main Properties for adjusting the scale of the layers being displayed on the map) to set up the canvas the way you’d like it to appear in the final printed layout. Placing a Scale Bar into the Layout in QGIS To add a scale bar to the map, select the Add New Scalebar icon on the toolbar and draw a box on the layout where you want the scale bar to be added. When the scale bar appears, click on it so that the four blue boxes appear at the corners. In the Item Properties box on the right-hand side of the screen, you can see several options for altering the scale bar. Use the options to change the following properties of the scale bar until it looks appropriate: The number of segments on the left or the right side of the 0 value on the scale bar The size of each segment The number of map units per bar units (to even out the breakpoints on the scale bar) The physical dimensions of the scale bar (height, line, width, etc.) The font and color used imcenter\nPlacing a North Arrow into the Layout in QGIS To add a north arrow to the map, select the Add Picture icon from the toolbar and draw a box on the layout where you want the north arrow to be added and draw a box the size that you want to north arrow symbol to be. An empty box and several different graphics options appear in the Item Properties tab. The Add Picture option allows you to add a graphic of your own to the map or to select from several pre-made graphics, including several different north arrows. Expand the options under Search Directories and scroll partway down, and you see a number of options for north arrows. imcenter\nSelect an appropriate north arrow, and it appears in the empty box on the layout. Use the cursor to position or resize the north arrow. Placing a Legend into the Layout in QGIS Each layer you use in QGIS (such as CalBounds) has a set of layer properties. Anything changed in the layer properties is reflected in changes to the layout. For instance, if you change the symbology of a layer in its properties, its appearance is transferred to a legend in the layout. Similarly, whatever name is given to a layer in the Map Legend carries over to a legend in the layout.\nReturn to the regular QGIS window. Currently, the CalBounds layer is named “CalBounds.” However, you can change this name to something more descriptive before adding a legend to the layout. In the QGIS Layers panel, right-click on the name you want to change, CalBounds, and select Rename Layer. In the Layers panel, you can type a new name for the layer (for example, Population per Square Km) and press the Enter key. Return to the layout. To add a legend to the map, click the Add New Legend icon on the toolbar and draw a box on the layout where you want the legend to be added. A default legend is added to the map, consisting of all the layers in the Map Legend with whatever names are assigned to them. Use the cursor to move and resize the legend as needed. To make changes to the default legend, click on the legend itself, and you see several options available in the Item Properties tab. You can change the name of the legend (don’t just call it “Legend”), its font, symbol width, and appearance, as well as the spacing between items. imcenter\nAdding Text to the Layout in QGIS To add text to the map (such as a title or any other text you might want to add), select the Add New Label icon from the toolbar and then draw a box on the layout where you want the text to be added of the size you want the text to be. Note that when text is added, a small box filled with the words “Lorem ipsum” (which is standard placeholder text) will be added to the layout. The lettering in the box, as well as its font, size, and color, can be changed in the Item Properties tab. In the Item Properties tab, under the Label heading, type the text you want to appear in the text box on the map. To alter the font, color, and other properties of the text, click the Font and Font Color drop-downs and choose from the available options. Note that you can alter each text box separately, create larger fonts for titles and smaller fonts for type, and so on. You can also resize each text box and move it to a new position. Other Map Elements in QGIS Though they’re not used in this lab, other map elements can be added, including: A shape: You can add the outline of a shape (an ellipse, a rectangle, or a triangle) to the layout for additional graphics or annotation. Use the Add Ellipse icon to do this. An arrow: You can add a graphic of an arrow (not a north arrow) so that you can point out or highlight areas to add further annotation to your layout. Use the Add Arrow icon to do this. An attribute table: You can add a graphic of a layer’s attribute table to the map to enable the presentation of additional information. Use the Add Attribute Table icon to do this. An HTML frame: You can add a frame that will contain a Web page or other Web content. In the Item Properties tab, you can specify the URL for what you want to display inside the frame. Use the Add HTML Frame icon to do this. Printing the Layout in QGIS When you have constructed the map the way you want it, choose the Layout pull-down menu, select Page Setup, and specify whether you want the map printed in landscape or portrait format and, if desired, add margins for printing. When the layout is ready, select the Print icon from the toolbar. In the Print dialog box, click Print, and QGIS prints to your computer’s default printer. If you want to convert the layout to a digital format instead of printing it, click on the Layout pull-down menu and choose one of the available options: Export as Image (to convert the layout to a format such as JPG or PNG), Export as PDF, or Export as SVG imcenter\nSubmission All you have to turn into blackboard for this week is the final image you created above.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 08"
    ]
  },
  {
    "objectID": "GEOINT_lab09B.html",
    "href": "GEOINT_lab09B.html",
    "title": "GEOINT Lab 09 B",
    "section": "",
    "text": "Lab 09B - Cloud based GIS\nimcenter\nThis lab is ungracefully ripped from my own tutorial on the UCGIS BoK entry for GEE. Please explore this version for more information and background.\nGoogle Earth Engine is still a beta product, and you will need to request access to the platform through your Google account here. These are still approved by hand and may take a few days to get. Once approved, you will need to follow the instructions in the email that is sent before you are able to access the platform. Therefore, be sure to do this step before you attempt to work though this lab. Learning Objective This lab serves as a highly hand held introduction to Google Earth Engine and cloud based geospatial operations. There will be a lot of code to look at here, but no prerequisite code experience is necessary and the steps have been broken into small, digestible pieces. The goal by the end of this tutorial is not that you have become an expert in JavaScript, but that you see the power and potential that cloud based geospatial analyses offers, and that you come away with a deeper appreciation for the technology necessary to facilitate effective global scale operations.\nOutline:\nLearning Objective Submission requirements Tutorial Background and outline Creating ImageCollections Joining the Collections Masking out large zenith angle pixels Reclassifying Calculating Snow Cover Frequency Trend Analysis Exporting the Results Building a web-based application Conclusions & Wrapping up Submission requirements You are submitting a URL to blackboard. There are no questions to answer or\nTutorial Background and outline Snow cover frequency, or the number of days that a spot is covered by snow divided by the number of days it has a valid observation, is an important metric of the cryosphere, and is critical to a number of biogeophysical processes. Using the daily MODIS snow cover dataset, we will calculate the snow cover frequency on a yearly basis for water years 2003 through 2018. A water year here is defined as October 1st to September 30th and is labeled as the year of the end date. For example, water year 2004 is from Oct. 1, 2003 to Sept. 30, 2004. We will then find and map the linear trend of snow cover frequency at every pixel across the globe. To add a little more nuance on this analysis, we will take into account the sensor zenith angle of each MODIS observation. At the extremes of the MODIS instruments sensor zenith, the pixel length has essentially doubled, and the width has increased more than 10 times that of the nidar pixel. These observations can skew aerial coverages, so we want to limit our daily snow cover dataset to just those observations that have a sensor zenith angle less than 25o, limiting pixels to 110% of the nominal area of a nadir pixel. To do this we need two daily datasets, the daily snow cover product MOD10A1 and the sensor properties contained in the MOD09GA dataset, both are available in GEE.\nTo accomplish this analysis, we will first need to join the two datasets (MOD10A1 and MOD09GA), to create a combined dataset. We then need to mask out those pixels with a high sensor zenith angle. After that, we need to reclassify the snow cover to a simple snow/no snow/missing dataset, which will make future calculations easier. After reclassifying the data, we want to count at each pixel the number of days that have a snow observation and the number of days that have a valid observation. We will then need to add a band representing the water year (time) and calculate the trend at each pixel before finally exporting our results. The steps of this process are exemplified in Figure 3. We will wrap much of that data up into functions, as outlined below, in the larger ovals. The general process we will follow is shown in Figure 3.\nimcenter\nA schematic of data analysis flow.\nCreating ImageCollections imcenter\nWhere we are in relation to the rest of our analysis. Here we explore the data and learn its associated structure in GEE.\nLooking at Figure 4, we will start by accessing the MOD10A1 dataset and examine its structure. You can search for the datasets with the search bar at the top of the Code Editor, and use associated GEE data IDs to access them. We start by searching for MOD10A1, and importing it, as shown in Figure 5. Using just over 40 characters, we have access to the entire MOD10A1 dataset in less than 20 seconds. These static variables can also be included as Imports, which sit above the code editor as Imports.\nimcenter\nThe view of the Code Editor IDE and how to import an ImageCollection into the API for analysis. The code is available at https://code.earthengine.google.com/8039423066ab1f923a9ba9c7db01512e\nWe can see that MOD10A1 is imported into your JavaScript as an ImageCollection. If you recall, the MOD10A1.006 data product is a daily global snow cover dataset and each Image in the ImageCollection has 9 bands. The one we are interested is the “NDSI_Snow_Cover” band which is a fractional snow coverage band with valid values from 0-100. Although the quality band has a high sensor zenith flag, the full sensor zenith angle data is only found in the MOD09GA.006 dataset. Therefore, our next goal is to join the two datasets together.\nJoining the Collections imcenter\nWhere we are in relation to the rest of our analysis. Here we demonstrate how to join collections.\nAfter we import the MOD09GA dataset as another ImageCollection, our goal is to get the sensor zenith angle band from the MOD09GA ImageCollection to join with our snow cover data. To select a particular band to work with, we can use the .select() method, as shown in Figure 7.\nimcenter\nExample of using the .select() and .filterDate() methods in GEE to pull images we need and examine their structure. The code is available at https://code.earthengine.google.com/585a7df6444cfa07d10a25a8dfbb337e\nJoining two collections requires a common property between them, just as it does in standard GIS software. In this case the best way to go about joining the collections is to join them based on their acquisition times, using the “system:Time_Start” property as the filter. If you look at the various types of joins in the documentation, you will see that there are a few ways to do this but we will use the join.inner(). The code to join the two datasets together is shown in Figure 8.\nimcenter\nThe code used to join the two MODIS datasets together, which is available at https://code.earthengine.google.com/e63a22be50d2c017363d22d0d0d1c423\nWe’ve just joined two MODIS collections together, and it took less than 20 lines of code to do so! However, if you print the resulting collection you will get something that looks like Figure 9.\nimcenter\nPrinted results of the joined collection, itself the result of the previous code execution.\nThe output collection from the join operation is a FeatureCollection, where the matching images are the “primary” and “secondary” properties of the features. In order to convert this to a format that we can work with, we need to run a function across the FeatureCollection, creating a new image that has the bands from the primary and secondary images. You can visualize the conversion as a series of operations shown in Figure 10.\nimcenter\nThe schematic of the MergeBands function we need to write to transform the results of our join into a more useable form.\nThe code for the MergeBands function and for applying the function to each matching pair of images in the FeatureCollection are shown in Figure 11. When run the code, be sure to note the difference between the input and output collections.\nimcenter\nThe code (available at https://code.earthengine.google.com/81bac4a77cf20821f86ad94b7611d457) for the MergeBands function and for applying the function to each pair of matched images, which convert the joined collection to a form more useful for subsequent analyses\nNow that we have joined the two collections, we need to remove the pixels that have a sensor zenith angle greater than 25o.\nMasking out large zenith angle pixels imcenter\nWhere we are in relation to the rest of our analysis. Here we demonstrate two useful functions to create a sensor filtered dataset.\nAs shown in Figure 12, with snow cover and zenith angle joined into images as bands, we need to remove the pixels from each image that have a sensor zenith angle larger than 2500 (the angle is stored as a multiple of *100). To do this we will introduce two of the most useful functions in GEE, .map() and .mask(). As seen in the MergeBands function, we used the .map() method. This method executes the same function across each element in a collection, and returns a collection of the same depth. Masking is another helpful method we’ll use. The input of mask is applied to the image and anything which isn’t specified within is converted to a null value. Figure 13 shows a simplistic example where we use the .mask() method to keep only the areas with an elevation larger than 2000 m from the SRTM90 elevation dataset in the image.\nimcenter\nThe code (available at: https://code.earthengine.google.com/60187ddb4b541fb209697f2c6820c65d) and results of an example .mask() method.\nUsing the .mask() and .map() methods, we’ll modify our code to mask out any pixels with a sensor zenith angle greater than 250. Because we want to do this for every image in the ImageCollection we will use the .map() method with the MaskSensorPixels() function as shown in the highlighted portion of Figure 14.\nimcenter\nThe code for masking out large zenith sensor angle pixels, which is available at https://code.earthengine.google.com/bfed155a968160db67253474cd707c9f\nReclassifying imcenter\nWhere we are in relation to the rest of our analysis. Here we need to reclassify our snow band to a binary no snow/snow dataset.\nAs shown in Figure 15, our next step is to reclassify the pixels on each image to snow, non-snow or missing. The “NDSI_Snow_Cover” band in MOD10A1.006 is a fractional band with valid values from 0-100, indicating the fractional snow coverage at a pixel. To perform a snow cover frequency analysis, we need to reclassify the band into the no-snow/snow/missing categories, using values that represent “snow” as 1, “No snow” as 0, and null for ‘missing’ values. To accomplish this GEE has a remap() method, which takes a list of values and maps the values to another list of values you specify. Because we need to reclassify every image in the collection, we will use the handy .map() method again. We are now at the end of our first data processing chain, “Prepare MODIS Snow cover”, meaning we can wrap up all the previous steps into a single function, PrepareModisSnowCover(), that takes two dates, a StartDate and a StopDate. The code to do this is shown in Figure 16. As an aside, these last few steps could be wrapped up in the same .map() call, but were separated here for demonstration purposes.\nimcenter\nThe code used to generate a year of sensor-zenith-angle filtered, reclassified snow cover data, available at https://code.earthengine.google.com/5e68e31ee7ead90a6d33b80ab8826e55\nCalculating Snow Cover Frequency imcenter\nWhere we are in relation to the rest of our analysis. Here we need to create a function that generates a time series of images which stores the snow cover frequency and the date of the image.\nNow that we have a function to create a processed MODIS snow cover dataset for a given time interval we need to construct a function to generate a time series of snow cover frequency as outlined in Figure 17. The PrepareMODISSnowCover function returns a single band, called remapped, that represents a sensor adjusted and reclassified snow cover for each day of the collection. In order to calculate snow cover frequency we need to know both the number of snow days and the number of days we have valid observations. The .count() method in GEE counts the number of times a pixel has a valid value within an image collection, and will ignore (not count) those images that have a null value, so it counts both snow and no-snow values in a pixel stack. We can also count the number of snow days at a pixel by using the .sum() method, which adds the values of a stack of pixels. We will wrap this up in a function that takes a start and end date, and the number of intervals to advance. This code to accomplish this is shown in Figure 18 below:imcenter\nThe code which calculates snow cover frequency within a certain time interval is available at https://code.earthengine.google.com/aeee1a828fb717397d38eaef236ed498\nTrend Analysis imcenter\nWhere we are in relation to the rest of our analysis. Here we need to pass our final dataset to the LinearFit Reducer.\nAs outlined in Figure 19, we have now created the necessary functions to create a time series of snow cover frequency, but to calculate a linear trend we also need a band which represents the time stamp. With a small modification to the GenerateSnowCoverFrequency, we can add the EndDate as a numerical band, which gives us everything we need to calculate the linear trend. If we look at the documentation, ee.Reducer.linearFit() returns an image with two bands, one called ‘scale’ and one called ‘offset’. This is Google’s terms for ‘slope’ and ‘intercept’, so the value we are after is the scale. The code to do so is shown in Figure 20.\nimcenter\nThe code to create snow cover frequency and linear trend is available at https://code.earthengine.google.com/c5dd98445765937e9ff1c1f4f6d24ec5\nExporting the Results imcenter\nWhere we are in relation to the rest of our analysis. Now we need to display and export our analysis.\nAs we can see from Figure 21, we are at the end of our analysis. The final step is to visualize and export the analysis so that others can view it. The first thing we need is to do is create color palettes. We then filter the slope based on a minimum value of snow cover frequency using the .mask() function with a .min() reducer over the entire collection. This ensures that we are not displaying the trend analyses for areas that have very low snow coverages. We then use the Map.addLayer() method to add a layer to the map, setting the min and max values and the palette we want. Then we will export our analysis as a GeoTIFF using Export.Image.toDrive(). For a little more flair, we’ll also export the timeseries of snow cover frequency as a video using Export.video.toDrive(), using another .map() call to force images into rgb space for video. The code to accomplish this is shown in Figure 22.\nimcenter\nThe code to accomplish the display and export of our analysis, available at https://code.earthengine.google.com/3ac848247d0e231e50c5feefd7d23f6d\nBuilding a web-based application Finally, let’s capitalize on the advantage of the new UI and app features in GEE to create an interactive web map so that anyone may view our analysis. The UI Examples provide several great frameworks to construct an app, so we will borrow some of them to create an application using our new snow cover trend data. The code to do so look like so: https://code.earthengine.google.com/bc3998bc98afca2007249eff2d8c6a1a. If you publish the app using the app button above the map and following the clicks, anyone can access the analysis at a URL similar to https://JamesMColl.users.earthengine.app/view/bokdemo as shown in Figure 23.imcenter\nAn interactive web application based on this analysis, available at https://JamesMColl.users.earthengine.app/view/bokdemo\nTaking a step back to look at what we’ve accomplished in this tutorial really accentuates what can be accomplished using GEE:\nWe pulled in 18 years for two daily MODIS datasets Joined them in space and time Filtered and reclassified the resulting dataset Calculated snow cover frequency Performed the linear fit with the snow cover frequency dataset Exported a GeoTIFF and video of the resulting slope map and created a web application so that anyone can interact with the analysis And the entire process probably took less time to perform than it would have needed to download a single day of MODIS data. This is just a small sample of the processing power and capabilities that Google Earth Engine has to offer, and an example of how this platform has revolutionized global scale geospatial analyses. Conclusions & Wrapping up Taken as a whole, GEE represents the most advanced, cloud-based geoprocessing platform to date. Although several other platforms encompass some of these aspects, no suite of tools currently available can replicate the access to geospatial data, relative simplicity of use, and sheer power of analysis that GEE offers. The platform is constantly under development and new features and algorithms are continuously updated. The GEE team and community is also incredibly approachable and helpful with GEE booths are commonly seen at many geospatial meetings, where they demo use cases and announce new features.\nQuestion to submit Using the code and app above, do the following.\nNavigate to someplace interesting in the world. Take a screenshot of the snow cover trend in that area and paste it at the top of your word document. In 1-2 complete paragraphs below that, answer the following: describe and interpret the trend and why it interests you. Why do you think this trend is occurring? What other information do you think you’d need to fully describe why the snow cover is changing in this way?",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 09 B"
    ]
  },
  {
    "objectID": "GEOINT_lab11.html",
    "href": "GEOINT_lab11.html",
    "title": "GEOINT Lab 11",
    "section": "",
    "text": "Lab 11 - Remotely Sensed Imagery and Color Composites\n\nThis lab is a gratefully modified version of lab 10 from Bradley A. Shellito’s Introduction to Geospatial Technologies p752\n\nLearning Objective This chapter’s lab introduces some of the basics of working with multispectral remotely sensed imagery. The goals to take away from this exercise:\nFamiliarize yourself with the basics of multispectral data manipulation in common geospatial softwares Load various bands into the color guns and examine the results Create and examine different color composites Compare the digital numbers of distinct water and environmental features in a remotely sensed satellite image in order to create basic spectral profiles Outline:\nLearning Objective Submission requirements Tutorial Submission Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab2Questions.docx Handout to turn in You are answering the questions (laid out in the word doc above and also included in the tutorial below) as you work through the lab. Use full sentences as necessary to answer the prompts and submit it to blackboard. Copy the folder Chapter 10, which contains a Landsat 8 OLI/TIRS satellite image (called clevelandjuly.img) of Cleveland, Ohio, from July 18, 2018. This file shows a subset of a larger Landsat satellite image. We will discuss Landsat imagery in more detail in Chapter 11, but for now, you just need to know that the OLI and TIRS imagery bands refer to the following portions of the electromagnetic (EM) spectrum, in micrometers (µm): * Band 1: Coastal (0.43 to 0.45 µm) * Band 2: Blue (0.45 to 0.51 µm) * Band 3: Green (0.53 to 0.59 µm) * Band 4: Red (0.64 to 0.67 µm) * Band 5: Near infrared (0.85 to 0.88 µm) * Band 6: Shortwave infrared 1 (1.57 to 1.65 µm) * Band 7: Shortwave infrared 2 (2.11 to 2.29 µm) * Band 8: Panchromatic (0.50 to 0.68 µm) * Band 9: Cirrus (1.36 to 1.38 µm) * Band 10: Thermal infrared 1 (10.60 to 11.19 µm) * Band 11: Thermal infrared 2 (11.50 to 12.51 µm) Keep in mind that Landsat 8 imagery has a 30-meter spatial resolution (except for the panchromatic band, which is 15 meters). Thus, each pixel you will examine covers a 30 meter × 30 meter area on the ground.\nTutorial ArcPro Add image to the map Read in options Symbology options Under Channels, you see the three color guns available to you (red, green, and blue). Each color gun can hold one band. (See the “Lab Data” section for a list of the bands that correspond to the various parts of the electromagnetic spectrum.) The number listed next to each color gun in the dialog box represents the band being displayed with that gun.\nDisplay band 5 in the red color gun, band 4 in the green color gun, and band 3 in the blue color gun.\nAccept the other defaults for now and click OK.\nA new window appears, and the clevelandjuly image begins loading. It might take a minute or two to load completely.\nZoom around the image, paying attention to some of the city, landscape, and water areas.\nQuestion 10.1 What wavelength bands were placed into which color guns? Question 10.2 Why are the colors in the image so strange compared to what you’re normally used to seeing in other imagery (such as Google Earth or Google Maps)? For example, why is most of the landscape red? Question 10.3 In this color composite, what colors are used to display the water, vegetated areas, and urban areas in the image?\nReopen the image, this time using band 4 in the red gun, band 3 in the green gun, and band 2 in the blue gun (referred to as a 4-3-2 combination). When the image reloads, pan and zoom around the image, examining the same areas you just looked at. Question 10.4 What kind of composite did you create in this step? How are the bands displayed in this color composite in relation to their guns? Question 10.5 Why can’t you always use the kind of composite from Question 10.4 when analyzing satellite imagery? Reopen the image yet again, this time using band 7 in the red gun, band 5 in the green gun, and band 3 in the blue gun (referred to as a 7-5-3 combination). When it reloads, pan and zoom around the image, examining the same areas you just looked at. Question 10.6 Once again, what kind of composite was created in this step? Question 10.7 How are vegetated areas being displayed in this color composite (compared with the arrangement in Question 10.4)? Why are they displayed in this color? Examining Color Composites and Color Formations Reopen the image one more time and return to the 5-4-3 combination (that is, band 5 in the red gun, band 4 in the green gun, and band 3 in the blue gun). You can close the other images, as you’ll be working with this one for the rest of the lab. Zoom and move around the image to find and examine Burke Lakefront Airport, From its shape and the pattern of the runways, you should be able to clearly identify it in the Landsat image.imcenter Examine the airfield and its surroundings. Question 10.8 Why do the areas in between the runways appear red? Open a new image, this time with a 4-5-3 combination. Examine Burke Lakefront Airport in this new image and compare it to the one you’ve been working with. Question 10.9 Why do the areas in between the runways now appear bright green? Open another new image, this time with a 4-3-5 combination. Examine Burke Lakefront Airport in this new image and compare it with the others you’ve been working with. Question 10.10 Why do the areas in between the runways now appear blue? At this point, keep only the 5-4-3 image open and close the other two. Examining Specific Digital Numbers and Spectral Profiles Regardless of how the pixels are displayed in the image, each pixel in each band of the Landsat 8 image has a specific digital number set in the 0–65535 range. By examining those pixel values for each band, you can chart a basic spectral profile of some features in the image.\nZoom in to the area around Cleveland’s waterfront and identify the urban areas. (In the image, these will mostly be the white or cyan regions.) From the Window pull-down menu, select New Selection Graph. Another (empty) window (called Selection Graph) opens in MultiSpec. In the image, locate a pixel that’s a good example of an urban or developed area. When the cursor changes to a cross shape, click the pixel once more. Important note: Zoom in so that you are selecting only one pixel with the cursor. A chart appears in the Selection Graph window, showing graphically the DNs for each band at that particular pixel. (See the “Lab Data” section for a list of the bands that correspond to the various parts of the electromagnetic spectrum.) The band numbers are on the x-axis, and the DNs are on the y-axis. Expand or maximize the chart as necessary to better examine the values.imcenter The Selection Graph window now shows the data that can be used to compute a simplified version of a spectral profile for an example of the particular urban land use pixel you selected from the image. For the next question, you need to find a pixel that’s a good example of water and another pixel that’s a good example of vegetation. You also need to translate the data from the chart to a spectral profile for each example. In drawing the profiles from the information on the chart, keep two things in mind: a. First, the values at the bottom of the Selection Graph window represent the numbers of the bands being examined. On the chart below, the actual wavelengths of the bands are plotted, so be very careful to make sure you properly match up each band with its respective wavelength. Note that bands 10 and 11 are not charted because they measure emitted thermal energy rather than reflected energy (as in bands 1 through 9). Note that band 8 (panchromatic) is also not charted. b. Second, the values on the y-axis of the Selection Graph window are DNs, not the percentage of reflection, as seen in a spectral signature diagram. There are a number of factors involved in transforming DNs into percent reflectance values because a DN and the percentage of reflectance don’t have an exact one-to-one ratio, as there are other factors that affect the measurement at the sensor, such as atmospheric effects. However, in this simplified example, you need to chart just the DN for this spectral profile. Examine the image to find a good example of a water pixel and a good example of a vegetation pixel. Plot a spectral profile diagram for the water and vegetation pixels you chose on the following diagram. (Remember to plot values as calculated from the DNs.) imcenter\nExamine your two profiles and answer Questions 10.11 and 10.12. Question 10.11 What information can you gain from the spectral profile for water about the ability of water to reflect and absorb energy? That is, what types of energy are most reflected by water, and what types of energy are most absorbed by water? Question 10.12 What information can you gain from the spectral profile for vegetation about the ability of vegetation to reflect and absorb energy? That is, what types of energy are most reflected by vegetation, and what types of energy are most absorbed by vegetation?\nExit MultiSpec by selecting Exit from the File pull-down menu. There’s no need to save any work in this exercise.\nQGIS Submission All you have to turn into blackboard for this week is the final image you created above.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 11"
    ]
  },
  {
    "objectID": "GEOINT_lab13.html",
    "href": "GEOINT_lab13.html",
    "title": "GEOINT Lab 13",
    "section": "",
    "text": "Lab 13 - Earth Observing Missions Imagery\nThis lab is a gratefully modified version of lab 12 from Bradley A. Shellito’s Introduction to Geospatial Technologies p880\nLearning Objective This chapter’s lab introduces some of the basics of examining imagery from three different Earth-observing satellite missions: Terra, Aqua, and Suomi NPP. You will be examining data from MOPITT as well as many types of imagery from MODIS and VIIRS. You will also be using online resources from NASA and others in conjunction with Google Earth. The goals for you to take away from this lab:\nUtilize Google Earth Pro as a tool for examining Earth-observing satellite imagery as an overlay Examine the usage and functions of the day-night band imagery from VIIRS Examine the usage and functions of MODIS imagery for various applications Examine the output from MOPITT imagery of global carbon monoxide Examine environmental and climate applications of satellite imagery from MODIS for land surface temperature, sea surface temperature, and snow cover Outline:\nLearning Objective Submission requirements Google Earth Pro Viewing VIIRS Satellite Imagery Overlays with Google Earth Pro Wrapping up Submission requirements Materials (click to download)\nData Name Description GEOG111_Lab2Questions.docx Handout to turn in You are answering the questions (laid out in the word doc above and also included in the tutorial below) as you work through the lab. Use full sentences as necessary to answer the prompts and submit it to blackboard. Copy the folder Chapter 12, which contains the following GeoTIFF datasets from NASA’s Earth Observatory: * Bakken_vir_2012317_geo.tif, a VIIRS image showing a section of northwestern North Dakota at night * russia_tmo_2012170_fires_geo.tif, a MODIS image showing fires in Siberia * samerica_vir_2012202_geo.tif, a VIIRS image showing a section of the South American eastern coastline at night * irene_amo_2011238_geo.tif, a MODIS image showing Hurricane Irene * Newzealand_amo_2017317_geo.tif, a MODIS image showing an algal bloom off the coast of New Zealand * The folder also contains the following KML dataset from the University of Wisconsin’s Space Science and Engineering Center: * Daily_MODIS_May3, a series of MODIS images covering the United States from May 3, 2017 ##Tutorial\nGoogle Earth Pro Since we arn’t doing much quantitative analysis with this data\nViewing VIIRS Satellite Imagery Overlays with Google Earth Pro Start Google Earth Pro (GEP), and once loaded turn on the option Borders and Labels. From the File pull-down menu, choose Open. Under the pull-down menu where you can change the types of files to open in Google Earth, choose Images to allow GEP to open files of .tif file type. Open the file Bakken_vir_2012317_geo.tif. On the dialog box that pops up, click Crop. imcenter\nWhen prompted, click the mouse somewhere in the middle of North Dakota. The VIIRS imagery should appear on the screen, properly aligned with Google Earth. In the New Image Overlay dialog box, Google Earth gives you some information about the image. Click OK in this dialog box to close it. A new layer called bakken_vir_2012317_geo.tif is added to Google Earth. Although North Dakota is sparsely populated, a lot of lights can be seen by VIIRS at night. These lights are related to the gas and oil drilling sites of the Bakken shale in the region. You may have to zoom out and pan around the imagery to see the whole North Dakota region. More information about this is available through NASA’s Earth Observatory\nQuestion 1 How does the VIIRS imagery help determine where shale and oil drilling is occurring in this section of North Dakota? In Google Earth Pro, in the Places box, right-click on the Bakken_vir_2012317_geo.tif file and choose Delete to remove it. Open the next GeoTIFF file, samerica_vir_2012202_geo.tif, in Google Earth Pro. GEP rotates to the Atlantic coast of South America and shows you an outline box of where the image should be placed. In the dialog box that appears, again click Crop and click the mouse somewhere in the center of the outline box. The VIIRS image appears properly aligned in GEP. In the New Image Overlay dialog box, Google Earth gives you some information about the image. Click OK in that box to close it. You see a VIIRS image captured during the night in July 2012 showing part of the eastern coast of South America. (More information about this is available through NASA’s Earth Observatory at https://earthobservatory.nasa.gov/images/79822/city-lights-of-south-americas-atlantic-coast.) You may have to zoom out and pan around the imagery to see the whole region covered by the imagery. Question 2 How does the VIIRS imagery help determine the spread of cities and populations across this section of South America? How can this imagery be used to aid in measuring demographics such as population density? In Google Earth Pro, in the Places box, right-click on the samerica_vir_2012202_geo.tif file and choose Delete to remove it. 12.2 Viewing MODIS Satellite Imagery Overlays with Google Earth Pro\nOpen the next GeoTIFF image to examine: russia_tmo_2012170_fires_geo.tif. GEP rotates to Russia and shows you an outline box of where the image should be placed. In the dialog box that appears, this time click Scale. The MODIS image appears properly aligned in GEP. In the New Image Overlay dialog box, Google Earth gives you some information about the image. Click OK in that box to close it. You see a MODIS image of wildfires burning in Siberia. (More information about this is available through NASA’s Earth Observatory, at https://earthobservatory.nasa.gov/images/78305/siberia-burns.) You may have to zoom out and pan around the imagery to see the whole region covered by the imagery. Question 3 How are the fires being shown in this MODIS imagery? How is the extent of the fires being tracked via MODIS? (Hint: What else is visible in the MODIS scene, aside from the fires?) In Google Earth Pro, in the Places box, right-click on the russia_tmo_2012170_fires_geo.tif file and choose Delete to remove it. Open the next GeoTIFF image to examine: newzealand_amo_2017317_geo.tif. This is a MODIS image of a phytoplankton bloom off the coast of New Zealand. (More information about this is available through NASA’s Earth Observatory Again, you may first have to zoom out to see the extent of the MODIS image and then zoom in to examine some of the details. Question 4 How is the phytoplankton bloom shown in this MODIS image (that is, what distinguishes the bloom from the surrounding ocean)? What is the approximate size of the bloom (in comparison with the New Zealand coastline)? In Google Earth Pro, in the Places box, right-click on the newzealand_amo_2017317_geo.tif file and choose Delete to remove it. Open the last of the GeoTIFF images to examine: irene_amo_2011238_geo.tif. In the dialog box that appears, click Scale. The MODIS image appears properly aligned in GEP. In the New Image Overlay dialog box, Google Earth gives you some information about the image. Click OK in that box to close it. This is a 2011 MODIS image of Hurricane Irene, which was a massively destructive storm. (More information about monitoring this storm is available at https://earthobservatory.nasa.gov/images/51931/hurricane-irene.) Again, you have to zoom out to see the extent of the imagery of Irene. Question 5 Why is MODIS used for monitoring weather and storms such as Hurricane Irene rather than other satellite systems we’ve discussed, such as WorldView-3 or Landsat 7 or 8? In Google Earth Pro, in the Places box, right-click on the irene_amo_2011238_geo.tif file and choose Delete to remove it. 12.3 Viewing Recent Earth-Observing Imagery with Google Earth Pro\nA system such as MODIS can image almost the entire Earth in a single day, and you can access very recent imagery from it. In Google Earth Pro, from the File pull-down menu select Open and then navigate to the Chapter 12 folder and open the KML file called Daily_MODIS_May3. When the file opens, GEP begins zooming very closely to Earth; you might want to stop it in mid-zoom and then zoom out to see the entire United States in Google Earth. This KML file consists of a series of MODIS images from May 3, 2017, that cover nearly all of the United States. Pan and zoom across the imagery to see the entire area covered by these MODIS images. Question 6 By examining the imagery, you can see that overlapping images were taken and that small pieces of imagery are missing. Approximately how much area of the United States is covered in one MODIS swath? Why do you think there are pieces missing from the imagery? imcenter\nThese MODIS images were downloaded from the University of Wisconsin’s Space Science and Engineering Center’s MODIS Today online tool. To see what’s happening with current MODIS imagery, point your Web browser to http://ge.ssec.wisc.edu/modis-today. When the Website opens, you see the currently available real-time MODIS imagery. Click on the radio buttons for Terra and Aqua to see what type of coverage of MODIS imagery is currently available for each one (as both satellites carry MODIS instruments). Choose either Terra or Aqua, based on which one gives you the better overall coverage for today’s date. If neither is acceptable, click on the Previous Day button and try again with the two satellites. When you have a good MODIS image of the United States to work with, click on Open in Google Earth. If you are prompted with a dialog box, choose to open the file in Google Earth Pro. If not, save the KML to the Chapter 12 folder and then manually open it in Google Earth Pro. You’ll see in Google Earth Pro’s Places box, under Temporary Places, the name of the initial KML you added (t1.17122) placed at the top of the list and then the second KML you added directly from the Website (shown in the graphic above as aqua_today.kml) placed second at the bottom. When you add KML/KMZ files to Google Earth Pro, the order of priority for displaying them goes from top to bottom in the Places box. So, whatever is at the bottom of the stack gets displayed first, then the next up from the bottom is displayed on top of it, until whatever is at the top of the stack gets displayed over everything else. (This will be important in the next section of the lab.) You can turn layers on and off by checking and unchecking their boxes. For now, display only the new KML file you downloaded from the Website. Question 7 Based on your examination of today’s MODIS imagery, are there any notable weather patterns you can see forming, such as large storms? How much of the United States is visible and not clouded over? Turn off both of the MODIS images. 12.4 Using the NASA Earth Observations (NEO) Web Resources\nIn your Web browser, go to http://neo.sci.gsfc.nasa.gov. This is the Website for NEO (NASA Earth Observations), an online source of downloadable Earth observation satellite imagery. In this portion of the lab, you’ll be using NEO’s imagery in KMZ format in conjunction with Google Earth Pro. Rather than provide you with only a view of a flat satellite image, NEO gives you the option of examining the imagery draped across a global view in GEP. imcenter\nClick on the Atmosphere option at the top of the page. Select the option Carbon Monoxide. You see an image showing the global carbon monoxide concentration for one month, collected using the MOPITT instrument aboard Terra. (See the About this dataset section of the NEO Website for more detailed information.) Select 2016 for the year and then choose the option June 2016. Select Google Earth from the Downloads File Type pull-down menu and click on the option 1440 × 720 to begin the download of the KMZ file. If prompted, open the file in Google Earth Pro. If you had to download the KMZ file, locate it on your computer and manually open the file in GEP. imcenter\nRotate Google Earth Pro to examine the MOPITT imagery draped over the globe. Make sure there’s a checkmark in Google Earth Pro’s Borders and Labels box. Question 8 Geographically, where were the highest concentrations of carbon monoxide for this month? Turn off the MOPITT Carbon Monoxide image in Google Earth Pro. Back on the NEO Website, select the Energy tab and then select Land Surface Temperature [Day]. Choose the option for 2019 and then select February. Use Google Earth for the Downloads File Type option and click on the 1440 × 720 option. If prompted, open the file in Google Earth Pro. If you had to download the KMZ file, locate it on your computer and manually open the file in Google Earth Pro. Rotate Google Earth Pro to examine the new MODIS image. Question 9 Geographically, where were the lowest daytime land surface temperatures for this month? Question 10 Geographically, where specifically in South America are the lowest daytime land temperatures for this month? Why are temperatures so low here when the rest of the continent has higher daytime land surface temperatures? (Hint: You may want to zoom in on some of these areas and then examine them with and without the MODIS imagery turned on.) Turn off the MODIS Land Surface Temperature image in Google Earth Pro. Back on the NEO Website, select the Ocean tab and then select Sea Surface Temperature 2002 + (MODIS). Choose 2019 for the year and then select January. Use Google Earth for the Downloads File Type option and click on the 1440 × 720 option. If prompted, open the file in Google Earth Pro. If you had to download the KMZ file, locate it on your computer and manually open the file in Google Earth Pro. Rotate Google Earth Pro and zoom in to examine the new MODIS composite image. Question 11 Geographically, where were the areas with the warmest sea surface temperatures on the planet for this month? (Hint: You may want to turn on the Grid functions in Google Earth Pro—under the View pull-down menu—to add further geographic context to help answer this question.) Question 12 How do the sea surface temperatures of the Atlantic Ocean off the coasts of Lisbon, Portugal; Saint Pierre, Newfoundland; London, England; and New York City, New York, all compare to one another (that is, rank them from warmest to coolest temperatures) for this month? In Google Earth Pro, turn off this MODIS image. Back on the NEO Website, select the Land tab and then select Snow Cover. Choose 2019 for the year and then select February. Use Google Earth for the Downloads File Type option and click on the 1440 × 720 option. If prompted, open the file in Google Earth Pro. If you had to download the KMZ file, locate it on your computer and manually open the file in Google Earth Pro.17. Rotate Google Earth Pro and zoom in to examine the new MODIS image, showing snow cover on Earth. Question 13 Geographically, where were the greatest concentrations of snow cover in the southern hemisphere in February 2019 (other than Antarctica and the south polar region)? In Google Earth Pro, turn off this MODIS image. Back on the NEO Website, select the Land tab and then select Snow Cover. Choose 2018 for the year and then select August. Use Google Earth for the Downloads File Type option and click on the 1440 × 720 option. If prompted, open the file in Google Earth Pro. If you had to download the KMZ file, locate it on your computer and manually open the file in Google Earth Pro. Rotate Google Earth Pro and zoom in to examine this second MODIS image showing snow cover on Earth. Question 14 Geographically, where were the greatest concentrations of snow cover in the northern hemisphere in August 2018 (other than the north pole and the Arctic) At this point, you can exit Google Earth Pro by selecting Exit from the File pull-down menu. Wrapping up There is no need to save anything from this lab, so when done you can simply close without saving. Submit your answers to the questions on blackboard.",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs",
      "GEOINT Lab 13"
    ]
  },
  {
    "objectID": "GEOINT_labs.html",
    "href": "GEOINT_labs.html",
    "title": "GEOINT labs",
    "section": "",
    "text": "Because this is an introductory class, the labs will be pretty explicitly hand-holdy. Make sure you take a second and read the instructions and digest what it is they are asking you to do. If you are more computer savvy or otherwise technically inclined, feel free to try and guess or intuitively feel out the instructions instead. At the end of the day, these are just computers so the consequences of a bad input combination or operation are easily recoverable. These labs follow the class lecture. While only a few are directly cumulative, they do progressively become more involved as the semester progresses.\n\nSeveral of these labs are gratefully pilfered and modified from labs in Bradley A. Shellito’s Introduction to Geospatial Technologies\n\n\n\n\n\n\n\nNote\n\n\n\nThis lab uses a few different software:\n\nGoogle Earth, A pretty straightforward installation with nothing special to it.\nArcPro & QGIS, GIS software can be difficult to correctly install. This page will help get you over most of those hurdles should you encounter an error.\nGoogle Earth engine, which requires Google approval an will need to be applied for prior to Lab 9.\nA photo viewer. Although the default photo viewer will suffice, if you want a more robust option I use IRFANVIEW (Windows only, sorry mac)\n\n\n\nIt will be covered more in class, but unless explicitly stated otherwise labs will be due a week after the lab session meets.\n\nLab 01 - Computer basics\nLab 02 - Introduction to Google Earth Pro\nLab 03 - Coordinates and Position Measurements\nLab 04 - GPS\nLab 05 - GIS introduction\nLab 06 - spatial analysis\nLab 07 - Digital Terrain Analysis\nLab 08 - Map making\nLab 09A - 3D Modeling and Visualization\nLab 09B - Cloud based GIS\nLab 10 - Visual Imagery Interpretation\nLab 11 - Remotely Sensed Imagery and Color Composites\nLab 12 - Landsat 8 Imagery\nLab 13 - Earth Observing Missions Imagery\nLab 14 - Final Lab activity",
    "crumbs": [
      "Introduction to GEOINT",
      "Labs"
    ]
  },
  {
    "objectID": "GR.html#how-to",
    "href": "GR.html#how-to",
    "title": "Georeferencing",
    "section": "How to",
    "text": "How to\n\nGeoreferenced arieal photography",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Georeferencing"
    ]
  },
  {
    "objectID": "GT.html#how-to",
    "href": "GT.html#how-to",
    "title": "Geotagging",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Analysis",
      "Network",
      "Geotagging"
    ]
  },
  {
    "objectID": "GZ.html#how-to",
    "href": "GZ.html#how-to",
    "title": "Generalization",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Math",
      "Generalization"
    ]
  },
  {
    "objectID": "HM.html#how-to",
    "href": "HM.html#how-to",
    "title": "Huff Model",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Analysis",
      "Network",
      "Huff Model"
    ]
  },
  {
    "objectID": "IS.html#how-to",
    "href": "IS.html#how-to",
    "title": "Image Stretching",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing",
      "Image Stretching"
    ]
  },
  {
    "objectID": "JColl_Defense_Presentation.html#section",
    "href": "JColl_Defense_Presentation.html#section",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "Meeting link: https://github.com/quarto-dev/quarto-web/blob/main/docs/presentations/revealjs/examples/per-slide-footer.qmd Derp meeting Monday, November 25 · 12:00 – 12:45pm Time zone: America/Denver Google Meet joining info Video call link: https://meet.google.com/oba- Or dial: (US) +1 More phone numbers: https://tel.meet/oba-mffg"
  },
  {
    "objectID": "JColl_Defense_Presentation.html#the-incompetent-modeler",
    "href": "JColl_Defense_Presentation.html#the-incompetent-modeler",
    "title": "So long as you get the GIS&T of it",
    "section": "The incompetent modeler",
    "text": "The incompetent modeler\n\n\n\nI like this Photoshop because not only is my heat miser haircut in full glory, but like Bob I now have PTSD and I really admire the artfully effortless style in which he sculpts the landscape under his brush. It’s what I’m struggling and striving to do with my efforts in fim, and hopefully by the end of this I’ll have a picture of something worth your attention.\nFor: Group updates, humans, and humanity. Title slide background from: https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExazFtdGlvYnVmcHl5Z3FpZGJkYTZsMmw3eXJuZ3A3YnJta2UxbTN2diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/111RMoa4bwH9zq/giphy.gif Background images from: Water PNGs by Vecteezy, https://www.vecteezy.com/members/ahasanaraakter"
  },
  {
    "objectID": "JColl_Defense_Presentation.html#presentation-navigation-tips",
    "href": "JColl_Defense_Presentation.html#presentation-navigation-tips",
    "title": "So long as you get the GIS&T of it",
    "section": "Presentation Navigation Tips",
    "text": "Presentation Navigation Tips\n\n\n\nSlide layout : This deck has one axis, use any key to advance.\n: This deck is narrative-oriented\n\nItems linked/bordered in green are cited in the tooltip on hover.\nItems linked/bordered in blue are hyperlinked to relevant resources.\n\n Photos are Allowed |  Questions are Encouraged\n: ~5 minutes | Last updated:08/25/2025 20:48:42\n! PLEASE !\nInterrupt me and ask questions or clarifications.\nI’m here to talk with you now, not to these slides.\n\n\nControl tips\n\nMy preferred (FOSS) flavor of slidedecks, revealjs, has intuitive but none the less unconventional PowerPoint presentation controls:\n\nSlides dynamically resize to use the entirety of the browser window, but you can still fullscreen with F.\n\nThis slide has a red border indicating the content extent.\n\nSlide navigation is mode dependent. If there are vertical slides, press space, N, or the down arrow key, not the right arrow to advance slides\nPress M to open to the menu, Press O for the slide deck overview, Press B to black out the presentation screen, Press S for a speaker view.\nYou can use the chalkboard to freemouse/touchpad draw.\nSlides should render as designed1 but you can press Alt/Opt + click on the slide to zoom in. Increase text size with Alt/Opt + +, Alt/Opt + - to decrease, and Alt/Opt + 0 reset to the default scale.\nPress C to declare victory and head home.2\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\n\n\n\n\nSpeaker notes\n\ntested in an updated version of google chromeA favored quote from one of my giants: Dr. David Maidment"
  },
  {
    "objectID": "JColl_Defense_Presentation.html#section-1",
    "href": "JColl_Defense_Presentation.html#section-1",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "If I have seen further than others, it is by standing upon the shoulders of giants.\n- Sir Isaac Newton\n\n\n\n\n\nThere are far too many giants to thank.\nAI Prompt: Can you draw me a picture of Sir Isaac Newton standing in a river shaded by many apple trees"
  },
  {
    "objectID": "JColl_Defense_Presentation.html#project-status-and-updates",
    "href": "JColl_Defense_Presentation.html#project-status-and-updates",
    "title": "So long as you get the GIS&T of it",
    "section": "Project Status and Updates",
    "text": "Project Status and Updates\n\n\n\n\n\n\n\n\n\n\n%%---\n%%displayMode: compact\n%%---\ngantt\n    dateFormat  YYYY-MM-DD\n    title       The 7 P's of Project Management\n    excludes    weekends\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    %% todayMarker stroke-width:5px,stroke:#0f0,opacity:0.5\n\n    section GANTT Chart Key\n    Completed task                :done,       des1, 2025-09-01, 20d\n    Completed critical task       :crit,done,  des2, after des1, 20d\n    Active task                   :active,     des3, after des2, 20d\n    Future task                   :            des4, after des3, 20d\n    critical task                 :crit,       des5, after des4, 20d\n\n    section Topics\n    \n    section Deadlines and Events\n    Labor Day                     : 2025-09-01, 1d\n    Columbus Day                  : 2025-10-13, 1d\n    Veterans Day                  : 2025-11-11, 1d\n    Thanksgiving                  : 2025-11-27, 1d \n    Finals week                   : 2025-12-08, 5d\n    Winter break                  : 2025-12-22, 10d\n    MLK Jr. Day                   : 2026-01-19, 1d \n    First day of spring semester  : 2026-01-20, 1d \n    Washington’s Birthday         : 2026-02-16, 1d \n    submission for commencement   : 2026-03-02, 1d \n    Spring break                  : 2026-03-16, 5d \n    Finals week                   : 2026-05-11, 5d\n    commencement                  : 2026-05-17, 2d \n\n\n\n\n\n\n\n\nRisks:\n\n\n\n1: I give up, unsustainably miserable.\n\n\n\n\n\n← likelihood →\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\nProgress:\n\n\n\nI have contributed nothing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n← consequence →\n\n\n\n\n\n\nI can’t computer and making it beep is not science"
  },
  {
    "objectID": "JColl_Defense_Presentation.html#a-statement",
    "href": "JColl_Defense_Presentation.html#a-statement",
    "title": "So long as you get the GIS&T of it",
    "section": "A Statement",
    "text": "A Statement\nI believe a competent modeler can use variations in modeling surface and the uncertainty in parameterizations to reach a suitable equifinality prediction.\nTherefore, I find it very hard to find something useful to say in the face of competing objectives and value judgments, and a shameful lack of observation and geographic context.\n\n\nI used to go outside…"
  },
  {
    "objectID": "JColl_Defense_Presentation.html#a-contribution",
    "href": "JColl_Defense_Presentation.html#a-contribution",
    "title": "So long as you get the GIS&T of it",
    "section": "A Contribution",
    "text": "A Contribution\nMy contribution to the science is a public, reproducible demonstration of technical and methodological excellence (ha). In order to communicate the nuances surroundings the quantification of accuracy and equifinality of FIM, I introduce concepts including mapping and modeling surfaces, and library creation and access patterns.\n\n\n\n\nI used to go outside…"
  },
  {
    "objectID": "JColl_Defense_Presentation.html#making-sense-around-wickedly-fun-problems",
    "href": "JColl_Defense_Presentation.html#making-sense-around-wickedly-fun-problems",
    "title": "So long as you get the GIS&T of it",
    "section": "Making Sense Around Wicked(ly fun!) Problems",
    "text": "Making Sense Around Wicked(ly fun!) Problems\n\n\n\n\n\n\n\nmindmap\n  Wicked Problem\n    Every problem is unique\n      Water is not the same as a biological outbreak\n    Every problem is connected to others\n      FEWS\n    There is no clear problem definition\n    Are multi-causal, multi-scaler, and interconnected.\n    Include multiple, invested stakeholders with different values, goals, and objectives.\n    Straddle organizational and disciplinary boundaries\n    Solutions to an aspect have implications/ramifications across the system\n    Solutions are not right or wrong, but good and bad\n    Can be difficult to measure or evaluate effects of implemented solutions\n    Problems are never completely solved\n      They are wicked problems, not wicked puzzles\n\n\n\n\n\n\n\nI demonstrate that these are critical factors in the execution of FIModeling, and that with this new framing we can try and make a little more sense around our wicked problem.\n\n\nI used to go outside…"
  },
  {
    "objectID": "JColl_Defense_Presentation.html#afraid-of-change-leave-it-here.",
    "href": "JColl_Defense_Presentation.html#afraid-of-change-leave-it-here.",
    "title": "So long as you get the GIS&T of it",
    "section": "Afraid of change? Leave it here.",
    "text": "Afraid of change? Leave it here.\n\n\n\n\n\n\n\n\n\n\n\nAdd a tip? \n\n\n\n\n\n\n\n 18% $3.50 \n\n\n\n\n\n\n 25% $8.49 \n\n\n\n\n\n\n 30% $16.99 \n\n\n\n\n\n\n\n\nCustom\n\n\n\n\n\n\n\n\nNo Tip\n\n\n\n\n\n\n\nAmounts based on premium pricing - Soon to be divorced ex-tech lead/under-housed crazy in a Toyota down by the river."
  },
  {
    "objectID": "KD.html#how-to",
    "href": "KD.html#how-to",
    "title": "Kernel Density",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Statistics",
      "Kernel Density"
    ]
  },
  {
    "objectID": "KR.html#how-to",
    "href": "KR.html#how-to",
    "title": "Kriging",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "LR.html#how-to",
    "href": "LR.html#how-to",
    "title": "Linear Referencing",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Linear Referencing"
    ]
  },
  {
    "objectID": "ME.html#how-to",
    "href": "ME.html#how-to",
    "title": "Mensuration",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing",
      "Mensuration"
    ]
  },
  {
    "objectID": "NW.html",
    "href": "NW.html",
    "title": "Network Analysis",
    "section": "",
    "text": "hmmm"
  },
  {
    "objectID": "OS.html#how-to",
    "href": "OS.html#how-to",
    "title": "Optimal Site",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Analysis",
      "Network",
      "Optimal Site"
    ]
  },
  {
    "objectID": "OWP_FIM.html",
    "href": "OWP_FIM.html",
    "title": "OWP FIM",
    "section": "",
    "text": "udunits database from /usr/share/xml/udunits/udunits2.xml\n\n\nThis build of rgl does not include OpenGL functions.  Use\n rglwidget() to display results, e.g. via options(rgl.printRglwidget = TRUE).",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "OWP FIM"
    ]
  },
  {
    "objectID": "OWP_FIM.html#executive-summary",
    "href": "OWP_FIM.html#executive-summary",
    "title": "OWP FIM",
    "section": "Executive summary",
    "text": "Executive summary\nThe generation of flood inundation maps is expensive, technically demanding, and heavily dependent on geography and user specified inputs. With the advances of the National Water Model and geospatial approximations to the hydraulic sciences, the ability to generate flood inundation maps via a Height Above Nearest Drainage and Synthetic Rating Curve library was realized. This technique is used in NOAA Office of Water Prediction operational flood forecasting as the FIM4 HAND libraries sheparded by the National Water Center. See the repo and accompanying wiki for more details.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "OWP FIM"
    ]
  },
  {
    "objectID": "OWP_FIM.html#intro",
    "href": "OWP_FIM.html#intro",
    "title": "OWP FIM",
    "section": "Intro",
    "text": "Intro\nAs one of the first methods of tying the National Water Model to a flood map, the once NFIE then CFIM &gt; FIM2 &gt; Cahaba (FIM3) &gt; GMS (FIM4) codebase is the Office of Water Predictions default means of creating FIM libraries. This current iteration of the OWP HAND based FIM repo is written in Python, distributed as a docker’d environment, and is executed in AWS cloud services. This repo performs the steps needed to create a Hight Above Nearest Drainage (HAND) Relative Elevation model (REM) and Synthetic Rating Curve (SRC). As both a research and an operational toolchain, this repo furthermore serves as an accessible entry point to some of the more “modern” computational patterns and better practices, and can be deployed in a number of ways.\n\n\n\n\n\n\nQuickstart for advanced users\n\n\n\nThe most relevant stearing files are the /src/bash_variables.env You enable HUCS with an inclusion in a list a la /data/inputs_huc_lists/testunit.lst\nWhile there are “smart defaults” for many of the runtime arguments, this tool is designed primarily for use at OWP and with the goal of generating outputs for the NWS at a continental scale. If you are not that use case, you may need to tweak the inputs intelligently and remember to update config\\params_template.env to reflect the data you have available in your environment.\n\nYou can look for help with the -h flag: /foss_fim/fim_pipeline.sh -h\n\n\nDid that not make sense? That’s O.K! See below for more verbose instructions and relevent background.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "OWP FIM"
    ]
  },
  {
    "objectID": "OWP_FIM.html#resources",
    "href": "OWP_FIM.html#resources",
    "title": "OWP FIM",
    "section": "Resources",
    "text": "Resources\nhttps://github.com/NOAA-OWP/inundation-mapping",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "OWP FIM"
    ]
  },
  {
    "objectID": "OWP_FIM.html#tutorials",
    "href": "OWP_FIM.html#tutorials",
    "title": "OWP FIM",
    "section": "Tutorials",
    "text": "Tutorials",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "OWP FIM"
    ]
  },
  {
    "objectID": "OWP_FIM.html#explainers",
    "href": "OWP_FIM.html#explainers",
    "title": "OWP FIM",
    "section": "Explainers",
    "text": "Explainers\n\nWhat is FIM4 doing?\nThe process of creating the FIM library is a complex one. See this repo workflow broken down in visual form here (also note the beautiful graphic from the wiki; pilfered and pasted at the top of this page as well!).",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "OWP FIM"
    ]
  },
  {
    "objectID": "OWP_FIM.html#how-tos",
    "href": "OWP_FIM.html#how-tos",
    "title": "OWP FIM",
    "section": "How-To’s",
    "text": "How-To’s\n\nHow to “run” the repo from pull to HAND.tif\n\nSee How I install and use docker for tips on setting up the environment\n\n\n\nInstallation\n\nGather Input data\nGrab input data.\n```{bash}\n# aws config\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/3dep_dems/10m_5070 . --exclude '*' --include 'HUC6_1209**_dem.tif' --request-payer requester --dryrun\naws s3 cp s3://noaa-nws-owp-fim/hand_fim/inputs/3dep_dems/10m_5070/fim_seamless_3dep_dem_10m_5070.vrt . --request-payer requester --dryrun\naws s3 cp s3://noaa-nws-owp-fim/hand_fim/inputs/3dep_dems/10m_5070/HUC6_dem_domain.gpkg . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/ahps_sites . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/bathymetery . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/huc_lists . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/landsea . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/nhdplus_vectors_aggregate . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/nld_vectors . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/nwm_hydrofabric . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/rating_curve . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/usgs_gages . --request-payer requester --dryrun\naws s3 cp --recursive s3://noaa-nws-owp-fim/hand_fim/inputs/wbd . --request-payer requester --dryrun\n```\n\n\nStage inputs\nThe pre_clip step shaves lots of time off the standard pattern of file &gt; open.\n\n\nBuild the docker image\n\nOpen admin powershell\n\n```{powershell}\nwsl\n```\n\nGet an error? Maybe start docker?\n\n```{sh}\nsystemctl start docker\n```\n\nPull the repo and build the Image\n\n```{sh}\n&gt; cd /mnt/OWP_FIM/inundation-mapping/\ngit clone https://github.com/NOAA-OWP/inundation-mapping.git\ncd ./inundation-mapping/\ndocker build -f Dockerfile.dev -t owpfim:1.0 /mnt/OWP_FIM/inundation-mapping/inundation-mapping\n```\n\nSee the changelog for why .dev is now included\n\n\n\ncreate the container\n```{powershell}\ndocker run \\\n    --rm \\\n    -it \\\n    --name 'FIM4' \\\n    -v /.../root/projects/floodmapping/methods/OWP_FIM/inundation-mapping/inundation-mapping/:/foss_fim \\\n    -v /.../root/projects/floodmapping/methods/OWP_FIM/outputs/:/outputs \\\n    -v /.../data/raw/FIM4/:/data \\\n    owpfim:1.1\n```\n\n\nRun pipeline\nThe most relevant steering files are the /src/bash_variables.env You enable HUCS with an inclusion in a list a la /data/inputs_huc_lists/testunit.lst\nWhile there are “smart defaults” for many of the runtime arguments, this tool is designed primarily for use at OWP and with the goal of generating outputs for the NWS at a continental scale. If you are not that use case, you may need to tweak the inputs intelligently and remember to update config\\params_template.env to reflect the data you have available in your environment.\n\nYou can always look for help with the -h flag: /foss_fim/fim_pipeline.sh -h\n\n```{sh}\nfim_pipeline.sh -u '12090301' -n FIM4 -ud NONE -bd NONE -zd NONE -skipcal\n```\n\n\n\n\nCopy files into data path\n[[20220908105839]]\n\n\nMake input files\n```{sh}\npython3 /foss_fim/data/wbd/generate_pre_clip_fim_huc8.py -n /data/inputs/pre_clip_huc8/20240927 -u /data/inputs/huc_lists/testunit.lst\n\n#python3 /foss_fim/data/aggregate_vector_inputs.py\npython3 /foss_fim/src/preprocess_rasters.py -dem_dir /data/inputs/nhdplus_rasters/\n```\n```{sh}\npython3 /foss_fim/data/acquire_and_preprocess_inputs.py -u 1209 &lt;huc4s_to_process&gt;\n#python3 /foss_fim/data/aggregate_vector_inputs.py\npython3 /foss_fim/src/preprocess_rasters.py -dem_dir /data/inputs/nhdplus_rasters/\n```\npython3 /foss_fim/data/acquire_and_preprocess_inputs.py -u 0804 python3 /foss_fim/data/preprocess_rasters.py -dem_dir /data/inputs/nhdplus_rasters/\nhttps://github.com/NOAA-OWP/inundation-mapping/blob/dev/data/bridges/pull_osm_bridges.py#L321 https://github.com/NOAA-OWP/inundation-mapping/blob/dev/data/wbd/generate_pre_clip_fim_huc8.py#L136\n\n\nRun pipeline\n```{sh}\nfim_pipeline.sh -u '12090301' -n raw_env -ud NONE -bd NONE -zd NONE -skipcal\n```",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "OWP FIM"
    ]
  },
  {
    "objectID": "OWP_revealjs_template.html#hydrofabric",
    "href": "OWP_revealjs_template.html#hydrofabric",
    "title": "So long as you get the GIS&T of it",
    "section": "Hydrofabric",
    "text": "Hydrofabric\n\n\n\n\n\n\nGoals:\n\nTo provide a structured template for us to use in presentations.\n\nOutcomes and Takeaways:\n\nA template to use.\nMore durable and data-parameterized documentation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation (placeholder)\n\n\n\n\nSlide layout:  default     Items linked/bordered in green are cited in the tooltip on hover.\n: narrative-oriented     Items linked/bordered in blue are hyperlinked to relevant resources.\n\n\nPreliminary\n\n\nThese are the speaker notes: Some things to touch on: * The div from lines 161 to 167 are just little helpers I like to leave for users who stumble across the presentation and don’t have one of us there to talk to the slides. * The .fragment is the way to animate the deck, I place it there just so you see some extra formatting options available to you. See more formatting tips in the official docs. * I avoid vertical slides; you want the user to consume the presented information, not learn a new control schema and the left and right arrow keys are too common a navigation method to break without cause."
  },
  {
    "objectID": "OWP_revealjs_template.html#some-more-slides",
    "href": "OWP_revealjs_template.html#some-more-slides",
    "title": "So long as you get the GIS&T of it",
    "section": "Some more slides",
    "text": "Some more slides"
  },
  {
    "objectID": "OWP_revealjs_template.html#whats-next",
    "href": "OWP_revealjs_template.html#whats-next",
    "title": "So long as you get the GIS&T of it",
    "section": "What’s next?",
    "text": "What’s next?\n\n\n\nOutcomes and Takeaways:\n\nA template to use.\nMore durable and data-parameterized documentation.\n\nNext Steps:\n\nThese are future cards?\nSolve Hydrology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation (placeholder)\n\n\n\n\nI like to repeat the start, as Trey likes to say: Tell them what you will tell them, tell them, and then tell them what you told them."
  },
  {
    "objectID": "PD_Histogram.html",
    "href": "PD_Histogram.html",
    "title": "Histogram",
    "section": "",
    "text": "path &lt;- \"~/data/raw/hydrofabric/prototype/\"\nflowlines_ml &lt;- arrow::read_parquet(file.path(path,\"flowline_attributes_ml.parquet\",fsep = .Platform$file.sep))\n\nin_channel_depth = (((flowlines_ml$TopWdth-flowlines_ml$BtmWdth)/2)/sin(atan(flowlines_ml$ChSlp)))*sin(90-atan(flowlines_ml$ChSlp)) %&gt;% as.data.frame()\nmax(in_channel_depth$.,na.rm=T)\nggplot2::ggplot(in_channel_depth %&gt;% dplyr::filter(.&lt;500), ggplot2::aes(x=.)) +\n  ggplot2::geom_histogram(binwidth=3, fill=\"#69b3a2\", color=\"#e9ecef\", alpha=0.9) +\n  ggplot2::ggtitle(\"'in-channel' depth historgram\") +\n  cowplot::theme_half_open() +\n  cowplot::background_grid()\nbase_data &lt;- \"~/data/raw/hydrofabric/v2.2/conus/conus_nextgen.gpkg\"\ndat &lt;- sf::st_read(base_data,layer=\"divides\")\n\ndatp = sf::st_transform(dat,sf::st_crs('EPSG:3857'))\ndatp$a5070 &lt;- sf::st_area(sf::st_transform(dat,sf::st_crs('EPSG:5070')))\ndatp$an5070 &lt;- datp$a5070 |&gt; as.numeric()\nmean_size &lt;- mean(datp$a5070,na.rm=T)\nmedian_size &lt;- median(datp$a5070,na.rm=T)\n\nchart_data &lt;- as.data.frame(datp[datp$an5070 &lt; 0.75e8,]$an5070)\ncolnames(chart_data) &lt;- 'size'\nchart_data$size = round(chart_data$size*1e-6,2)\n\nmean_size &lt;- mean(chart_data$size,na.rm=T)\nmedian_size &lt;- median(chart_data$size,na.rm=T)\n\nchart &lt;- ggplot2::ggplot(chart_data, ggplot2::aes(x=size)) +\n  ggplot2::geom_histogram(ggplot2::aes(y=ggplot2::after_stat(count)/sum(ggplot2::after_stat(count))), binwidth = 1, center = 0.5, fill = \"lightblue\", color = \"black\") +\n  ggplot2::geom_vline(ggplot2::aes(xintercept = as.numeric(mean_size), color = \"mean\"), linewidth = 1) +\n  ggplot2::geom_vline(ggplot2::aes(xintercept = as.numeric(median_size), color = \"median\"), linewidth = 1) +\n  ggplot2::labs(title = \"NextGen Divide Area Distribution\", x = \"Area (km2 - EPSG:5070)\", y = \"Frequency\", \n                caption = glue::glue(\"for basins under 75 km2 - {nrow(dat) - nrow(chart_data)} of {nrow(dat)} rows filtered\")) +\n  ggplot2::scale_y_continuous(expand = ggplot2::expansion(mult = c(0, 0.02))) +\n  ggplot2::scale_x_continuous(expand = c(0,0)) +\n  ggplot2::scale_color_manual(name = \"Callouts\", labels = c(glue::glue(\"Data mean: {round(mean_size,2)} km2\"), glue::glue(\"Data median: {median_size} km2\")), values = c(mean = \"red\", median = \"blue\")) +\n  cowplot::theme_half_open() +\n  cowplot::background_grid() +\n  ggplot2::theme(legend.position = c(.75, .95),\n                 # legend.justification = c(\"right\", \"top\"),\n                 # legend.box.just = \"right\",\n                 legend.margin = ggplot2::margin(6, 6, 6, 6)\n  ) \n\nchart\n\n\n\n_98c5dedbb98e7078a3ef610e4d37e6e8.png\n\n\n\n\n\nhttps://stackoverflow.com/questions/10770698/understanding-dates-and-plotting-a-histogram-with-ggplot2-in-r"
  },
  {
    "objectID": "PD_Histogram.html#examples",
    "href": "PD_Histogram.html#examples",
    "title": "Histogram",
    "section": "",
    "text": "path &lt;- \"~/data/raw/hydrofabric/prototype/\"\nflowlines_ml &lt;- arrow::read_parquet(file.path(path,\"flowline_attributes_ml.parquet\",fsep = .Platform$file.sep))\n\nin_channel_depth = (((flowlines_ml$TopWdth-flowlines_ml$BtmWdth)/2)/sin(atan(flowlines_ml$ChSlp)))*sin(90-atan(flowlines_ml$ChSlp)) %&gt;% as.data.frame()\nmax(in_channel_depth$.,na.rm=T)\nggplot2::ggplot(in_channel_depth %&gt;% dplyr::filter(.&lt;500), ggplot2::aes(x=.)) +\n  ggplot2::geom_histogram(binwidth=3, fill=\"#69b3a2\", color=\"#e9ecef\", alpha=0.9) +\n  ggplot2::ggtitle(\"'in-channel' depth historgram\") +\n  cowplot::theme_half_open() +\n  cowplot::background_grid()\nbase_data &lt;- \"~/data/raw/hydrofabric/v2.2/conus/conus_nextgen.gpkg\"\ndat &lt;- sf::st_read(base_data,layer=\"divides\")\n\ndatp = sf::st_transform(dat,sf::st_crs('EPSG:3857'))\ndatp$a5070 &lt;- sf::st_area(sf::st_transform(dat,sf::st_crs('EPSG:5070')))\ndatp$an5070 &lt;- datp$a5070 |&gt; as.numeric()\nmean_size &lt;- mean(datp$a5070,na.rm=T)\nmedian_size &lt;- median(datp$a5070,na.rm=T)\n\nchart_data &lt;- as.data.frame(datp[datp$an5070 &lt; 0.75e8,]$an5070)\ncolnames(chart_data) &lt;- 'size'\nchart_data$size = round(chart_data$size*1e-6,2)\n\nmean_size &lt;- mean(chart_data$size,na.rm=T)\nmedian_size &lt;- median(chart_data$size,na.rm=T)\n\nchart &lt;- ggplot2::ggplot(chart_data, ggplot2::aes(x=size)) +\n  ggplot2::geom_histogram(ggplot2::aes(y=ggplot2::after_stat(count)/sum(ggplot2::after_stat(count))), binwidth = 1, center = 0.5, fill = \"lightblue\", color = \"black\") +\n  ggplot2::geom_vline(ggplot2::aes(xintercept = as.numeric(mean_size), color = \"mean\"), linewidth = 1) +\n  ggplot2::geom_vline(ggplot2::aes(xintercept = as.numeric(median_size), color = \"median\"), linewidth = 1) +\n  ggplot2::labs(title = \"NextGen Divide Area Distribution\", x = \"Area (km2 - EPSG:5070)\", y = \"Frequency\", \n                caption = glue::glue(\"for basins under 75 km2 - {nrow(dat) - nrow(chart_data)} of {nrow(dat)} rows filtered\")) +\n  ggplot2::scale_y_continuous(expand = ggplot2::expansion(mult = c(0, 0.02))) +\n  ggplot2::scale_x_continuous(expand = c(0,0)) +\n  ggplot2::scale_color_manual(name = \"Callouts\", labels = c(glue::glue(\"Data mean: {round(mean_size,2)} km2\"), glue::glue(\"Data median: {median_size} km2\")), values = c(mean = \"red\", median = \"blue\")) +\n  cowplot::theme_half_open() +\n  cowplot::background_grid() +\n  ggplot2::theme(legend.position = c(.75, .95),\n                 # legend.justification = c(\"right\", \"top\"),\n                 # legend.box.just = \"right\",\n                 legend.margin = ggplot2::margin(6, 6, 6, 6)\n  ) \n\nchart\n\n\n\n_98c5dedbb98e7078a3ef610e4d37e6e8.png\n\n\n\n\n\nhttps://stackoverflow.com/questions/10770698/understanding-dates-and-plotting-a-histogram-with-ggplot2-in-r"
  },
  {
    "objectID": "PD_Line.html",
    "href": "PD_Line.html",
    "title": "Line",
    "section": "",
    "text": "https://mermaid.live/edit#pako:eNqtUTFuwzAM_ApBdGgBGbCLpkk8dOrapdkad2Bs2lYgS4YsBTGC_L2SleQF1XA6kqejQF6wNg1jiZWGcM5z3ZN12YEdpYyTTjFUuCPFE3zzibXn6i7P6Cwn2B9JC2j5IGAgK4BGG9ks4Oh1BBVyvhMw8SjA1E6ANicBDde_yWhORhXeGsCz1PD0UiG85XkOWfYBRRFYUh_Iwn4VQgHvC65XETevEbcLL_J0FXmKUmnBzVJZP57fvqCk5v90RYED24FkE6Z7iT0qdD0PYXhloA235JWLk7wGKXlndrOusXTWs0A_NuT4U1JnabgnuZHO2K-0sGVvAkfSP8Y8JNb4rseyJTXx9Q9UmYMW ## Examples ### Callouts"
  },
  {
    "objectID": "PD_Network.html",
    "href": "PD_Network.html",
    "title": "Network",
    "section": "",
    "text": "```{r}\n# https://christophergandrud.github.io/networkD3/ &gt; https://cdn.jsdelivr.net/gh/christophergandrud/networkD3@master/JSONdata/flare.json\nnetworkD3::diagonalNetwork(List = Flare, fontSize = 10, opacity = 0.9)\n```"
  },
  {
    "objectID": "PD_Network.html#examples",
    "href": "PD_Network.html#examples",
    "title": "Network",
    "section": "",
    "text": "```{r}\n# https://christophergandrud.github.io/networkD3/ &gt; https://cdn.jsdelivr.net/gh/christophergandrud/networkD3@master/JSONdata/flare.json\nnetworkD3::diagonalNetwork(List = Flare, fontSize = 10, opacity = 0.9)\n```"
  },
  {
    "objectID": "PD_Pie_chart.html",
    "href": "PD_Pie_chart.html",
    "title": "Pie chart",
    "section": "",
    "text": "https://mermaid.live/edit#pako:eNo1kEFrwzAMhf-K0dmUJqtXx9f1Ohjdbfii1WpqSOzgyGNZyH-flyzvIMTHkx7SDLfoCAwMngR77ki8EY8CXRyYnPicxFfscmCiNNogiixcYjtaEEY86eedvSBvTKsdXXdUKZDQU-rRuxI1_xks8IN6smBK6-iOuWMLNizFipnj-xRuYDhlkpAHh0wXj23CfocDBjAzfIM5SpjA6OrQ1Fqdq9N_XST8xFj8x0OzSala1bo6NxLIeY7pdbt9fcG68WMd2AJSzO0DzB27kZZfpNBaRw ## Examples ### Callouts"
  },
  {
    "objectID": "PD_Taylor_Diagram.html",
    "href": "PD_Taylor_Diagram.html",
    "title": "Taylor Diagram",
    "section": "",
    "text": "Taylor Diagram - [[20241028134259]] Taylor Diagram NSE vs KGE https://bookdown.org/david_carslaw/openair/ https://plotrix.github.io/plotrix/"
  },
  {
    "objectID": "PD_Taylor_Diagram.html#examples",
    "href": "PD_Taylor_Diagram.html#examples",
    "title": "Taylor Diagram",
    "section": "Examples",
    "text": "Examples\n\nCallouts"
  },
  {
    "objectID": "PF.html#how-to",
    "href": "PF.html#how-to",
    "title": "Parcel Fabric",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Parcel Fabric"
    ]
  },
  {
    "objectID": "PIV.html",
    "href": "PIV.html",
    "title": "Particle Image Velocimetry",
    "section": "",
    "text": "Velocimetry is a subset of metrology, and is the measurement of the velocity of the fluid. While this can be a complex process involving tracer measurements and sophisticated setups, camera technology and drones have advanced to the stage where we can track optical particle and, with georeferenced surfaces and measurement points, create a velocity field. This process, occasionally refered to as LSPIV (for large, Surface Particle Imaging Velocimetry) provides a robust, novel, and previously unavailable means of quantifying the state and fluxes of the water cycle.\n\nAside: “Environmental surveillance” is probably one of my next larger thrusts into scaling this process to the next step. While there are still a ton of questions, you can start here for a great singular starting point with an emphasis on the technical realization of that surveillance system.\n\n\n\nBlender is dark magic of the wildest kind. I don’t do it right and even more than some of my other applications, RAM and disk space are critical resources to have if you don’t want to have wait 8 years for results. I don’t know what I’m doing wrong but I’ve created multi-gig files of 15 second imagery, that can’t be right.\n\n\n\n\n\nThe first step is to stabilize the image to static markers. These are most appropriately spaced around the edges of the photo.\n\n\n\n\n\n\n“k” to slice\n\n\n\n\n```{python}\nfrom __future__ import print_function\nimport math\nimport csv\nimport bpy\nD = bpy.data\n\noutput_folder = \"G:/root/database/dro\"\nf2=open('export-markers.log','w')\nfor clip in D.movieclips:\n    print()\n```",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Particle Image Velocimetry"
    ]
  },
  {
    "objectID": "PIV.html#how-to-manipulate-blender",
    "href": "PIV.html#how-to-manipulate-blender",
    "title": "Particle Image Velocimetry",
    "section": "",
    "text": "Blender is dark magic of the wildest kind. I don’t do it right and even more than some of my other applications, RAM and disk space are critical resources to have if you don’t want to have wait 8 years for results. I don’t know what I’m doing wrong but I’ve created multi-gig files of 15 second imagery, that can’t be right.\n\n\n\n\n\nThe first step is to stabilize the image to static markers. These are most appropriately spaced around the edges of the photo.\n\n\n\n\n\n\n“k” to slice",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Particle Image Velocimetry"
    ]
  },
  {
    "objectID": "PIV.html#exporting-to-dataframe",
    "href": "PIV.html#exporting-to-dataframe",
    "title": "Particle Image Velocimetry",
    "section": "",
    "text": "```{python}\nfrom __future__ import print_function\nimport math\nimport csv\nimport bpy\nD = bpy.data\n\noutput_folder = \"G:/root/database/dro\"\nf2=open('export-markers.log','w')\nfor clip in D.movieclips:\n    print()\n```",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Particle Image Velocimetry"
    ]
  },
  {
    "objectID": "PS.html#how-to",
    "href": "PS.html#how-to",
    "title": "Pansharpening",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing",
      "Pansharpening"
    ]
  },
  {
    "objectID": "RA.html#how-to",
    "href": "RA.html#how-to",
    "title": "Raster Analysis",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "RAS2FIM_compliant_model.html#a-compliant-ras2fim-hec-ras-model",
    "href": "RAS2FIM_compliant_model.html#a-compliant-ras2fim-hec-ras-model",
    "title": "So long as you get the GIS&T of it",
    "section": "A compliant RAS2FIM HEC-RAS model",
    "text": "A compliant RAS2FIM HEC-RAS model\n\n\n\nGoals:\n\nTo visually outline what a “compliant” model\n\nOutcomes and Takeaways:\n\nA better understanding of what sort of HEC-RAS model RAS2FIM was developed against\nThe ability to predict (at a gross level) wheather a given HEC-RAS model is or is not expected to run\nThese HEC-RAS models are largely representative of the BLE standard, at least as implemented across FEMA region 6"
  },
  {
    "objectID": "RAS2FIM_compliant_model.html#ras2fim-timeline",
    "href": "RAS2FIM_compliant_model.html#ras2fim-timeline",
    "title": "So long as you get the GIS&T of it",
    "section": "RAS2FIM timeline",
    "text": "RAS2FIM timeline"
  },
  {
    "objectID": "RAS2FIM_compliant_model.html#ras2fim-workflow",
    "href": "RAS2FIM_compliant_model.html#ras2fim-workflow",
    "title": "So long as you get the GIS&T of it",
    "section": "RAS2FIM workflow",
    "text": "RAS2FIM workflow"
  },
  {
    "objectID": "RAS2FIM_compliant_model.html#hec-ras-data-format",
    "href": "RAS2FIM_compliant_model.html#hec-ras-data-format",
    "title": "So long as you get the GIS&T of it",
    "section": "HEC-RAS data format",
    "text": "HEC-RAS data format"
  },
  {
    "objectID": "RAS2FIM_compliant_model.html#example-model-1",
    "href": "RAS2FIM_compliant_model.html#example-model-1",
    "title": "So long as you get the GIS&T of it",
    "section": "Example model 1",
    "text": "Example model 1\n\nThe sample Iowa Flood Center HEC-RAS model\nModels: * 1D, 1 reach - nothing fancy * (2D/Storage) * Active plan points to flows\nStations: Can be a-spatial or have mismatched projections/datums Do not have to start at 0 Do not have to add up to the geometric/geographic length of the cross section"
  },
  {
    "objectID": "RAS2FIM_compliant_model.html#example-model-2",
    "href": "RAS2FIM_compliant_model.html#example-model-2",
    "title": "So long as you get the GIS&T of it",
    "section": "Example model 2",
    "text": "Example model 2"
  },
  {
    "objectID": "RAS2FIM_compliant_model.html#model-sources",
    "href": "RAS2FIM_compliant_model.html#model-sources",
    "title": "So long as you get the GIS&T of it",
    "section": "Model sources",
    "text": "Model sources\n\n\n\nA screenshot of the FEMA region 6 BLE dashboard\n\n\n\nThinking about how you would scale this out? There are a few different technical hurdles to look out for, see the need for a more FAIR HEC-RAS format here."
  },
  {
    "objectID": "RD.html#how-to",
    "href": "RD.html#how-to",
    "title": "Radar",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Remote Sensing",
      "Radar"
    ]
  },
  {
    "objectID": "RL.html#how-to",
    "href": "RL.html#how-to",
    "title": "Relate",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Relate"
    ]
  },
  {
    "objectID": "RRASSLER.html",
    "href": "RRASSLER.html",
    "title": "RRASSLER",
    "section": "",
    "text": "_rassler_key_image.png",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "RRASSLER"
    ]
  },
  {
    "objectID": "RRASSLER.html#timing",
    "href": "RRASSLER.html#timing",
    "title": "RRASSLER",
    "section": "Timing",
    "text": "Timing\nThe following timings were tested only once and are provided to help gage how much investment needs to go into processing. These results of course are a function of the mode RRASSLER is run in, hardware, and particular set of models you are ingesting and is by no means an exhaustive or comprehensive set of tables. Run as you would find demonstrated in Deploying RRASSLER\n\nIngest timing\n\n\nShow an example of how this was run\ningest_into_database(path_to_ras_dbase = ras_dbase,\n                     top_of_dir_to_scrape = \"./inst/extdata/sample_ras/timing_samples/5_models\",\n                     code_to_place_in_source = \"test: FEMA6\",\n                     proj_override = \"EPSG:2277\",\n                     apply_vdat_trans = FALSE,is_quiet = FALSE,is_verbose = TRUE,overwrite = FALSE,parallel_proc = FALSE)\n\n\n\n\n\n\n\n\nPC hardware\n\n\nNumber of models\n\n\nRuntime (hours)\n\n\n\n\n\n\n1\n\n\n5\n\n\n0.010\n\n\n\n\n1\n\n\n50\n\n\n0.087\n\n\n\n\n1\n\n\n500\n\n\n2.494\n\n\n\n\n1\n\n\n12491\n\n\n82.431\n\n\n\n\n2\n\n\n12096\n\n\n546.200\n\n\n\n\n\n\n\n\nFootprint rollup timing\n\n\n\n\n\n\nPC hardware\n\n\nNumber of models\n\n\nRuntime (hours)\n\n\n\n\n\n\n1\n\n\n5\n\n\n0.0001\n\n\n\n\n1\n\n\n50\n\n\n0.0010\n\n\n\n\n1\n\n\n500\n\n\n2.4940\n\n\n\n\n1\n\n\n12491\n\n\n2.9500\n\n\n\n\n1\n\n\n13700\n\n\n3.6370\n\n\n\n\n2\n\n\n24587\n\n\n32.4280\n\n\n\n\n\n\n\nNotes: PC ID 1 was RRASSLER, run on a docker’d image on a Intel Core i5-9600K CPU @ 3.7 GHz with 32 gigs of RAM available and moderate background use. PC 2 is a GFE, running RRASSLER on RStudio on a 13th Gen Intel Core i7-1355U CPU @ 1.7 GHz with 32 gigs of RAM available and light background use.\n\n\n\n\n\n\n\n\n\n\n\n\nPlacing model names back into cross sections\nPlacing the",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "RRASSLER"
    ]
  },
  {
    "objectID": "RRASSLER.html#explanations",
    "href": "RRASSLER.html#explanations",
    "title": "RRASSLER",
    "section": "Explanations",
    "text": "Explanations\n\nStatement of need\nThere are few hydraulic models as prolific as HEC-RAS, and since it’s first named release in 1995 users have created these models using public and private data and countless hours of engineering scrutinization in order to generate the best possible purpose-built representation of the world. Like any model, some level of input massaging is necessary in order to get the data into the specified mathematical format a model requires. Like most domain specific software solutions, that massaging was rather, forceful, to the point of permanently altering the shape of those inputs into something that most geospatial data readers are unable to handle. This creates a great deal of friction both in terms of model accountability and interoperability, particularly when you take the standpoint as a model consumer. The R based HEC-RAS Wrestler (RRASSLER) is here to mediate that. By internally versioning and aligning data and pointers, the resulting structure provides a bottoms up approach amenable to walking continental scale applications back to the specific point, cross section, and HEC-RAS model they were sourced from. See this in visual form here.\n\nWhy not just upload models to git?\nAside from the poor use of the tool, there are a few reasons not to do this. First, without a flatter structure you are still left to guess at where in the stack of folders your relevant model is. Naming collision will be more frequent since the models space is not forced into the schema, and although version controlling the data would be more robust (one of the principle features of GIT), none of the added benefits of the F.A.I.R.-ification that RRASSLER accomplishes are realized.\n\n\nIs this just a HEC-RAS data reader?\nAlthough this utility does implement a reader, it accomplishes far more than simply reading in data. While implementing the HEC-RAS format as a driver for inclusion in the most common reader (gdal) could in theory be possible, that solves only a few of the limitations encountered in using these models and does not fully address many of the concerns that we need to overcome. This is also a slightly more flexible reader in that larger context of related tools. Many parse only .HDF files, whereas RRASSLER parses and uses both in the spatialization of the model.\n\n\n\nThe HEC-RAS Model Format\nSee my HEC-RAS data page for a deeper dive into the nuances of the HEC-RAS data format. However, for the purposes of RRASSLER’s downstream use cases, we make a distinction in the “orientation” of how to track model data. The standard HEC-RAS model has a single project file that might point to many geometry files. Were our only goal to facilitate HEC-centric utilization of this data, the project would be an appropriate file level to track. However, we are also interested in making the pieces of the model data more reusable and so it becomes important to capture the distinctions in modeling surfaces that those models are intended to represent. These geometry files can point to the same space but have different cross section definitions (for instance, a pre- and post-dredge model), or they could point to an entirely different portion of the river. This is critical distinction that would be lost in a single layer.\n\n\nWhy river station was not included in the inital scoping:\nHEC-RAS rivers are not typically aligned with the hydrofabric on\nAn alternative approach: &gt; Note: since this was not a demention constrained in the inital RRASSLER data spesification might use something like the following to place that back into our data. As an exten should use:",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "RRASSLER"
    ]
  },
  {
    "objectID": "RRASSLER.html#discussion",
    "href": "RRASSLER.html#discussion",
    "title": "RRASSLER",
    "section": "Discussion",
    "text": "Discussion\n\nI am both an archivist/model creator and a RRASSLER user\nYou will unfortunately have to keep two copies of the data. RRASSLER isn’t creating anything you don’t already have in the archive in one form or another, and completely removes all metadata and formatting that your archive has so painstakingly created. Don’t change your workflow, consider RRASSLER a “post-processing” step to your archiving work, whose primary purpose is to make a selection of your models more amenable to operational deployments.\n\n\nUnit testing\nThe RRASSLER tutorials are a great end-to-end unit test of the sample data in the repo, but a more verbose walk through of the processing steps and touching points can be found here.\n\n\nLimitations\nAligning the different model surfaces is hard. Although every effort was made to account for standard edge cases and unit cohesion, you will, more often than not, find that a surface you use and a model do not align. That is not particularly surprising, but it is often disconcerting. 3DEP timestamps, resolutions, and even order of reprojection operations may alter the surfaces slightly, even if they are stated to have come from the same input database. Do your own sanity checks and try not to lose your mind, it’s probably easier to go out and measure it again. Finally, this was developed, tested, and deployed over primarily 1D data. Although 2D model may appear to ingest correctly, there was no consideration for those in either parsing or processing, and is not accounting or copying .tif files so the value of these models within the RRASSLER workflow is greatly diminished.\n\n\nGetting involved\nIf you have questions, concerns, bug reports, etc, please file an issue in this repository’s Issue Tracker. I know we are not the only ones attempting to align the world. General instructions on how to contribute can be found at CONTRIBUTING. More specifically, the following are known shortcomings and next steps.\n\n\nA few next steps\n\nHardening and extension\nI code out of necessity, not out of love, and I’ve been told my more than a few that I write awful code. More than half of the community will also think this is written in the wrong language. Efforts to harden the workflow and algorithm, extend this workflow into your language of choice, and general improvements would all be uses of time. For example, how multi-river models are extracted are still unclear.\n\n\nRRASSTAC\nAlthough not the most obvious use of STAC, or SpatioTemporal Asset Catalog, HEC-RAS models (most easily the footprints generated in the cataloging of the models) has been extended to the STAC framework as a STAC Item, and by that analogy each version of the catalog is a STAC collection. Formalizing this, most likely through an additional function such as STAC_catalog.R, which would generate the needed json’s and then appropriately serve those, would be a worthy task to undertake. The obvious extension of that, exposing the model cross sections with XYZ LINESTRING geometry as STAC items, is in theory what RRASSLER is accomplishing, but the framework and analogous STAC deployment is less clear and could also be explored. Some of the early attempts at this can be found in (Lawler et al. 2025)\n\n\n\nRRASSLER\nSTAC\n\n\n\n\nRAS Catalog\nCollection\n\n\nRecord\nItem\n\n\nModel Pieces\nAssets\n\n\n\n\n\n2D RRASSLER\nMany of the same considerations, concerns, and hurdles experienced working with and manipulating 1D RAS data will be encountered as we move into 2D model accounting. While many of the technical advancements and standards make the accounting of 2D models a bit more theoretically straightforward; since most valuable and standardized 2D models come with an associated .tif file which is footprintable, and new model formats are provided as .HDF, which has cloud compatible characteristics and is being developed, the actual work of constructing the utility which would account for those models needs to be done. There are some preliminary discussions along that particular thrust here.\n\n\n\nDependencies\nBeyond the package dependencies listed in the description file here, this is an R package and so a more recent version of R and RStudio is needed and recommended respectively in order to capitalize on the capabilities of this repository. This was written, tested, and packaged using using a modified rocker-versioned2 environment. Typically deployed via package installation on an RStudio instance and alongside a RAS2FIM conda environment in Windows.",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "RRASSLER"
    ]
  },
  {
    "objectID": "RRASSLER.html#open-source-licensing-info",
    "href": "RRASSLER.html#open-source-licensing-info",
    "title": "RRASSLER",
    "section": "Open source licensing info",
    "text": "Open source licensing info\n\nTERMS\nLICENSE",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "RRASSLER"
    ]
  },
  {
    "objectID": "RRASSLER.html#credits-and-references",
    "href": "RRASSLER.html#credits-and-references",
    "title": "RRASSLER",
    "section": "Credits and references",
    "text": "Credits and references\nCredit to the packages used in the development, testing, and deployment of RRASSLER including but not exclusive of the packages currently listed in the DESCRIPTON file. We are appreciative of the FEMA region 6 group and the BLE data they make publicly available. Built copying patterns from RAS2FIM.\n\nFor questions\nJim Coll (FIM Developer), Fernando Salas (Director, OWP Geospatial Intelligence Division)",
    "crumbs": [
      "Atlas",
      "Applications",
      "FIM",
      "RRASSLER"
    ]
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#accounting-a-model-from-hec-ras-to-hydrofabric",
    "href": "RRASSLER_process_unit_tests.html#accounting-a-model-from-hec-ras-to-hydrofabric",
    "title": "So long as you get the GIS&T of it",
    "section": "Accounting a Model from HEC-RAS to Hydrofabric",
    "text": "Accounting a Model from HEC-RAS to Hydrofabric\n\n\n\n\n\n\nGoals:\n\nTo walk through the HEC-RAS data from raw model to RRASSLER artifact\nHow that data model compares to the Hydrofabric data model\n\nOutcomes and Takeaways:\n\nA data driven accounting of a significant portion of the RRASSLER to Hydrofabric3D Workflow\nOriented spatially to data representation\nA “unit test” for RRASSLER artifact ingestion\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\nSlide layout:  default     Items linked/bordered in green are cited in the tooltip on hover.\n: narrative-oriented     Items linked/bordered in blue are hyperlinked to relevant resources.\n\n\nAlternative Title: A baptism of HEC-RAS data formats  I’m not sure which panel of this triptych is the best rendition of my current situation, but this image was too evocative of my general feelings on this topic to not include. To quote one of my giants: “These hands have been burnt by HEC-RAS project layouts before” - Seth Lawler\nWe make a lot of implicit assumptions about the geometric representation of the landscape, lets look at some of them as we verify the behavior of the RRASSLER code base.\nBackground image from: OWP template"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#orienting-to-one-dimension",
    "href": "RRASSLER_process_unit_tests.html#orienting-to-one-dimension",
    "title": "So long as you get the GIS&T of it",
    "section": "Orienting to One Dimension",
    "text": "Orienting to One Dimension\n\nHEC-RAS conventions\nCross sections should be laid out perpendicular to where water will flow in the channel and overbank areas. Therefore, most cross section lines should be created from a minimum of four points (the end points and points at the edge of the main channel). Cross sections will also be visualized when looking in the downstream direction; therefore, they should be created from left to right when looking downstream (RAS Mapper will automatically flip the line to have the correct orientation).\n\n\n\n\nLeft to right, looking downstream. Start high and walks down"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#orienting-to-one-dimension-1",
    "href": "RRASSLER_process_unit_tests.html#orienting-to-one-dimension-1",
    "title": "So long as you get the GIS&T of it",
    "section": "Orienting to One Dimension",
    "text": "Orienting to One Dimension\n\n\n\n\n\n\n\n A blend of HEC-RAS and Hydrofabric conventions\n\n\n\n\n\n\n\n\n\n\nSpace is arbitrary!"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#orienting-to-one-dimension-2",
    "href": "RRASSLER_process_unit_tests.html#orienting-to-one-dimension-2",
    "title": "So long as you get the GIS&T of it",
    "section": "Orienting to One Dimension",
    "text": "Orienting to One Dimension\nAside: routelink’s definition\n\n\n\n\nThe routelink dimensional diagram"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#orienting-to-one-dimension-3",
    "href": "RRASSLER_process_unit_tests.html#orienting-to-one-dimension-3",
    "title": "So long as you get the GIS&T of it",
    "section": "Orienting to One Dimension",
    "text": "Orienting to One Dimension\n\n\n\n\n\n\n\n\n\n\n\n\n\nplanform length =/= cross section spacing\n\n\n\nNot space"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#section",
    "href": "RRASSLER_process_unit_tests.html#section",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "NOAA-OWP.GITHUB.IO/RRASSLER\n\nRRASSLER addresses this! Let’s walk through it."
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#follow-along",
    "href": "RRASSLER_process_unit_tests.html#follow-along",
    "title": "So long as you get the GIS&T of it",
    "section": "Follow along!",
    "text": "Follow along!\n\n\nNOAA-OWP.GITHUB.IO/RRASSLER\n\n\n\n\n1) Make sure RRASSLER is installed\n\n# remotes::install_github(\"NOAA-OWP/RRASSLER\")  # If it asks for package updates: press 1\n# library(data.table)\nlibrary(RRASSLER)\nRRASSLER::marco()                           # A \"hello world\" test\n\n[1] TRUE\n\n\n  \n2) Download and examine the sample data\n\nfs::dir_tree(path = file.path(fs::path_package(\"\", package = \"RRASSLER\"),\"extdata\",\"sample_ras\",\"FEMA-R6-BLE-sample-dataset\",fsep = .Platform$file.sep), recurse = TRUE)\n\n/usr/local/lib/R/site-library/RRASSLER/extdata/sample_ras/FEMA-R6-BLE-sample-dataset\n└── 12090301\n    └── 12090301_models\n        └── Model\n            └── Alum Creek-Colorado River\n                ├── ALUM 006\n                │   ├── ALUM 006.O01\n                │   ├── ALUM 006.f01\n                │   ├── ALUM 006.g01\n                │   ├── ALUM 006.g01.hdf\n                │   ├── ALUM 006.p01\n                │   ├── ALUM 006.p01.comp_msgs.txt\n                │   ├── ALUM 006.p01.computeMsgs.txt\n                │   ├── ALUM 006.p01.hdf\n                │   ├── ALUM 006.prj\n                │   ├── ALUM 006.r01\n                │   ├── ALUM 006.xml\n                │   ├── Backup.g01\n                │   ├── Multiple\n                │   │   └── PostProcessing.hdf\n                │   └── _XsOutData\n                │       └── XsOutData.txt\n                ├── ALUM 107\n                │   ├── ALUM 107.O01\n                │   ├── ALUM 107.f01\n                │   ├── ALUM 107.g01\n                │   ├── ALUM 107.g01.hdf\n                │   ├── ALUM 107.p01\n                │   ├── ALUM 107.p01.comp_msgs.txt\n                │   ├── ALUM 107.p01.computeMsgs.txt\n                │   ├── ALUM 107.p01.hdf\n                │   ├── ALUM 107.prj\n                │   ├── ALUM 107.r01\n                │   ├── ALUM 107.xml\n                │   └── _XsOutData\n                │       └── XsOutData.txt\n                └── ALUM 114\n                    ├── ALUM 114.O01\n                    ├── ALUM 114.f01\n                    ├── ALUM 114.g01\n                    ├── ALUM 114.g01.hdf\n                    ├── ALUM 114.p01\n                    ├── ALUM 114.p01.comp_msgs.txt\n                    ├── ALUM 114.p01.computeMsgs.txt\n                    ├── ALUM 114.p01.hdf\n                    ├── ALUM 114.prj\n                    ├── ALUM 114.r01\n                    ├── ALUM 114.rasmap\n                    ├── ALUM 114.rasmap.backup\n                    ├── ALUM 114.xml\n                    └── _XsOutData\n                        └── XsOutData.txt\n\n\n\n\n\n\n\n\n    \n\nRepo sample data\n\n\n\n\nGFE install helper The sample data can also be grabbed here."
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#refresher-the-hec-ras-model-format",
    "href": "RRASSLER_process_unit_tests.html#refresher-the-hec-ras-model-format",
    "title": "So long as you get the GIS&T of it",
    "section": "Refresher: The HEC-RAS model format",
    "text": "Refresher: The HEC-RAS model format\n\n\n\n\n\n\nThe Theoretical Standard\n\n\n\nFiles to Track\n\n\n\n\n\n\n\n\nFile grep Pattern (# denotes single numeric wildcard)\nHEC-RAS Model Use\n\n\n\n\n.g##\nGeometry definitions\n\n\n.prj\nProjection (can be non-standard proj4 string defined file)\n\n\n.prj\nProject (same extension, defines how RAS models are wired)\n\n\n.p##\nPlan file, used to drive the model\n\n\n.f##\nSteady Flow file. Profile information, flow data and boundary conditions\n\n\n.h##\nHydraulic Design data file\n\n\n.v##\nVelocity file\n\n\n.o##\nOutput file\n\n\n.r##\nRun file for steady flow\n\n\n.u##\nunsteady Flow file. Profile information, flow data and boundary conditions\n\n\n.x##\nRun file for unsteady flow\n\n\n.dss\nData files\n\n\n.rasmap\nOutput plan\n\n\n\n\n\n\n\n\n\nThese files, followed by a .hdf, are transformations of their counterparts needed for newer versions of HEC-RAS and are automatically created as needed.\n\n\n\n\n\n\nSidecar files are fun."
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#refresher-the-hec-ras-model-format-1",
    "href": "RRASSLER_process_unit_tests.html#refresher-the-hec-ras-model-format-1",
    "title": "So long as you get the GIS&T of it",
    "section": "Refresher: The HEC-RAS model format",
    "text": "Refresher: The HEC-RAS model format\n\n\n\n\n\n\nModel Standards\n\n\n\n\n\n\n\nFiles to Track\n\n\n\n\n\n\n\n\nFile grep Pattern (# denotes single numeric wildcard)\nHEC-RAS Model Use\n\n\n\n\n.g##\nGeometry definitions\n\n\n.prj\nProjection (can be non-standard proj4 string defined file)\n\n\n.prj\nProject (same extension, defines how RAS models are wired)\n\n\n.p##\nPlan file, used to drive the model\n\n\n.f##\nSteady Flow file. Profile information, flow data and boundary conditions\n\n\n.h##\nHydraulic Design data file\n\n\n.v##\nVelocity file\n\n\n.o##\nOutput file\n\n\n.r##\nRun file for steady flow\n\n\n.u##\nunsteady Flow file. Profile information, flow data and boundary conditions\n\n\n.x##\nRun file for unsteady flow\n\n\n.dss\nData files\n\n\n.rasmap\nOutput plan\n\n\n\n\n\n\n\n\n\nThese files, followed by a .hdf, are transformations of their counterparts needed for newer versions of HEC-RAS and are automatically created as needed.\n\n\n\n\n\n\nSidecar files are fun."
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#refresher-the-hec-ras-model-format-2",
    "href": "RRASSLER_process_unit_tests.html#refresher-the-hec-ras-model-format-2",
    "title": "So long as you get the GIS&T of it",
    "section": "Refresher: The HEC-RAS model format",
    "text": "Refresher: The HEC-RAS model format\n\n\n\n\n\n\nA “real-world” example\n\n\nFiles to Track\n\n\n\n\n\n\n\n\nFile grep Pattern (# denotes single numeric wildcard)\nHEC-RAS Model Use\n\n\n\n\n.g##\nGeometry definitions\n\n\n.prj\nProjection (can be non-standard proj4 string defined file)\n\n\n.prj\nProject (same extension, defines how RAS models are wired)\n\n\n.p##\nPlan file, used to drive the model\n\n\n.f##\nSteady Flow file. Profile information, flow data and boundary conditions\n\n\n.h##\nHydraulic Design data file\n\n\n.v##\nVelocity file\n\n\n.o##\nOutput file\n\n\n.r##\nRun file for steady flow\n\n\n.u##\nunsteady Flow file. Profile information, flow data and boundary conditions\n\n\n.x##\nRun file for unsteady flow\n\n\n.dss\nData files\n\n\n.rasmap\nOutput plan\n\n\n\n\n\n\n\n\n\nThese files, followed by a .hdf, are transformations of their counterparts needed for newer versions of HEC-RAS and are automatically created as needed.\n\n\n\n\n\n\n\n\n\n\n\nData from the wild can be a bit unruly."
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#what-rrassler-is-running",
    "href": "RRASSLER_process_unit_tests.html#what-rrassler-is-running",
    "title": "So long as you get the GIS&T of it",
    "section": "What RRASSLER is running",
    "text": "What RRASSLER is running\n\n\nAside: *_ingest_record as a “Core function?”"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#aside-implications-of-geometric-orientation",
    "href": "RRASSLER_process_unit_tests.html#aside-implications-of-geometric-orientation",
    "title": "So long as you get the GIS&T of it",
    "section": "Aside: Implications of “geometric orientation”",
    "text": "Aside: Implications of “geometric orientation”\n\n\nAside: “geometric orientation?”"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#section-1",
    "href": "RRASSLER_process_unit_tests.html#section-1",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "Data “checkpoints”\n\nModel testing: ALUM 114.prj\n\n\n\nMajor touchpoints along the RRASSLER workflow"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#section-2",
    "href": "RRASSLER_process_unit_tests.html#section-2",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "Data “checkpoints”\n\n\nRRASSLER reading\n\nModel testing: ALUM 114.prj\n\n\n\n.g## file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData!"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#section-3",
    "href": "RRASSLER_process_unit_tests.html#section-3",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "Data “checkpoints”\n\n\nRRASSLER reading\n\nModel testing: ALUM 114.prj\n\n\n\n.g##.hdf file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData!"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#section-4",
    "href": "RRASSLER_process_unit_tests.html#section-4",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "Data “checkpoints”\n\n\nRRASSLER copy files\n\nFiles extract consistently\n\n\n\n\n.g## file\n\n\n\n\n\n\n\n\n\n\n\n.g##.hdf file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorks (this time…)\nSome points to note:\n\nNon-trivial: Of the 57 models OWP received, only 7 had HDF files.\nI am not competent but I am unable to run MCAT-RAS, see RRASSLER need.\n\nNow that we’ve demonstrated that there’s parity between the different ways RRASSLER creates geospatial artifacts, let’s follow those transformations through to the hydrofabric3D repo"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#section-5",
    "href": "RRASSLER_process_unit_tests.html#section-5",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "Data “checkpoints”\n\n\nCopy data\n\nModel testing: ALUM 114.prj\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreat, I can copy files. That was in question…"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#section-6",
    "href": "RRASSLER_process_unit_tests.html#section-6",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "Data “checkpoints”\n\n\nRRASSLER Saving\n\nModel testing: ALUM 114.prj\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreat, I can save files. That was also in question…"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#section-7",
    "href": "RRASSLER_process_unit_tests.html#section-7",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "Data “checkpoints”\n\n\nHydrofabric Lines\n\nRAS vs Automated Transects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHydrofab needs to flip, but match very well with previous toolings"
  },
  {
    "objectID": "RRASSLER_process_unit_tests.html#whats-next",
    "href": "RRASSLER_process_unit_tests.html#whats-next",
    "title": "So long as you get the GIS&T of it",
    "section": "What’s next?",
    "text": "What’s next?\n\n\n\nOutcomes and Takeaways:\n\nA data driven accounting of a significant portion of the RRASSLER to Hydrofabric3D Workflow.\nOriented spatially to data representation.\nA “unit test” for RRASSLER artifact ingestion.\n\nNext Steps:\n\nPublish RRASSLER.\nExplore better paramiterization pathways for applications like T-Route.\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\nSee more at the Official RRASSLER documentation."
  },
  {
    "objectID": "RT.html#how-to",
    "href": "RT.html#how-to",
    "title": "Route",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Analysis",
      "Network",
      "Route"
    ]
  },
  {
    "objectID": "SC.html#how-to",
    "href": "SC.html#how-to",
    "title": "Space-time Cubes",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Space-time Cubes"
    ]
  },
  {
    "objectID": "SFM.html",
    "href": "SFM.html",
    "title": "Structure from Motion",
    "section": "",
    "text": "Fly the drone\nTips for mapping: * https://help.dronedeploy.com/hc/en-us/articles/1500004964282-Making-Successful-Maps",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Structure from Motion"
    ]
  },
  {
    "objectID": "SFM.html#typical-data-flow",
    "href": "SFM.html#typical-data-flow",
    "title": "Structure from Motion",
    "section": "typical data flow",
    "text": "typical data flow\nOnce finished, download all assets\ntransform tiff to COG and push to S3 bucket\nTransform entwine point cloud to potree",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Structure from Motion"
    ]
  },
  {
    "objectID": "SFM.html#scaling-odm",
    "href": "SFM.html#scaling-odm",
    "title": "Structure from Motion",
    "section": "Scaling ODM",
    "text": "Scaling ODM\nODM is a docker image, so in theory it could be put on AWS, push the images to a bucket, spin up a lot of instance for the processing and scale back when done. I would like to know how to execute this but at the moment I can get all my processing done here so there’s no reason to invest more time in it now.",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Structure from Motion"
    ]
  },
  {
    "objectID": "SFM.html#sfm-in-adverse-conditions",
    "href": "SFM.html#sfm-in-adverse-conditions",
    "title": "Structure from Motion",
    "section": "SFM in adverse conditions",
    "text": "SFM in adverse conditions\n\nMeshlab",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Structure from Motion"
    ]
  },
  {
    "objectID": "SFM.html#collection",
    "href": "SFM.html#collection",
    "title": "Structure from Motion",
    "section": "Collection",
    "text": "Collection",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Structure from Motion"
    ]
  },
  {
    "objectID": "SFM.html#processessing",
    "href": "SFM.html#processessing",
    "title": "Structure from Motion",
    "section": "Processessing",
    "text": "Processessing\nThese come out as abc or ply, neither of which are currently supported formats for Potree. Therefore, we’ll need to convert them which leads us to:",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Structure from Motion"
    ]
  },
  {
    "objectID": "SFM.html#potree-converter",
    "href": "SFM.html#potree-converter",
    "title": "Structure from Motion",
    "section": "Potree converter",
    "text": "Potree converter\nhttps://github.com/potree/PotreeConverter https://github.com/potree/PotreeConverter/releases\n\nInstall\n\n\nRun\n```{Powershell}\n.\\PotreeConverter.exe G:\\dronedata\\KS_LearnedFlume\\sedflume\\20210310\\meshroom\\MeshroomCache\\ConvertSfMFormat\\c2ffbee2f8065999d1a8e24503cb345094f673f4\\sfm.ply -o G:\\dronedata\\KS_LearnedFlume\\sedflume\\20210310\\meshroom\\PotreeConvert\n```",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Structure from Motion"
    ]
  },
  {
    "objectID": "SFM.html#others-in-the-space",
    "href": "SFM.html#others-in-the-space",
    "title": "Structure from Motion",
    "section": "Others in the space",
    "text": "Others in the space\nhttps://www.unr.edu/uas-federation",
    "crumbs": [
      "Atlas",
      "Applications",
      "Metrology",
      "Structure from Motion"
    ]
  },
  {
    "objectID": "SJ.html#how-to",
    "href": "SJ.html#how-to",
    "title": "Spatial Join",
    "section": "How to ",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Math",
      "Spatial Join"
    ]
  },
  {
    "objectID": "SP.html#how-to",
    "href": "SP.html#how-to",
    "title": "Sampling",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "SP.html#graveyard",
    "href": "SP.html#graveyard",
    "title": "Sampling",
    "section": "Graveyard",
    "text": "Graveyard\nTraditional techniques involve splitting the testing sets up More robust techniques (jacknifing, bootstrapping), involve random sampling Might as well just use it all: (Shen, Tolson, and Mai 2022), (Kratzert et al. 2019)"
  },
  {
    "objectID": "ST.html#how-to",
    "href": "ST.html#how-to",
    "title": "Statistics",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Statistics"
    ]
  },
  {
    "objectID": "ST.html#past-format",
    "href": "ST.html#past-format",
    "title": "Statistics",
    "section": "Past format",
    "text": "Past format\n\nIt’s AI in the investor’s news letter, it’s ML in the grant application, it’s non-parametic in the classroom, and it’s linear fit in practice.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Statistics"
    ]
  },
  {
    "objectID": "ST.html#non-parametric-statistics",
    "href": "ST.html#non-parametric-statistics",
    "title": "Statistics",
    "section": "Non-parametric statistics",
    "text": "Non-parametric statistics\nThis sentiment can be interpreted in a number of ways, but I really like to latch onto the fact that despite its simplicity, extrapolating in the short term works fairly well. While there’s nothing wrong with the practice, we can be a little better in that it’s not much harder to perform a non-parametric test on your data and get a much more statistically defensible and meaningfully interpretable result\n\nA post about Mann Kendall and Sens slope in GEE. At some point I’ll get around to doing something with this too.\n\n\nSens Slope\n\n\nMann Kendall",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Statistics"
    ]
  },
  {
    "objectID": "ST.html#curve-fitting",
    "href": "ST.html#curve-fitting",
    "title": "Statistics",
    "section": "Curve fitting",
    "text": "Curve fitting\n(“The Broken Bridge Between Biologists and Statisticians  A Collection of Self-Starters for Nonlinear Regression in R” n.d.) (“The Broken Bridge Between Biologists and Statisticians  Self-starting Routines for Nonlinear Regression Models” n.d.)\nThe linear least-squares problem occurs in statistical regression analysis; it has a closed-form solution. The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.\nOccasionally we have to estimate a function which estimates a set of observations. Because observations are messy and we are modeling an otherwise interconnected system, the form that equation takes is not often particularly clear. Here’s a little template I use that sets up the calculation, reporting, and plotting of a range of curves. YMMV, and never show this to a mathematician.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Statistics"
    ]
  },
  {
    "objectID": "ST.html#quick-fit-check",
    "href": "ST.html#quick-fit-check",
    "title": "Statistics",
    "section": "Quick fit check",
    "text": "Quick fit check\n\nlinear_fit &lt;- stats::lm(Y ~ X, data = data)  ## $y = mX + b$\n#braggs3_nls_fit &lt;- stats::nls(Y ~ aomisc::NLS.bragg.3(X, b, d, e), data = data)\n\ntryCatch( { \n  result &lt;- log(\"not a number\"); print(res) \n  }, \n  error = function(e) {\n    ggplot2::ggplot() + \n  ggplot2::geom_point(data = data, ggplot2::aes(x = X, y = Y), color = \"red\") + \n  ggplot2::geom_text(\n    data = data.frame(x = mean(data$X),y = mean(data$Y), label = \"DID NOT CONVERGE\"), \n    ggplot2::aes(x, y, label = label),\n    hjust = 0.5, vjust = 0.5, angle = 45, size = 11, color = \"red\",inherit.aes = FALSE) +\n  cowplot::theme_half_open() +\n  cowplot::background_grid()\n    }\n  )",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Statistics"
    ]
  },
  {
    "objectID": "ST.html#quick-fit-check-1",
    "href": "ST.html#quick-fit-check-1",
    "title": "Statistics",
    "section": "Quick fit check",
    "text": "Quick fit check\n\n\n[1] \"test\"\n\n\nAnd for some slightly longer and more decorative explainers:",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Statistics"
    ]
  },
  {
    "objectID": "ST.html#curves",
    "href": "ST.html#curves",
    "title": "Statistics",
    "section": "Curves",
    "text": "Curves",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Statistics"
    ]
  },
  {
    "objectID": "ST.html#bell-curves",
    "href": "ST.html#bell-curves",
    "title": "Statistics",
    "section": "Bell curves",
    "text": "Bell curves\nThis function is connected to the normal (Gaussian) distribution and has a symmetric shape with a maximum equal to \\(d\\), that is reached when \\(X=e\\) and two inflection points. In this model, \\(b\\) relates to the slope at the inflection points; the response \\(Y\\) approaches 0 when X approaches \\(±∞\\): \\(y = d * exp[-b*(X - e)^2]\\), or with lower asymptotes \\(c =/= 0\\) as \\(y = c + (d - c) * exp[-b*(X - e)^2]\\)\n\nOther useful tools:\n\nhttps://www.graphreader.com/\nhttps://mycurvefit.com/",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Data Management",
      "Statistics"
    ]
  },
  {
    "objectID": "TA.html#how-to",
    "href": "TA.html#how-to",
    "title": "Terrain Analysis",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Terrain Analysis"
    ]
  },
  {
    "objectID": "TEAM.html",
    "href": "TEAM.html",
    "title": "TEAM",
    "section": "",
    "text": "Geographic context and seamless access to key environmental water cycling metrics is a fairly well defined operation with a massive amount of technical and digital friction associated with it. Recently, Google Earth Engine has added the ability to create UI features to the platform, creation seamless experience between data, processing, and presentation a “one stop shop, from observation to presentation”. This is an exploration of the methods and structure of that feature to more readily address what I feel are embarrassingly easy questions to pose, but an embarrassingly underwhelming capacity to quickly respond to. To do this, I’ve implemented a GUI driven, zonal statistics equivalent over a gap filled and value added evapotranspiration dataset using the MOD16 and MOD17 products. Go use the application.\nVideo\n\n\nA GEE booth presentation, CUAHSI presentation, and related Hydroshare artifacts."
  },
  {
    "objectID": "TEAM.html#references",
    "href": "TEAM.html#references",
    "title": "TEAM",
    "section": "",
    "text": "A GEE booth presentation, CUAHSI presentation, and related Hydroshare artifacts."
  },
  {
    "objectID": "TR.html#how-to",
    "href": "TR.html#how-to",
    "title": "Real-time Tracking",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Vector Analysis",
      "Network",
      "Real-time Tracking"
    ]
  },
  {
    "objectID": "VO.html#how-to",
    "href": "VO.html#how-to",
    "title": "Volume",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Analysis",
      "Volume"
    ]
  },
  {
    "objectID": "VS.html#how-to",
    "href": "VS.html#how-to",
    "title": "Viewshed",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "XSdm_intercomp_crosswalk.html",
    "href": "XSdm_intercomp_crosswalk.html",
    "title": "Cross Section Data Model Comparison and Interoperability Crosswalk",
    "section": "",
    "text": "Although they proport to represent the same concepts, many of the different forms of cross sections and cross section data you might encounter are formatted in application-specific ways. This makes describing the system difficult not only because we typically lack an application to point to when talking about the generic “cross section” concept, but also because each user comes to the table with there own preconceptions and assumptions about how that data ought to be formatted and dimensionally constrained. This is my attempt to harmonize the different forms into the hydrofabric standard.\nAs alluded to, HEC-RAS data is not the hydrofabric transect data model but can be made compliant with a little extra explainability. Here I’ll explore the most common forms we come across HEC-RAS transect data, the HEC-RAS models themselves, the RRASSLER catalog data, the MIP database, and the Hydrofabric3D format. For convenience, here are those formats in a single place:"
  },
  {
    "objectID": "XSdm_intercomp_crosswalk.html#independent-data-models",
    "href": "XSdm_intercomp_crosswalk.html#independent-data-models",
    "title": "Cross Section Data Model Comparison and Interoperability Crosswalk",
    "section": "Independent data models",
    "text": "Independent data models\n\nHEC-RASHydrofabric3DRRASSLERMCAT-RAS/MIP\n\n\nsee the HEC-RAS page for more information. The HEC-RAS data standard is a fun mesh of sidecar files that all need special parsing to transform to a more useful spatial format.\n\nData Model Schematic Constrained DimensionsData Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Model SchematicData Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Model SchematicData Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Model SchematicData Model"
  },
  {
    "objectID": "XSdm_intercomp_crosswalk.html#as-hec-ras-artifacts",
    "href": "XSdm_intercomp_crosswalk.html#as-hec-ras-artifacts",
    "title": "Cross Section Data Model Comparison and Interoperability Crosswalk",
    "section": "As HEC-RAS artifacts",
    "text": "As HEC-RAS artifacts\n\nHEC-RAS to RRASSLERHEC-RAS to MIP"
  },
  {
    "objectID": "XSdm_intercomp_crosswalk.html#harmonizing-the-models",
    "href": "XSdm_intercomp_crosswalk.html#harmonizing-the-models",
    "title": "Cross Section Data Model Comparison and Interoperability Crosswalk",
    "section": "Harmonizing the models",
    "text": "Harmonizing the models\nAs we can see, there are a number of small variations and subtle nuances to the way applications needed to schematically represent the concept of a cross section. The cleanest way to conceptually harmonize these is:"
  },
  {
    "objectID": "ZS.html#how-to",
    "href": "ZS.html#how-to",
    "title": "Zonal Statistics",
    "section": "How to",
    "text": "How to",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Raster Math",
      "Zonal Statistics"
    ]
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "Applications",
    "section": "",
    "text": "One of the best ways to demo concepts is to use real world examples. Here are a cross section of some of the most common ones I deploy, including a set of my own making. See also my past life as an academic making Google Earth Engine applications and my GitHub profile.",
    "crumbs": [
      "Atlas",
      "Applications"
    ]
  },
  {
    "objectID": "apps.html#flood-mapping",
    "href": "apps.html#flood-mapping",
    "title": "Applications",
    "section": "Flood Mapping",
    "text": "Flood Mapping\nThe largest portion of my applications deals with FIM\n\nHEC family of applications: Notes on the HEC originating modeling utilities with an emphasis on HEC-RAS 1 and 2D\nRRASSLER: A HEC-RAS 1D model accounting software\nRAS2FIM: A HEC-RAS 1D model library generation software\nCFIM: Continental Flood Inundation Mapping libraries from OakRidge\nFOSSFlood: A technically accessible Flood Impact Mapping software using CFIM\nOWP HAND FIM: The NOAA OWP HAND library generation software\nSCHISM: Total Water Level forecasts from the Office of Water Prediction\nFLDPLN: A stage based Flood Inundation Mapping Library from the University of Kansas\nFrankenFIM: A unifying framing for communicating FIM generation and execution.",
    "crumbs": [
      "Atlas",
      "Applications"
    ]
  },
  {
    "objectID": "apps.html#metrology",
    "href": "apps.html#metrology",
    "title": "Applications",
    "section": "Metrology",
    "text": "Metrology\nThere are also several ways we measure the world. Most of these techniques require a remote pilot’s licence.\n\nGPS - Global Positioning Systems\nSFM - Structure from Motion\nPIV - Partical Imaging Velocimitry",
    "crumbs": [
      "Atlas",
      "Applications"
    ]
  },
  {
    "objectID": "apps.html#assorted",
    "href": "apps.html#assorted",
    "title": "Applications",
    "section": "Assorted",
    "text": "Assorted\nAnd a random grab bag of things I’ve found useful to document including:\n\nnlp - Natural Language Processing\ncensus data\n\nI’ve also got a “now” page, glossary and junk drawer.",
    "crumbs": [
      "Atlas",
      "Applications"
    ]
  },
  {
    "objectID": "atlas_index.html",
    "href": "atlas_index.html",
    "title": "Atlas",
    "section": "",
    "text": "Taking an object oriented approach to a geographers workflow, a lot of the steps (functions) that one might want to perform on a day to day basis can be generalized and simplified into bite sized pieces of copy-paste-able code that I’ve attempted to sort out here in what I’ve taken to calling my Atlas. In the spirit of “working with the garage door open”, these are my references, odd bits and pieces of operations, self contained teaching units, and open air workflows/frameworks that I find myself repeatedly searching for or need to reference back to when I pick the project up again in 6 months. My garage is constantly under construction, please excuse the mess. It’s also attached to the house (the rest of my note taking system/digital footprint), so you may find placeholder pages, unfinished topics, links that 404 on you, or rarely strange bracket fenced numbers like: [[20220227120733]]. That’s just the most streamlined form I can come up with given the constraints of the systems and tools I use.",
    "crumbs": [
      "Atlas"
    ]
  },
  {
    "objectID": "atlas_index.html#atlas-layout",
    "href": "atlas_index.html#atlas-layout",
    "title": "Atlas",
    "section": "Atlas layout",
    "text": "Atlas layout\nThe format and layout you’ll find here is still a little too fluid for my liking, but is broadly separated into the following categories:\nI place my larger frameworks and workflows in sensemaking.\nEarth Science is a backstop category for me to place foundation knowledge not encompassed by…\nGeoprocessing, which is my more technical reference material, particularly related to accomplishing common geospatial operations (reprojecting, interpolation, raster math) but also includes a smattering of tutorials and how-to’s on useful tools.\nApplications is a place for me to drop more generic how-to’s and resources (e.g. Working with Census data, TIN editing, ect.), and my own set of applications and explanations for research and work.\nGlossary is a glossary.\nFinally, my Junk drawer is the place I’m throwing all the miscellaneous links and bits that don’t quite fold into the other categories but I’m unwilling to let go of quite yet.",
    "crumbs": [
      "Atlas"
    ]
  },
  {
    "objectID": "atlas_index.html#better-use-of-your-time",
    "href": "atlas_index.html#better-use-of-your-time",
    "title": "Atlas",
    "section": "Better use of your time",
    "text": "Better use of your time\nThe form this Atlas takes is partially my own but also largely a reflection, distillation, and grateful pilfering of several far more authoritative and comprehensive references and thinkers. I heartily recommend checking them out before you waste your time here. If you are just starting your geospatial journey, Bradley A. Shellito makes a phenomenal signpost with his Introduction to Geospatial Technologies, from which I gratefully pilfer much of the structure found in my own Introduction to GEOINT. After that, I lean on and gratefully pilfer content heavily from Spatial Data Science by Edzer Pebesma and Roger Bivand. For general explainers and references related specifically to GIS I usually land on gisgeography.com, or hydro-informatics.com for slightly more advanced hydrogeocompuation guidance. The Physics Travel Guide is a really neat conceptual starting point for a lot of physics-centric references, and if you’d like some “general” engineering refreshers you’ll be hard pressed to do better than the Engineering Library. There’s also a wealth of earth science references from the COMET MetEd program, appropriately self-billed as the authoritative “Teaching and Training Resources for the Geoscience Community”. If you need workflows, the framings of Diataxis make it pretty easy to more readily create more durable words and content, and the Zettlr and Quarto resources can help you capitalize on those efforts. After that, I would go explore, in no particular order:\n\nWhile I reread and revisit many of these links frequently; I point to them as resources and references. I am not endorsing or intend to suggest that you need to buy, subscribe, or log into anything.\n\nA densely linked list of incredible people, comprehensive documents, and full-fledged classes and books.\n\nManuel Gimnod’s Introduction to GIS and Spatial Analysis.\nRobin Lovelace’s Geocomputation with R.\nAn Introduction to Statistical Learning in both R and Python.\nMike Johnson’s (R based) Quantitative Reasoning for Ecosystem Science and introduction to Geographic Information Science classes.\nOpen Geospatial Solutions and Qiusheng Wu’s website and the classes pointed from his GitHub Page.\nBig Book of R compiled by Oscar Baruffa is a great catchall to start your R exploration including:\n\nData Visualization: A practical introduction by Kieran Healy. \nUsing Spatial Data with R by Claudia A Engel.\nr-statistics.co by Selva Prabhakaran.\nWeb Application Development with R Using Shiny by Chris Beeley and Shitalkumar R. Sukhdeve.\nR for Data Science by Garrett Grolemund.\nblogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, and Alison Presmanes Hill.\nThe FACES Research group has a series of geospatial (and adjacent) courses available.\n\nCyril Desjouy’s homepage is a great python/hydralics document landing page.\nDavid Maidment’s digital trail is a standard I strive towards.\nhttps://geoscripting-wur.github.io/, a very neat implementation of a class.\nGeospatial Analysis — A comprehensive guide (6th edition) by de Smith, M. J., M. F. Goodchild, P. A. Longley (2018) may serve as a great guide to use as a course book should you want one.\n\nsits: Satellite Image Time Series Analysis on Earth Observation Data Cubes and the SITS repo.\nGIS lounge has compiled a nice cross section of links to open course-ware across the academic disciplines, and a few GIS-centric resources.\ngisgeography.com provides a wide range of general explainers and references related specifically to GIS.\nESRI documentation, although obviously platform-centric, does a good job of putting theory before practice.\nA massive list of resource guides from Mike Royal.\n\n\nOther great GIS Resources\n\nfrom UCGIS\n\nA compiled list of resources to expand classes which serve as a great spot to find tutorials and other class material.\nRecorded webinars from the UCGIS Webinars & Workshops series.\nThe UCGIS Body of Knowledge is a great place to start for your broad GIS questions or to get an overview on a particular topic.\n\n\n\nFrom ESIP\n\nThe ESIP community and the Cloud Computing Cluster are the place to be to see cutting edge cloud “better operating procedures” in action.\n\n\n\nGoogle Earth Engine\n\nThe official list of Google Earth Engine tutorials and developer guides are a great place to start your self-guided introduction to the platform.\nOther useful slide decks: Google Earth Engine Outreach (from AGU 2019)\n\n\n\nFrom PANGEO & Friends\n\nThe Pangeo platform is a great place to start if you’re particularly interested in the infra and comp. sci. side of pythonic geospatial.\nProject Pythia: “An education and training hub for the geoscientific Python community” has some great cookbooks to run.\nEarth Lab has a massive amount of R and Python tutorials related to Earth Data Science.\n\n\n\nFrom the broader community\n\nWhat is “Geoscience”: Maps of Academic Disciplines.\nTutorials covering a nice cross section of social GIS from the Center for Spatial Data Science (CSDS).\nThe COMET MetEd program is the authoritative “Teaching and Training Resources for the Geoscience Community”.\nMIT OpenCoarseWare, free college classes with none of the skewing that “buy course credit” implies.\n\n\n\n\nModels and inspiration\nA curated list of incredible people, broad or targeted reference materials and visual inspiration to aim for.\n\nEveryone I follow on GitHub and LinkedIn :wink\nThe Physics Travel Guide is a really neat conceptual starting point for a lot of physics-centric references.\nhydro-informatics.com and hydrolearn.org provides hydrogeocompuation guidance from a broad range of professors.\nThe Quantum Well covers topics across mathematics and physics in an inspiring way.\nComplexity Explorables is “A collection of interactive explanations of complex systems in biology, physics, mathematics, social sciences, epidemiology, ecology and other fields….”.\nA few from John Nelson’s website, youtube channel, and professional outlet, a phenomenal cartographer and tutorial writer.\n\nESRI educational manager Joseph Kerski, and the Spatial Reserves blog are a great example of what a skilled and broadly trained geographer is capable of accomplishing.\nhydroblog from Josh Erickson has a neat assortment of his work for the US forest service.\nTake some cartographic/technologic inspiration from Derek Watkins.\nPaul Ramsey’s blog follows adventures in FOSS.\nFabio Crameri’s website, in particular his earth science graphics, are a striking source of visual/conceptual inspiration.\nHakim El Hattab made revealjs, and his site is a phenomenal living document.\nNavid Constantinou’s page is another visual benchmark to aim for.\nPeter Kovesi’s body of work is another fascinating angle into the technologic.\nMichael Pyrcz’s hope page is chock full of knowledge nuggets.\nDan Oehm blends data science (R) and passion projects very elegantly.\nWalker Data presents an amazingly polished open source ecosystem around census and R-centric topics.\nWould this list be complete without the FOSS Academic?\nLucas Sterzinger does amazing Open Source work across the data and atmospheric sciences.\nQiusheng Wu’s blog, website, and YouTube Channel, who also straddles the FOSS/GEE line.\nCarl Boettiger’s lab group is a great place for little R tips.\nTJ Mahr’s blog is another great place for R-centric tips.\nDanielle Navarro is a reference I reach for often straddling data science and R.\nSee the other inspirations below!\n\n\nNeat youtubes\n\n3Blue1Brown is absolutely amazing. I wish this was available when I was sitting through math, and consider this mandatory viewing.\nPBS Space Time puts together physics-centric videos that revolve around, well, space and time.\nNumberphile covers interesting or otherwise unique aspects of math in a casual but informative way.\nMy inner film major loves the overly analyzed aspects of film covered in Every Frame a Painting.\nCaptain Disillusion does a phenomenal job explaining digital concepts and demonstrating how modern CGI can be used.\nThe CUAHSI, UCGIS, ESIP, Pangeo, and The Surface Dynamics Modeling Lab at UA channels feature many of their recorded webinars.\nStruthless has a pretty phenomenal way of making sense around the insanity.\nFinally, Apetor can teach us all a little something about how we should live life (I’d start here).\n\n\n\nOther inspirations\n\nThe NowNowNow pages, InformationIsBeautiful, and daywreckers are a great place to find inspiration.\n\nWebring is another cool concept that points to people who have jumped on the “learning out loud” train\n\nSearch: site:publish.obsidian.md &lt;term&gt;\nSzymon Kaliski, Q, and Xah Lee makes a really neat homepage and visualizations spanning a grabbag of topics I’m invested or adjacent to and who’s form resonated with me.\nI can always find a prompt (in a neat form) to at Q’s notes, James Quiambao’s page, Wesleyac’s notebook, mentalnodes, WorryDream and his quote list, Change2change, ifatglassman\nA host of folks who “learn out loud” including inspirations in form and content from:\n\nAndy Matuschak\nTiago Forte & PARA\n“How to make sense of any mess” by Abby Covert\nRob’s Haisfield’s site and scaling synthesis\nThe Dayton Experiment\nJoel Hooks\nTom Critchlow\nKlajdi Puka\nBrendan Schlagel\nMasumbuko Semba\n\nFramings and modern day philosophizing from:\n\nHendrik Erz and Zettlr.\nDiataxis.\nA lifetimes worth of friends, mentors, and great conversation partners.",
    "crumbs": [
      "Atlas"
    ]
  },
  {
    "objectID": "carto.html",
    "href": "carto.html",
    "title": "Cartography",
    "section": "",
    "text": "(Stoelzle and Stein 2021)\nOther tips: Put a bird on it."
  },
  {
    "objectID": "carto.html#tutorial",
    "href": "carto.html#tutorial",
    "title": "Cartography",
    "section": "tutorial",
    "text": "tutorial"
  },
  {
    "objectID": "carto.html#how-to-guides",
    "href": "carto.html#how-to-guides",
    "title": "Cartography",
    "section": "how-to guides",
    "text": "how-to guides"
  },
  {
    "objectID": "carto.html#reference",
    "href": "carto.html#reference",
    "title": "Cartography",
    "section": "Reference",
    "text": "Reference\n\nExamples\n\nInteractive\n\nAncient Earth Globe: What the earth looked like n years ago ### Other resources https://nationalequityatlas.org/lab/get-started-data-visualization https://handsondataviz.org/ https://blog.mapbox.com/7-best-practices-for-mapping-a-pandemic-9f203576a132 https://rpubs.com/KingaHill/947744 https://environmentalinformatics-marburg.github.io/mapview/popups/html/popups.html https://flatgeobuf.org/examples/leaflet/filtered.html https://stackoverflow.com/questions/50111566/applying-leaflet-map-bounds-to-filter-data-within-shiny https://plugins.qgis.org/planet/user/29/tag/uncategorized/#:~:text=From%20the%20Print%20Layout%20toolbar,to%20a%20circular%20shape!)."
  },
  {
    "objectID": "carto.html#explanation",
    "href": "carto.html#explanation",
    "title": "Cartography",
    "section": "Explanation",
    "text": "Explanation\n\nAbout Brutalist Cartography\n\n\n\nfcc64f58866e4c4e53ea54437b3f21b2.png\n\n\nBrutalist Cartography is a term of my own making which I use to describe the way I prepare the QGIS window for a screenshot. Even if you’ve mastered the toolset and have the eye for aesthetics, good cartography can take quite a while to assemble and present. That artistic flair is usually appreciated by all, but can slow the iteration process down quite substantially and is secondary to the main task of that map (for most use cases: see [[20240508224456]] Defining roles and goals), which is to place a tractable example of a theoretical state of the data in front of someone else with the goal of exampling problems, placing extra geographic context on a piece of data, or simply adding a better looking (in my opinion) screenshot.\nTo accomplish the Brutalist cartography Aesthetic, one simply turns off most of the QGIS user interface while more prominently accentuation an organized legend, an overview window, and some of the more common map decoration items like the corner attribution image and the scale bar.\nAs a helpful note to future me, the panels that are typically turned on include:\nPanels:\n\nBrowser\nLayers\n\nToolbars:\n\nAnnotations\nAttributes\nData Source Manager\nDigitizing\nHelp\nLabel\nManage layers\nMap Navigation\nMesh digitization\nPlugin\nProject\nSelection\nVector\nWeb\n\n\nThe Final extra Step\nGo to settings and turn font size to something larger like 24, close and reopen. The downside is that you have to close and reopen to change this back again, and unfortunatly this makes many of the menues unusable since the moniter resolution remains fixed."
  },
  {
    "objectID": "carto.html#old-holdover",
    "href": "carto.html#old-holdover",
    "title": "Cartography",
    "section": "Old holdover",
    "text": "Old holdover\nAlthough we could spend the day dissecting all the minute cartographic and stylistic errors the authors may have made in there initial map, the larger issue is the disingenuous presentation of the data.\nThe map in the inital assessment, shown below and recreated for demonstration porpouses, was the map provided for the impact assesment. \n\n\n\nimage (1).png\n\n\nThe assessment makes it appear that only 1 census block of predominantly minority populations are affected, when looking at the breakdown of impacted population as a total of the study domain we see those are far closer to a 50/50 distribution"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "```{r}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(doParallel)\nlibrary(workflowsets)\nlibrary(finetune)\nlibrary(themis)\n\n# Set random seed\nset.seed(234)\n\n# split data for train/test, stratify by “phase”\ndata_split  &lt;- rsample::initial_split(df, strata =“phase”)\n\n# training data split\nyour_training_data  &lt;- rsample::training(data_split)\n\n# testinng data split\nyour_testing_data  &lt;- rsample::testing(data_split)\n\n# Base recipe\nbase_recipe &lt;- \n  recipes::recipe(\n    formula = variable_you_want_to_predict ~ ., \n    data    = your_training_data\n  ) %&gt;% \n  recipes::update_role( # update any roles that are just used for IDing observations and not used as predictors\n    game_id, team, opponent, season, week, new_role = \"ID\"\n  ) \n\n# normalized recipe\nnorm_recipe &lt;- \n  base_recipe %&gt;% \n  # recipes::step_novel(recipes::all_nominal_predictors()) %&gt;% \n  # recipes::step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;% \n  recipes::step_dummy(recipes::all_nominal_predictors()) %&gt;% \n  recipes::step_zv(recipes::all_predictors()) %&gt;% \n  recipes::step_normalize(recipes::all_numeric_predictors())\n# themis::step_smote(win, over_ratio = 0.8,  skip = T)  # balance out uneven categories in variable you’re trying to predict \n\n# ********************\n# ---- Model Spec ----\n# ********************\n\nranger_spec &lt;-\n  rand_forest(\n    mtry  = tune(),\n    min_n = tune(),\n    trees = tune()\n  ) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\")\n\nxgboost_spec &lt;- \n  boost_tree(\n    trees          = tune(),\n    min_n          = tune(),\n    tree_depth     = tune(), \n    learn_rate     = tune(), \n    loss_reduction = tune(), \n    sample_size    = tune()\n  ) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  parsnip::set_engine(\"xgboost\", importance = \"permutation\") \n\n# ********************************\n# ---- Cross Validation folds ----\n# ********************************\n\n# Set seed for resampling \nset.seed(432)\n\n# CV folds\nmy_folds &lt;- rsample::vfold_cv(your_training_data, v = 10, strata = “phase”)\n\n# ---- Workflow set of models ----\nnfl_wfs &lt;-\n  workflow_set(\n    preproc = list(\n      xgboost_rec     = norm_recipe,\n      ranger_rec     = norm_recipe\n    ),\n    \n    models  = list(\n      xgboost        = xgboost_spec,\n      ranger = ranger_spec\n    ),\n    cross = F\n  )\n\n\n# Choose metrics\nmet_set &lt;- yardstick::metric_set(\n  roc_auc, accuracy, mn_log_loss, \n  sensitivity, specificity, j_index\n)\n\n# Set up parallelization, using computer's other cores\nparallel::detectCores(logical = FALSE)\nmodeltime::parallel_start(6, .method = \"parallel\")\n\n# Set Random seed\nset.seed(589)\n\n\n# Efficient Tuning of models in workflowset\nmy_wfs &lt;-\n  my_wfs %&gt;%\n  workflowsets::workflow_map(\n    \"tune_race_anova\",\n    resamples = my_folds,\n    grid      = 20,\n    metrics   = met_set,\n    control = finetune::control_race(\n      verbose       = TRUE,\n      save_pred     = TRUE,\n      verbose_elim  = TRUE,\n      save_workflow = TRUE\n    ),\n    verbose   = TRUE\n  )\n\n# Tune models in workflowset\n#my_wfs &lt;-\n#my_wfs %&gt;%\n#workflow_map(\n# \"tune_grid\",\n#resamples = my_folds ,\n#grid      = 20,\n#metrics   = met_set,\n#control   = control_grid(\n# verbose   = TRUE,\n#save_pred = TRUE),\n#verbose   = TRUE\n#)\n\n# Stop parrallelization\nmodeltime::parallel_stop()\n\n# print ranks table\nwfs_ranks &lt;- rank_results(my_wfs)\nwfs_ranks\n\n# Comparing Accuracy and ROC AUC of models\nclass_mod_comp_plot &lt;-\n  my_wfs %&gt;%\n  autoplot() + \n  labs(\n    col = \"\",\n    title    = \"Classification Model comparisons\"\n  ) \n\nclass_mod_comp_plot \n```"
  },
  {
    "objectID": "classification.html#machiene-learning",
    "href": "classification.html#machiene-learning",
    "title": "Classification",
    "section": "",
    "text": "```{r}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(doParallel)\nlibrary(workflowsets)\nlibrary(finetune)\nlibrary(themis)\n\n# Set random seed\nset.seed(234)\n\n# split data for train/test, stratify by “phase”\ndata_split  &lt;- rsample::initial_split(df, strata =“phase”)\n\n# training data split\nyour_training_data  &lt;- rsample::training(data_split)\n\n# testinng data split\nyour_testing_data  &lt;- rsample::testing(data_split)\n\n# Base recipe\nbase_recipe &lt;- \n  recipes::recipe(\n    formula = variable_you_want_to_predict ~ ., \n    data    = your_training_data\n  ) %&gt;% \n  recipes::update_role( # update any roles that are just used for IDing observations and not used as predictors\n    game_id, team, opponent, season, week, new_role = \"ID\"\n  ) \n\n# normalized recipe\nnorm_recipe &lt;- \n  base_recipe %&gt;% \n  # recipes::step_novel(recipes::all_nominal_predictors()) %&gt;% \n  # recipes::step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;% \n  recipes::step_dummy(recipes::all_nominal_predictors()) %&gt;% \n  recipes::step_zv(recipes::all_predictors()) %&gt;% \n  recipes::step_normalize(recipes::all_numeric_predictors())\n# themis::step_smote(win, over_ratio = 0.8,  skip = T)  # balance out uneven categories in variable you’re trying to predict \n\n# ********************\n# ---- Model Spec ----\n# ********************\n\nranger_spec &lt;-\n  rand_forest(\n    mtry  = tune(),\n    min_n = tune(),\n    trees = tune()\n  ) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\")\n\nxgboost_spec &lt;- \n  boost_tree(\n    trees          = tune(),\n    min_n          = tune(),\n    tree_depth     = tune(), \n    learn_rate     = tune(), \n    loss_reduction = tune(), \n    sample_size    = tune()\n  ) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  parsnip::set_engine(\"xgboost\", importance = \"permutation\") \n\n# ********************************\n# ---- Cross Validation folds ----\n# ********************************\n\n# Set seed for resampling \nset.seed(432)\n\n# CV folds\nmy_folds &lt;- rsample::vfold_cv(your_training_data, v = 10, strata = “phase”)\n\n# ---- Workflow set of models ----\nnfl_wfs &lt;-\n  workflow_set(\n    preproc = list(\n      xgboost_rec     = norm_recipe,\n      ranger_rec     = norm_recipe\n    ),\n    \n    models  = list(\n      xgboost        = xgboost_spec,\n      ranger = ranger_spec\n    ),\n    cross = F\n  )\n\n\n# Choose metrics\nmet_set &lt;- yardstick::metric_set(\n  roc_auc, accuracy, mn_log_loss, \n  sensitivity, specificity, j_index\n)\n\n# Set up parallelization, using computer's other cores\nparallel::detectCores(logical = FALSE)\nmodeltime::parallel_start(6, .method = \"parallel\")\n\n# Set Random seed\nset.seed(589)\n\n\n# Efficient Tuning of models in workflowset\nmy_wfs &lt;-\n  my_wfs %&gt;%\n  workflowsets::workflow_map(\n    \"tune_race_anova\",\n    resamples = my_folds,\n    grid      = 20,\n    metrics   = met_set,\n    control = finetune::control_race(\n      verbose       = TRUE,\n      save_pred     = TRUE,\n      verbose_elim  = TRUE,\n      save_workflow = TRUE\n    ),\n    verbose   = TRUE\n  )\n\n# Tune models in workflowset\n#my_wfs &lt;-\n#my_wfs %&gt;%\n#workflow_map(\n# \"tune_grid\",\n#resamples = my_folds ,\n#grid      = 20,\n#metrics   = met_set,\n#control   = control_grid(\n# verbose   = TRUE,\n#save_pred = TRUE),\n#verbose   = TRUE\n#)\n\n# Stop parrallelization\nmodeltime::parallel_stop()\n\n# print ranks table\nwfs_ranks &lt;- rank_results(my_wfs)\nwfs_ranks\n\n# Comparing Accuracy and ROC AUC of models\nclass_mod_comp_plot &lt;-\n  my_wfs %&gt;%\n  autoplot() + \n  labs(\n    col = \"\",\n    title    = \"Classification Model comparisons\"\n  ) \n\nclass_mod_comp_plot \n```"
  },
  {
    "objectID": "color.html",
    "href": "color.html",
    "title": "Color",
    "section": "",
    "text": "From https://r-graph-gallery.com/38-rcolorbrewers-palettes.html\nRColorBrewer::display.brewer.all()\nFrom https://colorspace.r-forge.r-project.org/\ncolorspace::hcl_palettes(\"sequential (multi-hue)\", n = 7, plot = TRUE)\nFrom https://github.com/thomasp85/scico\nscico::scico_palette_show()\nFrom https://sape.inf.usi.ch/quick-reference/ggplot2/colour\nd=data.frame(c=colors(), y=seq(0, length(colors())-1)%%66, x=seq(0, length(colors())-1)%/%66)\nggplot2::ggplot() +\n  ggplot2::scale_x_continuous(name=\"\", breaks=NULL, expand=c(0, 0)) +\n  ggplot2::scale_y_continuous(name=\"\", breaks=NULL, expand=c(0, 0)) +\n  ggplot2::scale_fill_identity() +\n  ggplot2::geom_rect(data=d, mapping=ggplot2::aes(xmin=x, xmax=x+1, ymin=y, ymax=y+1), fill=\"white\") +\n  ggplot2::geom_rect(data=d, mapping=ggplot2::aes(xmin=x+0.05, xmax=x+0.95, ymin=y+0.5, ymax=y+1, fill=c)) +\n  ggplot2::geom_text(data=d, mapping=ggplot2::aes(x=x+0.5, y=y+0.5, label=c), colour=\"black\", hjust=0.5, vjust=1, size=3)",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Color"
    ]
  },
  {
    "objectID": "color.html#references",
    "href": "color.html#references",
    "title": "Color",
    "section": "References",
    "text": "References\n\nTools\n * https://encycolorpedia.com/ is a good place to get color names, tailwindcss is also a good reference. * colorbrewer a great place to find good looking palette, needs no introduction. * Canva’s color wheel has a quick picker and a nice refresher on harmonious color palettes, paletton gives you even more control. * Scientific color maps, Perceptually Uniform Colour Maps, and a handy ramp generator for a pinch. * A website/tri-color palette tester/generator lets you see what a palette looks like on things. * Cooler has: * A quick pallete generator * image color picker * inspiration * (Crameri 2023) through R and Python. * Video refreshers on Color Theory and Color Theory in Practice.\n https://www.sessions.edu/color-calculator/",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "Color"
    ]
  },
  {
    "objectID": "coords.html",
    "href": "coords.html",
    "title": "Coordinates",
    "section": "",
    "text": "```{md}\nhome                      &lt;- landing page\n├── tutorial              &lt;- landing page\n│   ├── part 1\n│   ├── part 2\n│   └── part 3\n├── how-to guides         &lt;- landing page\n│   ├── install\n│   ├── deploy\n│   └── scale\n├── reference             &lt;- landing page\n│   ├── commandline tool\n│   ├── available endpoints\n│   └── API\n└── explanation           &lt;- landing page\n    ├── best practice recommendations\n    ├── security overview\n    └── performance\n```"
  },
  {
    "objectID": "coords.html#datums",
    "href": "coords.html#datums",
    "title": "Coordinates",
    "section": "Datums",
    "text": "Datums\nNAD83-2011 is the 2011 realization (Epoch 2010.00) of the NAD83 geographic coordinate system. NAD83 is defined on the GRS80 ellipsoid and tied to the North American tectonic plate (hence, the periodic updates). WGS84 is a geographic coordinate system defined on the WGS84 ellipsoid (also updated periodically). That said, the GRS80 ellipsoid and the WGS84 ellipsoid are very similar in size and shape.\n(WGS84) UTM Zone 10N is an example of a projected coordinate system; where WGS84 coordinates in Zone 10N are projected (using the transverse Mercator method) onto the Zone 10N grid.\n(NAD83-2011) State Plane Texas Central is another example of a projected coordinate system; the NAD83-2011 coordinates in central Texas are projected (using the Lambert Conformal Conic method) onto the Texas Central grid.\n\n\n\nRef. Sys.\nEllipsoid\na (m)\n1/f\nf=(a-b)/a\n\n\n\n\nWGS 84\nWGS 84\n6378137.0\n298.257223563\n\n\n\nNAD 83\nGRS 80\n6378137.0\n298.257222101\n\n\n\nNAD 27\nClarke 1866\n6378206.4\n294.9786982\n\n\n\n\nOrthometric heights - NAVD 88 geopotential heights elipsoidal heights\nhttps://www.google.com/search?q=ngvd29&rlz=1C1ONGR_enUS1058US1058&oq=nvgd&aqs=chrome.2.69i57j0i10i512l9.3343j0j7&sourceid=chrome&ie=UTF-8 https://geodesy.noaa.gov/datums/vertical/north-american-vertical-datum-1988.shtml https://www.foresitegroup.net/post/understanding-navd-88-and-ngvd-29-elevation-measurements\n\nTidal heights\nCoasts are a particularly difficult thing to measure due to the efer changing nature of the landscape and the intense ossolations of the tides. This difficulty is compounded by the different pollitical bounds the different orginizations exercise over the system, and the relative “arbitrarieness” that accompaines assosiating a datum to a water level. Below is a small inforgraphic of the different datums, zones, and the different state and federal entities that exersize control over their domains. These are not all of them, but the most common include (from inland domatins) the Mean High, High Water Mark, a mean of the highest high water marks for a given area, 2) the https://tidesandcurrents.noaa.gov/datum_options.html"
  },
  {
    "objectID": "coords.html#vdatum",
    "href": "coords.html#vdatum",
    "title": "Coordinates",
    "section": "VDATUM",
    "text": "VDATUM\n\npythonr\n\n\nFrom the inundation-mapping repo: https://github.com/NOAA-OWP/inundation-mapping/blob/c585dec276f1ae953a708962539cd0ee836403d0/tools/tools_shared_functions.py#L989\ndef ngvd_to_navd_ft(datum_info, region = 'contiguous'):\n    '''\n    Given the lat/lon, retrieve the adjustment from NGVD29 to NAVD88 in feet. \n    Uses NOAA tidal API to get conversion factor. Requires that lat/lon is\n    in NAD27 crs. If input lat/lon are not NAD27 then these coords are \n    reprojected to NAD27 and the reproject coords are used to get adjustment.\n    There appears to be an issue when region is not in contiguous US.\n\n    Parameters\n    ----------\n    lat : FLOAT\n        Latitude.\n    lon : FLOAT\n        Longitude.\n\n    Returns\n    -------\n    datum_adj_ft : FLOAT\n        Vertical adjustment in feet, from NGVD29 to NAVD88, and rounded to nearest hundredth.\n\n    '''\n    #If crs is not NAD 27, convert crs to NAD27 and get adjusted lat lon\n    if datum_info['crs'] != 'NAD27':\n        lat, lon = convert_latlon_datum(datum_info['lat'],datum_info['lon'],datum_info['crs'],'NAD27')\n    else:\n        #Otherwise assume lat/lon is in NAD27.\n        lat = datum_info['lat']\n        lon = datum_info['lon']\n    \n    #Define url for datum API\n    datum_url = 'https://vdatum.noaa.gov/vdatumweb/api/convert'     \n    \n    #Define parameters. Hard code most parameters to convert NGVD to NAVD.    \n    params = {}\n    params['lat'] = lat\n    params['lon'] = lon\n    params['region'] = region\n    params['s_h_frame'] = 'NAD27'     #Source CRS\n    params['s_v_frame'] = 'NGVD29'    #Source vertical coord datum\n    params['s_vertical_unit'] = 'm'   #Source vertical units\n    params['src_height'] = 0.0        #Source vertical height\n    params['t_v_frame'] = 'NAVD88'    #Target vertical datum\n    params['tar_vertical_unit'] = 'm' #Target vertical height\n    \n    #Call the API\n    response = requests.get(datum_url, params = params, verify=False)\n\n    #If successful get the navd adjustment\n    if response:\n        results = response.json()\n        #Get adjustment in meters (NGVD29 to NAVD88)\n        adjustment = results['t_z']\n        #convert meters to feet\n        adjustment_ft = round(float(adjustment) * 3.28084,2)                \n    else:\n        adjustment_ft = None\n    return adjustment_ft \n\n\nif(vdat) {\n    date &lt;- as.POSIXct(in_epoch_override, origin = \"1970-01-01\")\n    this_date_YYYY &lt;- format(date, format=\"%Y\")\n    this_date_mm &lt;- format(date, format=\"%m\")\n    this_date_dd &lt;- format(date, format=\"%d\")\n    this_date_start &lt;- lubridate::decimal_date(lubridate::ymd(paste0(this_date_YYYY,\"-\",this_date_mm,\"-\",this_date_dd)))\n    this_date_now &lt;- lubridate::decimal_date(Sys.time())\n\n    # transform datum\n    mean_X &lt;- mean(sf::st_coordinates(sf::st_cast(sf_cross_section_lines, \"POINT\"))[,1])\n    mean_Y &lt;- mean(sf::st_coordinates(sf::st_cast(sf_cross_section_lines, \"POINT\"))[,2])\n    center_point = data.frame(lon = mean_X, lat = mean_Y) |&gt;\n      sf::st_as_sf(coords = c(\"lon\", \"lat\")) |&gt;\n      sf::st_set_crs(sf::st_crs(\"EPSG:6349\"))\n\n    # determine and apply z transform\n    datum_url &lt;- paste0(\n      \"https://vdatum.noaa.gov/vdatumweb/api/convert?\",\n      \"s_x=\",as.character(sf::st_coordinates(center_point)[1,][1]),\n      \"&s_y=\",as.character(sf::st_coordinates(center_point)[1,][2]),\n      \"&s_v_unit=m&t_v_unit=m\",\n      \"&s_h_frame=\",'NAD83_2011',\"&s_v_frame=\",this_datum,\n      \"&t_h_frame=NAD83_2011&t_v_frame=NAVD88\",\n      \"&epoch_in=\",this_date_start,\"&epoch_out=\",this_date_now\n    )\n    if(!quiet) { message(paste0(\"URL:\",datum_url)) }\n    resp &lt;- httr::GET(datum_url)\n    if(httr::http_error(resp)) {\n      print_warning_block()\n      message(paste('poorly formed url - Request URL:', datum_url))\n      return(list(data.frame()))\n    }\n    jsonRespParsed &lt;- httr::content(resp,as=\"parsed\")\n  }\n  pt_z &lt;- (point_slice[point_index,]$z * stn_unit_norm) * elev_unit_norm + as.numeric(jsonRespParsed$t_z)\n\n\n\nFrom exposure to the outside world, these distinctions seem to be glossed over by many, but who then immediately jump into sub-meter comparisons. While you can make a bit of forward progress when ignoring some of those cascading errors and uncertainties, the misalignment of those model surfaces is quickly exposed by visual inspection at the human scale, and so you end up having to backtrack and reanalysis all of the processing steps and assumptions before you can be productive again."
  },
  {
    "objectID": "css.html",
    "href": "css.html",
    "title": "CSS",
    "section": "",
    "text": "One of the most objective pieces of evidence I can point to that demonstrates that I’ve really lost it is that with the strange patterns of my working system I now make slides using CSS…",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "CSS"
    ]
  },
  {
    "objectID": "css.html#relevant-resources",
    "href": "css.html#relevant-resources",
    "title": "CSS",
    "section": "Relevant resources",
    "text": "Relevant resources\n\n(Fioritto, n.d.)\nhttps://vue-grid-generator.netlify.app/\nhttps://css-tricks.com/css-​masonry-css-grid/",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization",
      "CSS"
    ]
  },
  {
    "objectID": "dataflow.html",
    "href": "dataflow.html",
    "title": "Data Flow",
    "section": "",
    "text": "Thinking about data flow is something I try to do as infrequently as possible, but that’s unavoidable in this day and age. The largest issue I have with it is that data flow and the “better practices” of the day are not only dependent on the ever-shifting nature of technology and tools at your disposal, but also on the hat you are wearing for the day, the task you are trying to accomplish, and what sort of resource constraints your environment provides, which makes it exceptionally hard to take a firm stance on the topic. That being said, some of the firmest ground I’ve found thus far is to think about data flow as “where that data lives”.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Flow"
    ]
  },
  {
    "objectID": "dataflow.html#where-data-can-live",
    "href": "dataflow.html#where-data-can-live",
    "title": "Data Flow",
    "section": "Where Data can live",
    "text": "Where Data can live\nThe answer to this question is really a semantic distinction because all ‘real data’ is stored on a hard drive (I’m excluding archived data which may live in an archival format or data which is not digital e.g. hard-copy records), but I occasionally use terms for data access patterns that can be confusing and so I’ll elaborate here:\nLocal data: Local data, sometimes called on-disk or disk data, is data that that lives alongside the compute you are using. You can open up a file explorer or otherwise navigate to it from a mounted drive and is typically the fastest and most efficient manner in which you interact with a dataset.\nCloud native: The alternative to that is cloud native data, or data which lives or is accessed from an S3 bucket. Although that data really lives on someone elses hard drive, you interact and access it using urls and an active internet connection, and the “better practice” is to pull in small subsets of the data into your local memory to interact with at a time.\nLocation agnostic: A term I use more to describe the nature of the reader you deploy to place that data on your local machine. While all data could in theory be “location agnostic” with syntax candy, this moreso describes data stored in formats like parquet and flatgeobuf which, as is implemented in the sf package in R, can take either a path to a local file on your disk or an s3 url. The function itself takes care of the loading of that data for manipulation and you as the end user don’t have to alter syntax or thought patterns in order to take advantage of the benefits the different locations of the data provide.",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Flow"
    ]
  },
  {
    "objectID": "dataflow.html#attributes-and-tags",
    "href": "dataflow.html#attributes-and-tags",
    "title": "Data Flow",
    "section": "Attributes and tags",
    "text": "Attributes and tags\nBeyond the location and nature (vector, raster, tabular) of the data, there are several attributes, tags, standards, and principals that quickly inform you of the sorts of tools, techniques, and challenges that you will face as you start working with that data.\n\nBig\n[[BD]] Big Data\n\n\nFAIR\nDefining the FAIR acronym [[20240313142448]] FAIR data\n\n\nCloud native data\nDefining the ARCO acronym [[20240313183434]] A note on “ARCO” and “Cloud Native” data\n“Analysis-ready Cloud Optimized” (ARCO) and “Cloud Native” are terms used to connote nature of the sturcture, shape, and",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Flow"
    ]
  },
  {
    "objectID": "dataflow.html#formats",
    "href": "dataflow.html#formats",
    "title": "Data Flow",
    "section": "Formats",
    "text": "Formats\nCloud Optimized GeoTIFFs (COG) Zarr Kerchunk Cloud-Optimized HDF5 and NetCDF Cloud-Optimized Point Clouds (COPC) GeoParquet FlatGeobufStac PMTiles",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Data Flow"
    ]
  },
  {
    "objectID": "diagramming.html",
    "href": "diagramming.html",
    "title": "Diagramming",
    "section": "",
    "text": "Diagramming adds spatial intuition to what would otherwise be an aspatial problem and as a communication aid, diagrams are a critical currency in both content and understanding. So do it more.…\n[[DF.qmd]] Data Flow Diagram [[20240917010756]] Causal loop diagrams"
  },
  {
    "objectID": "dtools.html",
    "href": "dtools.html",
    "title": "Data Science",
    "section": "",
    "text": "I hate computers and I’d balk if you called me anything adjacent to a technologist, but too much of my domain of experience and expertise overlaps with what others might call computer science/data science/data engineer/[insert newest hot job title here]. The only reason I can get the computer to beep at me the right way is because I spend an unhealthy amount of time in front of it worrying over the problem, rerunning procedures with small tweaks, and many sleepless nights weeping softly curled up in a ball under my desk. I really struggle to find any value in my opinions on data science. You probably know how to use a computer better than I, and there’s something too ephemeral and mutable about computers, technology, and the best practices that make it seem like a problem I can’t gain traction on or contribute meaningfully to. Much like the rest of my site, this side will likely never be fleshed out but that backstop is important to be able to talk intelligently about this topic. Unlike my attempts at [functionalizing GISelementsofGIST.qmd) though, I don’t feel a great need to identify the functional bits of computer science in quite the same way. Separating conversations about data and compute can be a natural breakline to follow, but after a certain level, separating those concerns ultimately seems to result in more headache than it saves, and anyone who’s looking to actually make tractable progress on a wicked puzzle is better off taking a step back and listen/observe what the actual objective is before leaping in. What I can scrape together I’ll place here; some of my notes and references related directly to how I accomplish common data manipulations."
  },
  {
    "objectID": "dtools.html#hydroinformatics-vs-data-scinece",
    "href": "dtools.html#hydroinformatics-vs-data-scinece",
    "title": "Data Science",
    "section": "Hydroinformatics vs data scinece",
    "text": "Hydroinformatics vs data scinece\nI could probably take a whole chapter to discuss the finer points of the techincal revolution and the rise of digital as it relates to geosciences and why now is sort of special. Hydroinformatics involves numerical modeling as well as data science methods for the efficient and sustainable use of water resources (definition derived from the IAHR’s and IWA Joint Committee on Hydroinformatics)."
  },
  {
    "objectID": "eHydro.html",
    "href": "eHydro.html",
    "title": "eHydro",
    "section": "",
    "text": "eHydro is the name of the database of official soundings from the US Army Corp Of Engineers. It’s been largely standardized in data structure and richly metadata’d, but requires extensive modeler skill in order to be able to capitalize on the full breadth of data available to you. If you need a shortcut to the most recent and relevant data in power-user form, consider deploying ePydRo, or simply reach for the best possible, pre-made surface from NOAA’s master modeling surface here. As a data product, the eHydro database grows on an almost daily basis and is as close to a publicly available, near-real time observation of vital model parameterizations needed to make hydrodynamic models suitable for operational deployment.\n[[20240606113900]] quick ehydro/ras map\nWe’ve developed tools and workflows that capitalize on all that hard work and make this invaluable database more accessible, see [[ePydRo]] ePydRo\n\nDownload updated footprints\nYou can find a link to those in a few different formats at the USACE arcgis data page (note: you may have to just try and click a few times if you get an initial download errror): https://geospatial-usace.opendata.arcgis.com/datasets/80a394bae6b547f1b5788074261e11f1/explore?location=4.370274%2C167.280410%2C1.00\n\n\nPrep a survey\n\n\nReformat\n\nread\n\n\nvdatum\n\n\nWrite\n\n\n\nSTAC\n\n\nPush to AWS\naws s3 sync G:\\data\\raw\\eHydro\\eHydro_Survey_Data_-8591331731584774235\\ePydRo\\ s3://lynker-spatial/bathymetry/eHydro/ePydRo/ aws s3 sync G:\\data\\raw\\eHydro\\eHydro_Survey_Data_-8591331731584774235\\stac\\ s3://lynker-spatial/bathymetry/eHydro/stac/"
  },
  {
    "objectID": "earthscience.html",
    "href": "earthscience.html",
    "title": "Earth Science",
    "section": "",
    "text": "Earth Science encompass the branches of science dealing with the physical, chemical, and biological connections of the major earth systems, and geographers place that in context with social systems and technology. If that seems broad it’s because it is. Between the breadth of the topics I can touch, the power that bringing visual and geographic context can have on a problem, and working through the friction associated with reproducibly measuring, modeling, and communicating the state of our world, there’s a lot to love about being a GIScientist / Hydrologist / Technologist? (and one of the things that is quickly killing me). This is a place for me to reference materials and whatnot that are more conceptual than programmatic. This side of my site will likely never be fleshed out, but I wanted some form of a backstop/boundary here.\n\nResources\n\nBroadly accessible Earth Science: Brian Weir’s https://earthscience.xyz/\nCollege level earth science review from COMET MetEd: https://www.meted.ucar.edu/education_training?query=&page=1\nNEON training and educational resources: https://www.neonscience.org/resources\nComputational hydrology helpers: hydro-informatics.com\nEngineering-centric refreshers: Engineering Library\nHydrolearn: https://hydrolearn.org/\n\n\n\nSection Table of Contents\n\nHydrology\nLandforms\nPrecipitation\nSnow\nHydraulics\nHydrology and Hydraulics",
    "crumbs": [
      "Atlas",
      "Earth Science"
    ]
  },
  {
    "objectID": "evapotranspiration.html",
    "href": "evapotranspiration.html",
    "title": "Evapotranspiration",
    "section": "",
    "text": "Water flows down-gradient, and in the case of water transformed to water vapor via plant transpiration or solar energy that’s typically upwards towards the atmosphere following a vapor pressure gradient. We call this Evapotranspiration.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Evapotranspiration"
    ]
  },
  {
    "objectID": "evapotranspiration.html#executive-summary",
    "href": "evapotranspiration.html#executive-summary",
    "title": "Evapotranspiration",
    "section": "",
    "text": "Water flows down-gradient, and in the case of water transformed to water vapor via plant transpiration or solar energy that’s typically upwards towards the atmosphere following a vapor pressure gradient. We call this Evapotranspiration.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Evapotranspiration"
    ]
  },
  {
    "objectID": "evapotranspiration.html#references",
    "href": "evapotranspiration.html#references",
    "title": "Evapotranspiration",
    "section": "References",
    "text": "References\n\nThe Temporal Evapotranspiration Aggregation Method\n\nExecutive Summary\nGeographic context and seamless access to key environmental water cycling metrics is a fairly well defined operation with a massive amount of technical and digital friction associated with it. Recently, Google Earth Engine has added the ability to create UI features to the platform, creation seamless experience between data, processing, and presentation a “one stop shop, from observation to presentation”. This is an exploration of the methods and structure of that feature to more readily address what I feel are embarrassingly easy questions to pose, but an embarrassingly underwhelming capacity to quickly respond to. To do this, I’ve implemented a GUI driven, zonal statistics equivalent over a gap filled and value added evapotranspiration dataset using the MOD16 and MOD17 products. Go use the application.\nVideo\n\n\nReferences\nA GEE booth presentation, CUAHSI presentation, and related Hydroshare artifacts.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Evapotranspiration"
    ]
  },
  {
    "objectID": "fire.html",
    "href": "fire.html",
    "title": "Fire",
    "section": "",
    "text": "Is hot.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Fire"
    ]
  },
  {
    "objectID": "gcontext.html",
    "href": "gcontext.html",
    "title": "Adding Geographic Context",
    "section": "",
    "text": "In defining spatial thinking, I outlined several spatio-temporal skills a critical spatial thinker might deploy. While almost none of them are specific to visual perception, it’s the most natural and accessible means of describing them, at least to me. This speaks to the raw power that adding geographic context to a problem has in making that problem far more tractable and explainable than it otherwise would be as a wall of text, audio recording, or interpretive dance performance.  However, adding that geographic context is difficult. First, the challenge of mapping or plotting data can be an exercise in data manipulation to get that table into a spatial form in the first place. It can also be an uphill battle to get ancillary data with which to add that context against. That mosaicing of (geo)data often comes with it’s own set of hidden assumptions which are rarely explicitly communicated. Finally, everyone is an art critic and you can always make a good looking map look better with enough time and effort (and the implied diminishing returns on that investment of time that could otherwise be invested in building for that moving target).\nvis\nCartography\nI’d also like to take a quick second to call out some of my more cartographically focused resources:",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Visualization"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "I’m not usually one to play the semantics game or argue over for the strict application of language and terminology, that ends up far too often being an exercise in us trying to define words. However, It is occasionally necessary to nit-pick, and so at those times I come here; to this glossary from a man with a tenuous grasp on English in general.\nSources: “FEMA Directory” (n.d.)",
    "crumbs": [
      "Atlas",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#common",
    "href": "glossary.html#common",
    "title": "Glossary",
    "section": "Common",
    "text": "Common\n\nSurface ancroyms\n\nA DEM (Digital Elevation Model) Represents the bare-Earth surface, removing all natural and built features;\nA DTM (Digital Terrain Model) typically augments a DEM, by including vector features of the natural terrain, such as rivers and ridges. A DTM may be interpolated to generate a DEM, but not vice versa.\nDSM (Digital Surface Model) is a model that is intended to represent the bare-Earth and all of its above-ground features.\nREM - (Relative Elevation Model)s reorganize space in relation to a feature.\nHAND is the Height above nearest drainage, a REM relative to a drainage network.\n\n\n\nWords you might encounter that are interrelated:\nHydrology is the study of the cycling of the mass of water across the landscape, hydraulics is the study of how we can transform the instantaneous state of that cycle into an elevation, and flood inundation mapping is the act of placing geographic context on those calculations.\nAlluvial: Alluvial refers to sediment type created by running water. Particles often appear visibly rounded. That running water is typically found in…\nFluvial: “of or found in a river”, if you can point to a body of water, that is likely fluvial as opposed to…\nPluvial (flooding): is flooding independent of a source of water.\nFrom the standpoint of precipitation, that water can mechanically get to a location in two ways. The first…\n\nInfiltration excess: is when the rate of rainfall exceeds the rate at which water can infiltrate the soil column\n\nSaturation excess: is when added rain can not infiltrate the soil because all void space is filled.\n\nBoth tend to generate Hortonian overland flow, often modeled as lateral inflow.\n\n\nModeling terms\nThere’s not really a difference between irreproducible, unreproducible, and non-reproducible?\nThere’s not a right way to phrase irreplicable / unreplicable, but it’s not unrelicatable.\nI am of the opinion most incorrectly apply capitalization to LIDAR. LiDAR is a company, not a technique, like RADAR, and is most commonly called radar. Deering and Stoker (2014)\nI model and color, no extra “l”’s and “u”’s; we won that debate in 1776.\n\n\nWords I am trying to learn:\n\nNo\nSchedule, in spelling and spirit\nsummary, not summery the season\naerial imagery vs areal areas\nPropose a purpose\nWhether or not the weather is nice\nThe historical record has many historic floods\npublicly, not publically",
    "crumbs": [
      "Atlas",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#acronyms",
    "href": "glossary.html#acronyms",
    "title": "Glossary",
    "section": "Acronyms",
    "text": "Acronyms\n\nOrganizations and programs\n\n\nExpand\n\n\n\n\n\n\n\nAcronym\n\n\nLongform\n\n\nDefinition\n\n\n\n\n\n\nAASHTO\n\n\nAmerican Association of State Highway and Transportation Officials\n\n\n\n\n\n\nAREMA\n\n\nAmerican Railroad Engineering and Maintenance-of-Way Association\n\n\n\n\n\n\nCIROH\n\n\nCooperative Institute for Research to Operations In Hydrology\n\n\n\n\n\n\nCISESS\n\n\nCooperative Institute for Satellite Earth System Sensing\n\n\n\n\n\n\nCUAHSI\n\n\nConsortium of Universities for the Advancement of Hydrologic Science, Inc. \n\n\n\n\n\n\nESRI\n\n\nEnvironmental Systems Research Institute\n\n\n\n\n\n\nFEMA\n\n\nFederal Emergency Management Agency\n\n\nThe independent Federal agency that, among many other responsibilities, oversees the administration of the National Flood Insurance Program.\n\n\n\n\nFEMA\n\n\nFederal Emergency Management Agency\n\n\n\n\n\n\nFHWA\n\n\nFederal Highway Administration\n\n\n\n\n\n\nFIMA\n\n\nFederal Insurance and Mitigation Administration\n\n\nThe component of FEMA that has direct responsibility for administering the National Flood Insurance Program.\n\n\n\n\nHEC\n\n\nHydraulic Engineering Circular\n\n\n\n\n\n\nISO\n\n\nInternational Organization for Standardization\n\n\n\n\n\n\nJALBTCX\n\n\nJoint Airborne Lidar Bathymetry Technical Center of Expertise\n\n\n\n\n\n\nNGS\n\n\nNational Geodetic Survey\n\n\n\n\n\n\nNHI\n\n\nNational Highway Institute\n\n\n\n\n\n\nNOAA\n\n\nNational Oceanic and Atmospheric Administration\n\n\n\n\n\n\nNRC\n\n\nNational Research Council\n\n\n\n\n\n\nNWS\n\n\nNational Weather Service\n\n\n\n\n\n\nOWP\n\n\nOffice of Water Prediction\n\n\n\n\n\n\nTIGER\n\n\nTopologically Integrated Geographic Encoding and Referencing System\n\n\nThe nationwide digital database of planimetric base map features developed by the U.S. Bureau of the Census for the 1990 Census.\n\n\n\n\nUK\n\n\nUnited Kingdom\n\n\n\n\n\n\nUS\n\n\nUnited States\n\n\n\n\n\n\nUSACE\n\n\nUS Army Corps of Engineers\n\n\n\n\n\n\nUSDA\n\n\nUS Department of Agriculture\n\n\n\n\n\n\nUSGS\n\n\nUS Geological Survey\n\n\n\n\n\n\nCRESIS\n\n\nCenter for the Remote Sensing of Ice Sheets\n\n\n\n\n\n\nInc. \n\n\nIncorporated\n\n\n\n\n\n\nNASA\n\n\nNational Association of Silly Acronyms\n\n\n\n\n\n\nNOAA\n\n\nNational Organization for the Advancement of Acronyms\n\n\n\n\n\n\n\n\n\n\n\nFIM related\n\n\nExpand\n\n\n\n\n\n\n\nAcronym\n\n\nLongform\n\n\nDefinition\n\n\n\n\n\n\n\n\n500-Year Flood\n\n\nSee 0.2-Percent-Annual-Chance Flood\n\n\n\n\n\n\n50-Year Flood\n\n\nSee 2-Percent-Annual-Chance Flood.\n\n\n\n\n\n\nOrdinance Level A\n\n\nPer 44 CFR 60.3 (a), this ordinance level occurs when the Administrator has not defined the special flood hazard areas within a community, has not provided water surface elevation data, and has not provided sufficient data to identify the floodway or coastal high hazard are, but the community has indicated the presence of such hazards by submitting an application to participate in the Program.\n\n\n\n\n\n\nOrdinance Level B\n\n\nPer 44 CFR 60.3 (b), this ordinance level occurs when the Administrator has designated areas of special flood hazards (A zones) by the publication of a community’s FHBM or FIRM, but has neither produced water suface elevation data nor identified a floodway or coastal high hazard area.\n\n\n\n\n\n\nOrdinance Level C\n\n\nPer 44 CFR 60.3 (c), this ordinance level occurs when the Administrator has provided a notice of final flood elevations for one or more special flood hazard areas on the community’s FIRM and, if appropriate, has designated other special flood hazard areas without base flood elevations on the community’s FIRM, but has not identified a regulatory floodway or coastal high hazard area.\n\n\n\n\n\n\nOrdinance Level D\n\n\nPer 44 CFR 60.3 (d), this ordinance level occurs when the Administrator has provided a notice of final flood elevations within Zones A1-30 and/or AE on the community’s FIRM and, if appropriate, has designated AO zones, AH zones, A99 zones, and A zones on the community’s FIRM, and has provided data from which the community shall designate its regulatory floodway.\n\n\n\n\n\n\nOrdinance Level E\n\n\nPer 44 CFR 60.3 (e), this ordinance level occurs when the Administrator has provided a notice of final base flood elevations within Znes A1-30 and/or AE on the community’s FIRM and, if appropriate, has designated AH zones, AO zones, A99 zones, and A zones on the community’s FIRM, and has identified on the community’s FIRM coastal high hazard areas by designating Zones V1-30, and/or V.\n\n\n\n\n\n\nUnnumbered A Zone\n\n\nFlood insurance rate zones, designated “Zone A” on a FIRM, that are based on approximate studies.\n\n\n\n\n\n\nzonal classification\n\n\nClassification based data set value (ranges) and/or material properties or other attributes specified for polygonal regions as is common with GIS products.\n\n\n\n\nSFHA\n\n\nSpecial Flood Hazard Area\n\n\nThe area delineated on a National Flood Insurance Program map as being subject to inundation by the base flood. SFHAs are determined using statistical analyses of records of riverflow, storm tides, and rainfall; information obtained through consultation with a community; floodplain topographic surveys; and hydrologic and hydraulic analyses.\n\n\n\n\nSOMA\n\n\nSummary of Map Actions\n\n\nA list, generated by FEMA and delivered to the community, that summarizes the LOMAs, LOMR-Fs, and LOMRs that are or will be affected by a physical update to a FIRM.\n\n\n\n\nSOMA Category 1\n\n\nSOMA Category 1 - LOMRs and LOMAs Incorporated\n\n\nThe modifications effected by the LOMRs and LOMAs have been reflected on the Preliminary copies of the revised FIRM panels. However, these LOMRs and LOMAs will remain in effect until the revised FIRM becomes effective.\n\n\n\n\nSOMA Category 2\n\n\nSOMA Category 2 - LOMRs and LOMAs Not Incorporated (Revalidated)\n\n\nThe modifications effected by the LOMRs and LOMAs have not been reflected on the Preliminary copies of the revised FIRM panels because of scale limitations or because the LOMR or LOMA issued had determined that the lot(s) or structure(s) involved were outside the Special Flood Hazard Area, as shown on the FIRM. These LOMRs and LOMAs will be revalidated free of charge 1 day after the revised FIRM becomes effective through a single letter that reaffirms the validity of the previous LOMC.\n\n\n\n\nSOMA Category 3\n\n\nSOMA Category 3 - LOMRs and LOMAs Superseded\n\n\nThe modifications effected by the LOMRs and LOMAs have not been reflected on the Preliminary copies of the revised FIRM panels because they are being superseded by new detailed flood hazard information or the information available was not sufficient to make a determination. These LOMRs and LOMAs will no longer be in effect when the revised FIRM becomes effective.\n\n\n\n\nSOMA Category 4\n\n\nSOMA Category 4 - LOMRs and LOMAs To Be Redetermined\n\n\nThe LOMCs in Category 2 will be revalidated through a single letter that reaffirms the validity of the determination in the previously issued LOMC. For LOMCs issued for multiple lots or structures where the determination for one or more of the lots or structures has changed, the LOMC cannot be revalidated through this administrative process. The NSP will review the data previously submitted for the LOMR or LOMA request and issue a new determination for the affected properties after the effective date of the revised FIRM.\n\n\n\n\nZone A\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year floodplains that are determined in the FIS by approximate methods. Because detailed hydraulic analyses are not performed for such areas, no base flood elevations or depths are shown within this zone.\n\n\n\n\nZone A99\n\n\n\n\nThe flood insurance rate zone that corresponds to areas of the 100-year floodplain what will be protected by a Federal flood protection system where construction has reached specified statutory milestones. No base flood elevations or depths are shown within this zone.\n\n\n\n\nZone AE\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year floodplains that are determined in the FIS by detailed methods. In most instances, whole-foot base flood elevations derived from the detailed hydraulic analyses are shown at selected intervals within this zone.\n\n\n\n\nZone AH\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year shallow flooding (usually areas of ponding) where average depths are between 1 and 3 feet. Whole-foot base flood elevations are derived from detailed hydraulic analyses are shown at selected intervals within this zone.\n\n\n\n\nZone AO\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year shallow flooding ( usually sheet flow on sloping terrain) where average depths are between 1 and 3 feet. Average whole-foot depths derived from the detailed hydraulic analyses. The highest top of curb elevation adjacent to the lowest adjacent grade (LAG) must be submitted if the request lies within this zone.\n\n\n\n\nZone AR\n\n\n\n\nZone AR is the flood insurance rate zone used to depict areas protected from flood hazards by flood control structures, such as a levee, that are being restored. FEMA will consider using the Zone AR designation for a community if the flood protection system has been deemed restorable by a Federal agency in consultation with a local project sponsor; a minimum level of flood protection is still provided to the community by the system; and restoration of the flood protection system is scheduled to begin within a designated time period and in accordance with a progress plan negotiated between the community and FEMA. Mandatory purchase requirements for flood insurance will apply in Zone AR, but the rate will not exceed the rate for unnumbered A zones if the structure is built in compliance with Zone AR floodplain management regulations. For floodplain management in Zone AR areas, elevation is not required for improvements to existing structures. However, for new construction, the structure must be elevated (or floodproofed for non-residential structures) such that the lowest floor, including basement, is a maximum of 3 feet above the highest adjacent existing grade if the depth of the base flood elevation (BFE) does not exceed 5 feet at the proposed development site. For infill sites, rehabilitation of existing structures, or redevelopment of previously developed areas, there is a 3 foot elevation requirement regardless of the depth of the BFE at the project site. The Zone AR designation will be removed and the restored flood control system shown as providing protection from the 1% annual chance flood on the NFIP map upon completion of the restoration project and submittal of all the necessary data to FEMA.\n\n\n\n\nZone D\n\n\n\n\nThe flood insurance rate zone that corresponds to unstudied areas where flood hazards are undetermined but possible.\n\n\n\n\nZone Gutter\n\n\n\n\nBoundary, shown on a Flood Insurance Rate Map, dividing Special Flood Hazard Areas of different Base Flood Elevations, base flood depths, flood velocities, or flood insurance risk zone designations.\n\n\n\n\nZone V\n\n\n\n\nThe flood insurance rate zone that corresponds to the 100-year costal floodplains that have additional hazards associated with storm waves. Because approximate hydraulic analyses are performed for such areas, no base flood elevations (BFEs) are shown within this zone. Mandatory flood insurance purchase requirements apply.\n\n\n\n\nZone VE\n\n\n\n\nZone VE is the flood insurance rate zone that corresponds to the 100-year coastal floodplains that have additional hazards associated with storm waves. BFEs derived from the detailed hydraulic analyses are shown at selected intervals within this zone. Mandatory flood insurance purchase requirements apply.\n\n\n\n\nZone X\n\n\n\n\nThe flood insurance rate zone that corresponds to areas outside the 500-year floodplain, areas within the 500- year floodplain, and areas of 100-year flooding where average depths are less than 1 foot, areas of 100-year flooding where the contributing drainage area is less than 1 square mile, and areas protected from 100-year flood by levees. No base flood elevations or depths are shown within this zone.\n\n\n\n\nZone X (shaded and unshaded) B, and C\n\n\n\n\nAre the flood insurance rate zones that corresponds to areas outside the 500-year floodplain, areas within the 500- year floodplain, and areas of 100-year flooding where average depths are less than 1 foot, areas of 100-year flooding where the contributing drainage area is less than 1 square mile, and areas protected from 100-year flood by levees. No base flood elevations or depths are shown within these zones.\n\n\n\n\n\n\n\n\n\nTechnical terms\n\n\nExpand\n\n\n\n\n\n\n\nAcronym\n\n\nLongform\n\n\nDefinition\n\n\n\n\n\n\n\n\nBit\n\n\nAn abbreviation for binary digit, a number that can have only a value of 0 or 1.\n\n\n\n\n\n\nByte\n\n\nA group of bits that can be stored and retrieved as a unit.\n\n\n\n\n\n\nKilobyte\n\n\nA unit of memory representing 1,024 bytes and often designated with the symbol K, as 4Kb or 4 kilobytes. The symbol K is also used to refer to 1,024 words of any specified size.\n\n\n\n\n\n\nHypervisor\n\n\nthe software that runs virtual machines, typically sits between hardware and software (contrasted with docker, which is software only)\n\n\n\n\n\n\nMicro service\n\n\nsoftware is composed of small independent services that communicate over well-defined APIs.\n\n\n\n\nHPC\n\n\nHigh Performance Computing\n\n\n\n\n\n\nPC\n\n\nPersonal Computer\n\n\n\n\n\n\nREST API\n\n\nRepresentational State Transfer Application Programming Interface\n\n\nREST stands for Representational State Transfer. REST defines a set of functions like GET, PUT, DELETE, etc. that clients can use to access server data. Clients and servers exchange data using HTTP.\n\n\n\n\nASCII\n\n\nAmerican Standard Code for Information Interchange\n\n\nA popular standard for the exchange of alphanumeric data.\n\n\n\n\nMatLab\n\n\n\n\nDesktop program that expresses matrix and array mathematics directly\n\n\n\n\nCSV\n\n\ncomma-separated values\n\n\n\n\n\n\nDIM\n\n\nDense Image Matching\n\n\n\n\n\n\nECW\n\n\nEnhanced Compression Wavelet\n\n\n\n\n\n\nJPEG\n\n\noint Photographic Experts Group\n\n\ncommonly used method of lossy compression for digital images\n\n\n\n\nLAS\n\n\n\n\nan industry-standard binary format for storing lidar data\n\n\n\n\nLIDAR\n\n\nLight Detection and Ranging\n\n\nAn aerial survey method that illuminates the target with pulsed laser light and measures the reflected pulses.\n\n\n\n\nMrSID\n\n\n\n\nimage, multiresolution seamless image database\n\n\n\n\nNED\n\n\nNational Elevation Dataset\n\n\n\n\n\n\nPNG\n\n\nPortable Network Graphics\n\n\n\n\n\n\nSIF\n\n\nStandard Interchange Format\n\n\nA commonly used format for the exchange of alphanumeric data.\n\n\n\n\nTIFF\n\n\nTagged Image File Format\n\n\n\n\n\n\nTIN\n\n\nTriangulated Irregular Network\n\n\nA set of non-overlapping triangles developed from irregularly spaced points that is used to represent the facets of a surface.\n\n\n\n\nVPF\n\n\nVector Product Format\n\n\nA standard format, structure, and organization, used by National Imagery and Mapping Agency, for large geographic databases that are based on a georelational data model and are intended for direct use. VPF is designed to be compatible with a wide variety of applications and products. VPF uses tables and indexes that permit direct access by spatial location and thematic content and is designed to be used with any digital geographic data in vector format that can be represented using nodes, edges, and faces. VPF defines the format of data objects, and the georelational data model provides a data organization within which software can manipulate the VPF data objects.\n\n\n\n\nflt\n\n\nfloat grid file\n\n\n\n\n\n\n\n\nAperture\n\n\nwords\n\n\n\n\nAPI\n\n\nApplication Programming interface\n\n\n\n\n\n\nCAD\n\n\nComputer-Aided Design\n\n\n\n\n\n\nCEO\n\n\nChief Executive Officer\n\n\nThe official of a community who has the authority to implement and administer laws, ordinances, and regulations for that community.\n\n\n\n\nCPU\n\n\nCentral Processing Unit\n\n\n\n\n\n\nPO\n\n\nProject Officer\n\n\nA FEMA staff member in the Regional Office or in Headquarters who performs contract monitoring functions, which include providing technical direction to FEMA contractors, monitoring the progress of contractors’ work, and evaluating contractor performance.\n\n\n\n\n\n\nChoropleth Map\n\n\nA map with shaded or hatched areas. (Choro = place and pleth = value.)\n\n\n\n\n\n\ninterferometric\n\n\nTechniques in which electromagnetic waves are superimposed, causing the phenomenon of interference in order to extract information.\n\n\n\n\n\n\nphotogrammetry\n\n\nThe science of making measurements from photographs, especially for recovering the exact positions of surface points.\n\n\n\n\nADCP\n\n\nAcoustic Doppler Current Profiler\n\n\n\n\n\n\nADP\n\n\nAcoustic Doppler Profiler\n\n\n\n\n\n\nCFD\n\n\nComputational Fluid Dynamics\n\n\n\n\n\n\nCFL\n\n\nCourant-Friedrichs-Lewy\n\n\n\n\n\n\nIFSAR\n\n\nInterferometric Synthetic Aperture Radar\n\n\n\n\n\n\nLIDAR System\n\n\nLight Detection and Ranging System\n\n\nAn airborne laser system, flown aboard rotary or fixed-wing aircraft, that is used to acquire x, y, and z coordinates of terrain and terrain features that are both manmade and naturally occurring. LIDAR systems consist of an airborne Global Positioning System with attendant base station(s), Inertial Measuring Unit, and light-emitting scanning laser.\n\n\n\n\n\n\n\n\n\nAlphabetical\n\nSearchableCatagorically grouped",
    "crumbs": [
      "Atlas",
      "Glossary"
    ]
  },
  {
    "objectID": "healing.html#references",
    "href": "healing.html#references",
    "title": "Healing",
    "section": "References",
    "text": "References\n[[pointcloud.qmd]] pointcloud.qmd [[terrain.qmd]] terrain.qmd"
  },
  {
    "objectID": "healing.html#bridge-healing-prototype-and-cookbook",
    "href": "healing.html#bridge-healing-prototype-and-cookbook",
    "title": "Healing",
    "section": "Bridge healing prototype and cookbook",
    "text": "Bridge healing prototype and cookbook\n\nFrom: The FIM team including Jim Coll, Laura Keys, et al.\nData processed from a LIDAR flight typically includes information such as x, y, z, t, and values for reflectance intensities. To derive groups (such as ”building” or “shrub”, post processing needs to occur and can be subject to the classification pipeline that the vendor applies. In a healing workflow, we can either take the entire point cloud as it was provided, place some arbitrary boundaries to limit processing, and heal everything we find inside those bounds, or we can take a more nuanced approach to establishing those bounds and heal the surface in with a bit more discretion. I like to distinguish those two approaches as asset-less and asset’d (or rarely, seeded). If the point cloud is well classified and there’s not a strong need to associate each and every point in the point cloud to a particular entity, an asset-less approach removes a lot of the extra effort and massaging needed to arrive at the healed surface. However, if the point cloud is not well classified, if the features you are trying to heal are novel, or if the end application might capitalize on feature level summaries, then an asset guided approach to healing may be more desirable/appropriate.\nIn the case of bridges, we have an interesting blend of those two. Point cloud classification can be rough but as the 3DEP program and vender capabilities have matured, the accuracy of ”well classified” point clouds has increased quite substantially from earlier collections. As we’ve also discovered, a synchronized database of bridges is rare and also fraught with shortcomings. From representing pairs of bridges with a single point to bridges we don’t care to process (e.g. railroads, culverts) to poorly placed linework, using the bounding boxes from OpenStreetMaps (OSM) or the National Bridge Inventory (NBI) makes asset-enabled healing far less programmatic than the head work might suggest. We’ve explored pieces of the workflow needed to conflate and align the different data sources, but since we are already using the OSM bridge lines as a starting point for the HAND based healing (initial PR here)\n“The Point Data Abstraction Library (PDAL) is a C++ library for translating and manipulating point cloud data”. Much like the other giants of geospatial processing, and due perhaps to its orientation in relation to ”big data”, PDAL has really light-posted the pathways towards cloud native and optimized data manipulations. By that, I refer to the ability to perform partial read operations via http protocol, as opposed to more traditional computing pathways which require the entire dataset to be read into memory or otherwise serially streamed. It’s worth noting however, that just because there are cloud native paths to stream that data into the compute environment, does not mean that it’s the fastest or even cheapest way to accomplish your end goal. If you plan on hitting the data more than a few times or if the dataset is static, you may be able to read more data in faster by performing the one-time download of the massive dataset than you would by taking tiny subsets over and over each time you’d like to run your analysis. Because this is so hardware and geographic dataset specific, timings have been hard to track down but using a docker’d image on a Intel Core i5-9600K CPU @ 3.7 GHz with 32 gigs of RAM available and light background use, the following timings were found:\n\nJim is slow (Timings here)\nThe rest of this notebook is based on a loose workflow that was heavily aided by GUI GIS (QGIS) workflows to visualize outputs and construct GIF’s. If the docker image gives you too much trouble, installation in OSGeo is relativly straightforward and more recent versions may be as simple as a pip install pdal if you’ve pulled the right image."
  },
  {
    "objectID": "howto_site.html",
    "href": "howto_site.html",
    "title": "How to use this site",
    "section": "",
    "text": "In an effort to streamline both my time and efforts and to reduce the excessive repetition, unnecessary technical restrictions, absurd pyramid-scheme pricing, and the ecosystems of bloat and faff that typically accompanies a “web-based learning management system”, class material (including dates and assignments) but excluding submission portals and grade administrivia, will be hosted through these webpages. Lectures will typically not be recorded but slides will be linked from the main class page. Labs will have dedicated pages under the respective classes. Each lab will start off with an objectives section that outlines the analyses or skills the lab aims to teach. Following this, the requisite data and question document is attached. Page level navigation can be accomplished with the menu table of contents at the top-left of the page.\n\n\nThe remainder of this page will visually call out the different elements I tend to deploy across this site. Sections (major parts of labs, navigation headings) will have a level 2 heading, and will look like so:\n\n\n\nMajor steps you take in an analysis will generally have a level 3 heading, and look like this:\n\n\nSubsteps or other milestones are in a level 4 heading like so:\n\n\nWriting for technical documentation is awkward and unpleasant, but I’ll try and make it less so. If you need to click or select something I attempt to Bold them. This includes toolbar and options clicks. If I want you to write something out explicitly I’ll “typically quotation it”. If I’m directing you to an option to chose or an intermediate click in some sort of settings or sub tabs I’ll italicize it.\n\nNotes (formatted like this) typically serve as asides, parentheticals, or image credits where appropriate.\n\n{{% notice style=question color=“fuchsia” title=“There may be pirates” %}} Questions in the word documents you have to answer are also repeated in the tutorials like so. {{% /notice %}}\n\n\n\n\n\nThe lab is a huge part of my teaching. Lectures can take you pretty far, but what you can accomplish in your own time is what you will be judged for. I have attempted to make these labs as consistent, organized, and followable as I can; but inevitably something would have updated with a breaking change, I will have missed something, or you will encounter an error. Fortunately for you, you happen to be sitting in front of one of the most powerful tools the world has ever assembled. I refer of course, to the magic rectangle you are currently staring at, and our overlords of Google, who make the internet searchable. Being able to effectively Google is critical to success, so if you ask me a question regarding what went wrong with your analysis, the conversation will generally play out like so:\n\nYou: I found an error, what did I do wrong?\nMe: What was the error?\nYou: It was XYZ…\nMe: What did you Google?\nYou: Not XYZ…\nMe: What happens when you Google XYZ?\nYou: I find the 200,000 other people who have had this same issue along with a solution… and the 800,000 AI generated regurgitations of those solutions… and documentation 12 years ago on the first page… and then documentation from 2 weeks ago that is now obsolete once you limit results…\n\nWhen in doubt, you can always try to execute something and then reverse engineer your way back. These are just PC’s and nothing we’ll do is mission critical. The worst thing you’ll do is cause the computer to BSOD (Blue Screen of Death), and although those aren’t great, they are not the end of the world and your work is backed up. It is backed up, isn’t it?\n\n\n\n\n\n\nAdvanced debugging tips\n\n\n\n\n\nThe following is my general checklist to follow as I debug:\n\nDid turning it off and on again fix it?\n\nWhat version are you running?\n\nDoes the file actually exist and do you know what/where are you running?\nWhat assumptions have you made?\nIs it reproducible?\n\nMost of the time that gets me past my problem, and if the error message is less than helpful and there are no relevant Google results, you now have all the pieces needed to ask for help in public.\n\n\n\n\n\n\nI dislike touching on this subject as I find it a bit counterproductive to the goal behind attending university, but it seems warranted given that is likely how you’ve ended up on this page :) There are many things I love about teaching, but gatekeeping is not one of them. A 4 year degree is a great signpost on your resume that says you are a well rounded and capable individual with the requisite background and theoretical foundation necessary to excel in your chosen field (and who has hopefully learned how to play well in the sandbox). However, I dislike that my say so (in the form of a pass or fail grade) can act as that barrier to your perceived success or failure. Even more so, a grade is one of the last things you as a learner should be concerned about. If you find yourself chasing points, in my mind you’ve playing the wrong game. You should be concerned about whether you understand the steps and rational behind the material, and how well you are able to apply that understanding to new situations. Although I have no wish to contribute to grade inflation via grade leniency, the bulk of the grading on these labs are as positivistic as math in that there is a right and wrong answer, and I do not grade on a curve. I will of course push you to do your best and go the extra step, but many of the labs are cut and dry when it comes to assigning a points value. If you do well, you get an A, and I’d rather not explore the alternative end of what that range is.\nFinally, although it should be obvious at this point in your academic careers, under no circumstances is cheating tolerated. This includes but is not limited to plagiarism in papers; I’ve got a whole page dedicated to how to cite sources here, using previous students’ course material in quizzes and tests, and submitting others’ work, (including trained word vomits from learning algorithms) as your own. Not only does this detract from the overall integrity of the department and the school (the lesser of the evils in my mind), but cheating in these classes sets you up for disappointment and misery further down the line. You’ll have failed to adequately learn foundational concepts which lead to the more advanced skills that the world (and employers) are looking for and which all subsequent material will stand upon. In short, it is counterproductive to the very concept of attending college in the first place and sets you up for a world of friction down the line (credit to Dr. Brian Harvey as the inspiration for this framing). I am always available through email, slack, and office hours (or by appointment) and am here to help, so don’t do yourselves the disservice of cheating through what should otherwise be interesting and simulating material."
  },
  {
    "objectID": "howto_site.html#layouts",
    "href": "howto_site.html#layouts",
    "title": "How to use this site",
    "section": "",
    "text": "The remainder of this page will visually call out the different elements I tend to deploy across this site. Sections (major parts of labs, navigation headings) will have a level 2 heading, and will look like so:"
  },
  {
    "objectID": "howto_site.html#independent-unit-analysis",
    "href": "howto_site.html#independent-unit-analysis",
    "title": "How to use this site",
    "section": "",
    "text": "Major steps you take in an analysis will generally have a level 3 heading, and look like this:\n\n\nSubsteps or other milestones are in a level 4 heading like so:\n\n\nWriting for technical documentation is awkward and unpleasant, but I’ll try and make it less so. If you need to click or select something I attempt to Bold them. This includes toolbar and options clicks. If I want you to write something out explicitly I’ll “typically quotation it”. If I’m directing you to an option to chose or an intermediate click in some sort of settings or sub tabs I’ll italicize it.\n\nNotes (formatted like this) typically serve as asides, parentheticals, or image credits where appropriate.\n\n{{% notice style=question color=“fuchsia” title=“There may be pirates” %}} Questions in the word documents you have to answer are also repeated in the tutorials like so. {{% /notice %}}"
  },
  {
    "objectID": "howto_site.html#general-lab-syllabus-help",
    "href": "howto_site.html#general-lab-syllabus-help",
    "title": "How to use this site",
    "section": "",
    "text": "The lab is a huge part of my teaching. Lectures can take you pretty far, but what you can accomplish in your own time is what you will be judged for. I have attempted to make these labs as consistent, organized, and followable as I can; but inevitably something would have updated with a breaking change, I will have missed something, or you will encounter an error. Fortunately for you, you happen to be sitting in front of one of the most powerful tools the world has ever assembled. I refer of course, to the magic rectangle you are currently staring at, and our overlords of Google, who make the internet searchable. Being able to effectively Google is critical to success, so if you ask me a question regarding what went wrong with your analysis, the conversation will generally play out like so:\n\nYou: I found an error, what did I do wrong?\nMe: What was the error?\nYou: It was XYZ…\nMe: What did you Google?\nYou: Not XYZ…\nMe: What happens when you Google XYZ?\nYou: I find the 200,000 other people who have had this same issue along with a solution… and the 800,000 AI generated regurgitations of those solutions… and documentation 12 years ago on the first page… and then documentation from 2 weeks ago that is now obsolete once you limit results…\n\nWhen in doubt, you can always try to execute something and then reverse engineer your way back. These are just PC’s and nothing we’ll do is mission critical. The worst thing you’ll do is cause the computer to BSOD (Blue Screen of Death), and although those aren’t great, they are not the end of the world and your work is backed up. It is backed up, isn’t it?\n\n\n\n\n\n\nAdvanced debugging tips\n\n\n\n\n\nThe following is my general checklist to follow as I debug:\n\nDid turning it off and on again fix it?\n\nWhat version are you running?\n\nDoes the file actually exist and do you know what/where are you running?\nWhat assumptions have you made?\nIs it reproducible?\n\nMost of the time that gets me past my problem, and if the error message is less than helpful and there are no relevant Google results, you now have all the pieces needed to ask for help in public."
  },
  {
    "objectID": "howto_site.html#grading",
    "href": "howto_site.html#grading",
    "title": "How to use this site",
    "section": "",
    "text": "I dislike touching on this subject as I find it a bit counterproductive to the goal behind attending university, but it seems warranted given that is likely how you’ve ended up on this page :) There are many things I love about teaching, but gatekeeping is not one of them. A 4 year degree is a great signpost on your resume that says you are a well rounded and capable individual with the requisite background and theoretical foundation necessary to excel in your chosen field (and who has hopefully learned how to play well in the sandbox). However, I dislike that my say so (in the form of a pass or fail grade) can act as that barrier to your perceived success or failure. Even more so, a grade is one of the last things you as a learner should be concerned about. If you find yourself chasing points, in my mind you’ve playing the wrong game. You should be concerned about whether you understand the steps and rational behind the material, and how well you are able to apply that understanding to new situations. Although I have no wish to contribute to grade inflation via grade leniency, the bulk of the grading on these labs are as positivistic as math in that there is a right and wrong answer, and I do not grade on a curve. I will of course push you to do your best and go the extra step, but many of the labs are cut and dry when it comes to assigning a points value. If you do well, you get an A, and I’d rather not explore the alternative end of what that range is.\nFinally, although it should be obvious at this point in your academic careers, under no circumstances is cheating tolerated. This includes but is not limited to plagiarism in papers; I’ve got a whole page dedicated to how to cite sources here, using previous students’ course material in quizzes and tests, and submitting others’ work, (including trained word vomits from learning algorithms) as your own. Not only does this detract from the overall integrity of the department and the school (the lesser of the evils in my mind), but cheating in these classes sets you up for disappointment and misery further down the line. You’ll have failed to adequately learn foundational concepts which lead to the more advanced skills that the world (and employers) are looking for and which all subsequent material will stand upon. In short, it is counterproductive to the very concept of attending college in the first place and sets you up for a world of friction down the line (credit to Dr. Brian Harvey as the inspiration for this framing). I am always available through email, slack, and office hours (or by appointment) and am here to help, so don’t do yourselves the disservice of cheating through what should otherwise be interesting and simulating material."
  },
  {
    "objectID": "hydraulics.html",
    "href": "hydraulics.html",
    "title": "hydraulics",
    "section": "",
    "text": "All models are wrong but some are useful -George Box (Box 1976)\n\nThe study of modeling the flow of water over a surface is hydraulics. That particular topic (fluid dynamics), is a complex one that probes deep gaps in our collective knowledge of physical laws governing reality. the scale needed to represent the system to a degree of accuracy required, and the friction we experience as we carry assumptions and simplifications from conception to realization. As we make assumptions about the nature of nature, we can reasonably make simplifications to our system. In fluid sciences, one of the most powerful ones we can make is the inclusion or execution of particular forces, as the series of kinematic, diffusive, and dynamic wave model equations might exemplify, or in the reduction of the problems spatial dimensionality, as would be demonstrated by modeling the same domain using three-dimensional computational fluid dynamics solvers like TELEMAC-3D and one-dimensional hydraulic solvers like HEC-RAS 1D. These simplifications, sometimes referred to as “low complexity” models, are nearly universally favored over more the more complex representations of the world due to their conceptual and computational efficiency. While these low dimensional models have their uses, the real world is not as rigid and some knowledge is intrinsically lost in the simplification. This phenomenon is particularly present in the field of river hydraulics and open channel flow which was, and in may ways still is, dominated by one-dimensional models and measurement techniques. One of the assumptions made in these one-dimensional models is that all flow passes perpendicular through a given transect (cross section of a river). These 1-dimensional models have been used since the inception of computational fluid flow and have been widely adopted across public, private, and academic sectors.  As an example, virtually all FEMA 100-year flood inundation maps are created using HEC-RAS 1D flow solver, a publicly available 1-dimensional flow solver from the U.S. Army Corp of Engineers.\nAlthough these 1-dimensional models have seen widespread acceptance and are used in scientific, engineering design, and operational capacities, they require extensive knowledge and judicious use of modeler discretion in order to provide serviceable results. It is also not uncommon to find that model results that may be realistic in one transect, may be off by several feet in another. Post-event analysis of such failures universally point to mis-parameterization of the model or a violation of the models assumptions. Additionally, this type of modeling, while widely applicable to a single reach, is wholly unsuited to widespread applications in flooding. As seen in Figure 1, when the application in question requires the inclusion of more than one reach, the workflow and system that is needed to appropriately addresses the complexities of that merger grows exponentially.\n\nGiven that and the state of advancement computers continue to experience, it seems rather a rather aimless exercise to devise and execute a convoluted series of steps to couple model. At that level of complexity, is it not simply better to increase the dimensionality of the problem and present a clearer path towards explainability, potential gains in accuracy, at the “cost” of computation time?\n\n\n\n\n\nReferences\n\nBox, George E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356): 791–99. https://doi.org/10.1080/01621459.1976.10480949.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "hydraulics"
    ]
  },
  {
    "objectID": "hydrofab_h_f.html#section",
    "href": "hydrofab_h_f.html#section",
    "title": "So long as you get the GIS&T of it",
    "section": "",
    "text": "NBI Hydrolocation Considerations\n\nGoals:\n\nOrient ourselves to the National Bridge Inventory (NBI)\nDocument the conflation process applied to create hydrolocations\nProvide database\n\nOutcomes:\n\nA better understanding of the data and processing algorithm applied in the creation of a national database\nA file useful for hydrofabric network creation and OSM/NBI ID conflation workflows\n\n\n\n\n\n\nFor: Background images from:\n\n\nOWP presentation template background: ./_Water_color.png or ./_Water_bw.jpg - “cover”\n\n\nMy own efforts from Water PNGs by Vecteezy, https://www.vecteezy.com/members/ahasanaraakter: _vecteezy_ai-generated-water-wave-splash.png / _vecteezy_ai-generated-water-wave-splash_bw.png / _vecteezy_ai-generated-water-wave-splash_bw_blur.png - “contain”"
  },
  {
    "objectID": "hydrofab_h_f.html#presentation-navigation-tips",
    "href": "hydrofab_h_f.html#presentation-navigation-tips",
    "title": "So long as you get the GIS&T of it",
    "section": "Presentation Navigation Tips",
    "text": "Presentation Navigation Tips\n\n\n\nSlide layout : This deck has one axis, use any key to advance.\n: This deck is narrative-oriented\n\nItems linked/bordered in green are cited in the tooltip on hover.\nItems linked/bordered in blue are hyperlinked to relevant resources.\n\n Photos are Allowed |  Questions are Encouraged\n: ~5 minutes | Last updated:08/25/2025 21:03:13\n! PLEASE !\nInterrupt me and ask questions or clarifications.\nI’m here to talk with you now, not to these slides.\n\n\nControl tips\n\nMy preferred (FOSS) flavor of slidedecks, revealjs, has intuitive but none the less unconventional PowerPoint presentation controls:\n\nSlides dynamically resize to use the entirety of the browser window, but you can still fullscreen with F.\n\nThis slide has a red border indicating the content extent.\n\nSlide navigation is mode dependent. If there are vertical slides, press space, N, or the down arrow key, not the right arrow to advance slides\nPress M to open to the menu, Press O for the slide deck overview, Press B to black out the presentation screen, Press S for a speaker view.\nYou can use the chalkboard to freemouse/touchpad draw.\nSlides should render as designed1 but you can press Alt/Opt + click on the slide to zoom in. Increase text size with Alt/Opt + +, Alt/Opt + - to decrease, and Alt/Opt + 0 reset to the default scale.\nPress C to declare victory and head home.2\n\n\n\n\n\n\n\n    \n\nThis presentation\n\n\n\n\n\n\n\n\nSpeaker notes\n\ntested in an updated version of google chromeA favored quote from one of my giants: Dr. David Maidment"
  },
  {
    "objectID": "hydrofab_h_f.html#what-is-nbi",
    "href": "hydrofab_h_f.html#what-is-nbi",
    "title": "So long as you get the GIS&T of it",
    "section": "What is NBI",
    "text": "What is NBI\n\nSpeaker notes"
  },
  {
    "objectID": "hydrofab_h_f.html#nbi-field-descriptions",
    "href": "hydrofab_h_f.html#nbi-field-descriptions",
    "title": "So long as you get the GIS&T of it",
    "section": "NBI field descriptions",
    "text": "NBI field descriptions\n\nSpeaker notes"
  },
  {
    "objectID": "hydrofab_h_f.html#hydrolocations",
    "href": "hydrofab_h_f.html#hydrolocations",
    "title": "So long as you get the GIS&T of it",
    "section": "Hydrolocations",
    "text": "Hydrolocations\nHydrolocations store the source, identifier, and index of a critical location Points of Interest (POIs) aggregate co-located hydrolocations to singular network locations\n\nSpeaker notes"
  },
  {
    "objectID": "hydrofab_h_f.html#conflation-method",
    "href": "hydrofab_h_f.html#conflation-method",
    "title": "So long as you get the GIS&T of it",
    "section": "Conflation method",
    "text": "Conflation method\n\n\n\n\n\n\nSpeaker notes"
  },
  {
    "objectID": "hydrofab_h_f.html#select-cases",
    "href": "hydrofab_h_f.html#select-cases",
    "title": "So long as you get the GIS&T of it",
    "section": "Select cases",
    "text": "Select cases\n\nSpeaker notes"
  },
  {
    "objectID": "hydrofab_h_f.html#reprocessing",
    "href": "hydrofab_h_f.html#reprocessing",
    "title": "So long as you get the GIS&T of it",
    "section": "Reprocessing",
    "text": "Reprocessing\n\nSpeaker notes"
  },
  {
    "objectID": "hydrofab_h_f.html#concluding-reconmendations",
    "href": "hydrofab_h_f.html#concluding-reconmendations",
    "title": "So long as you get the GIS&T of it",
    "section": "Concluding reconmendations",
    "text": "Concluding reconmendations\n\nSpeaker notes"
  },
  {
    "objectID": "hydrofab_h_f.html#whats-next",
    "href": "hydrofab_h_f.html#whats-next",
    "title": "So long as you get the GIS&T of it",
    "section": "What’s next?",
    "text": "What’s next?\n\n\n\nOutcomes and Takeaways:\n\nOrient ourselves to the NBI database\n\nNext Steps:\n\nBuild your own!\n\n\n\n\n\n\n\n\n\n    \n\nThis presentation"
  },
  {
    "objectID": "hydrology.html",
    "href": "hydrology.html",
    "title": "Hydrology",
    "section": "",
    "text": "Water Cycle from (“USGS_WaterCycle_English_ONLINE_20230302.png (114007800)” n.d.)\nWater is so essential to life that it’s mere presence is a primary indication as to whether a planet can support life. One of earths most precious resources, the magnitude of its presence or absence can drastically alter the landscape and the choices made therein. Because it is a resource, there is a natural tendency for humans to manage it. This is accomplished via infrastructure (damming of rivers, irrigation techniques, construction of aqueducts and such), technology (weather forecasting, hydrologic modeling), and policy (water rights and environmental protections). Additionally, like many resources there is an intrinsic need to quantify its presence and reserves in order to best manage it. This effort is stymied by the sheer quantity of water on the planet, the ease at which it crosses physical and political boundaries, and its communal nature.\nThe study of water is known as hydrology. The study of modeling the flow of water over a surface is hydraulics. Together, these two fields represent the state of human knowledge over the domain of water. One of the most fundamental equations in hydrology, and a primary means of quantifying the amount of water in a watershed, is the water mass balance equation. This is build off the principle of conservation of mass, and can be stated as: precipitation into a watershed must equal the sum of the discharge out, the evapotranspiration, and the change in storage. Mathematically, this is expressed as: P = Q + eT + \\vartriangle S Though conceptually simple, this equation summarizes a plethora of processes and assumptions. Furthermore, the measurements of these parameters are generally taken in an indirect manner. For example, rainfall amounts are derived from the reflectivity of a radar pulse, as opposed to measuring the actual volume of water that falls on an area. Additionally, the magnitude of these fluxes and the associated spatio-temporal scales of these variables can vary across more than 6 orders of magnitude (groundwater flow can be measured in centuries, precipitation intensities vary on the scales of minutes, and discharge is generally measured as flow past a point per second.\nThe intense variation of the geographic context of these variables adds additional uncertainty to the already significant uncertainties present in the measurement of these variables, and makes the accurate modeling of these systems tenuous at best. This complexity and the lack of interconnectedness across the disparate sciences that touch hydrology has resulted in a failure to “close the water balance” across the domain. This means that the conservation of mass, while applied to individual components or models, is not applied to the system and therefore the potential for errors compound. If we are to accurately manage this resource, we must first close the water balance. It is against this background that my broader research agenda sits.",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Hydrology"
    ]
  },
  {
    "objectID": "hydrology.html#precipitation",
    "href": "hydrology.html#precipitation",
    "title": "Hydrology",
    "section": "Precipitation",
    "text": "Precipitation",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Hydrology"
    ]
  },
  {
    "objectID": "hydrology.html#evapotranspiration",
    "href": "hydrology.html#evapotranspiration",
    "title": "Hydrology",
    "section": "Evapotranspiration",
    "text": "Evapotranspiration",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Hydrology"
    ]
  },
  {
    "objectID": "hydrology.html#runoff",
    "href": "hydrology.html#runoff",
    "title": "Hydrology",
    "section": "Runoff",
    "text": "Runoff",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Hydrology"
    ]
  },
  {
    "objectID": "hydrology.html#river-flow",
    "href": "hydrology.html#river-flow",
    "title": "Hydrology",
    "section": "River flow",
    "text": "River flow",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Hydrology"
    ]
  },
  {
    "objectID": "hydrology.html#groundwater",
    "href": "hydrology.html#groundwater",
    "title": "Hydrology",
    "section": "Groundwater",
    "text": "Groundwater",
    "crumbs": [
      "Atlas",
      "Earth Science",
      "Hydrology"
    ]
  },
  {
    "objectID": "install_GIS.html",
    "href": "install_GIS.html",
    "title": "Installing GIS",
    "section": "",
    "text": "GISoftware comes in many forms and flavors, and one of the largest hurdles a new user might experience is actually getting an application to work. However, repetition and “practice like you play” would suggest that using GIS (or really any skill you are trying to pick up) at least twice a week greatly improves the time to learn, and the easiest way to do so is to have that tool readily available to you. Here I’ll walk through the ways to install and set up some of the most common software tools I use and my workflow. This page is primarily for local installs, but when you want to get fancier and can leave the GUI driven world, computing through docker is how I run most of my larger workflows. See how I build my PC for parts lists and how I set up my physical space.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing GIS"
    ]
  },
  {
    "objectID": "install_GIS.html#google-earth-pro",
    "href": "install_GIS.html#google-earth-pro",
    "title": "Installing GIS",
    "section": "Google Earth Pro",
    "text": "Google Earth Pro\nThe Google Earth pro landing page walks through a few of the different utilities you might want to install. If you are at this stage,the Desktop version is the one we are after. All you need to do to get started is to download and run the executable.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing GIS"
    ]
  },
  {
    "objectID": "install_GIS.html#google-earth-engine",
    "href": "install_GIS.html#google-earth-engine",
    "title": "Installing GIS",
    "section": "Google Earth Engine",
    "text": "Google Earth Engine\nThis cloud based system requires only a Google account, sign up for it at https://developers.google.com/earth-engine/guides/access, and because this is an entire system accessed remotely though your browser, there’s nothing to install other than that browser: Google Chrome",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing GIS"
    ]
  },
  {
    "objectID": "install_GIS.html#arcmaparcpro",
    "href": "install_GIS.html#arcmaparcpro",
    "title": "Installing GIS",
    "section": "ArcMap/ArcPro",
    "text": "ArcMap/ArcPro\nThe ESRI suite of products tends to be a demanding set of programs, so you should have a fairly modern computer if you want to install them for yourself. The ArcGIS system requirements can be found here. I also recommend at least 64 gigs of free hard drive space at minimum. In addition to the hardware requirements, ArcGIS also requires the Windows operating system. Below find installation instructions for different platforms.\n\n\n\n\n\n\nNote\n\n\n\nThe Virtual option is by far the easiest option if you are a recently virtualized member of the KU community. The QGIS virtual option is what I use when I’m not wed to ArcMap.\n\n\n\nWindowsMac & Linux (via VirtualBox)\n\n\n\nBefore you install ArcGIS for Desktop\n\nTo install ArcMap on windows, check the system requirements to make sure your computer has the hardware and software required. You should also close any other open programs or windows.\n\n\n\nActivate your authorization code\n\nVisit https://wwww.esri.com/educationedition to begin the process of activating and downloading your ArcGIS for Desktop Student Trial software.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you need an authorization code, you can use the free trial ESRI provides by sighing up at https://www.esri.com/en-us/arcgis/trial.\n\n\n\nCreate or Log in using your ESRI account as necessary. You will need to use this log in again so remember what you enter here.\nEnter the authorization code and click Activate ArcGIS.\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you received the ArcGIS for Desktop software from your instructor or license administrator, or will be installing from a network server, proceed to step 10.\n\n\n\nClick the button for the ArcGIS for Desktop software version being activated.\n\n\n\nDownload and install ArcGIS for Desktop Student Trial\n\nIf necessary, download the ArcGIS Uninstall Utility and uninstall previous versions of ArcGIS Desktop or Server. The software cannot be installed on a computer that has a previous version of ArcGIS for Desktop or ArcGIS for Server installed. It’s OK if the computer has ArcGIS Explorer installed.\nIf necessary, install the Microsoft .NET Framework (version 3.5 Service Pack 1 or higher). However, if you have a current version of Windows installed it is highly likely this is already installed. If you are unsure if you have the .NetFramework installed, you can download a .net Version detector from http://www.asoft.be/prod_netver.html.\nDetermine the location for the ArcGIS for Desktop software you wish to install and click the Download button. You can also download tutorial Data, if desired.\nDouble-click ArcGIS_Desktop_10XXXXXXX.exe (Depending on the version you selected, the file extensions may be different) to extract the installation files.\nLocate and run Setup.exe to install ArcGIS for Desktop. The “Complete” installation is recommended.\nAfter the files are installed, the Authorization Wizard will open and prompt you to choose a product to authorize; select “ArcGIS Desktop Advanced (Single Use)” and click continue.\nThe Authorization Wizard will prompt you for an authorization code; enter your activated code. Follow the prompts and the software will authorize and be ready for use. Note: leave the default option for the software extensions selected; they will be authorized automatically.\nFinally, restart your computer and try to open ArcMap. If the installation was successful, you should be greeted with the ArcMap template window.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the installation was unsuccessful, you may need to double check your license authorization. Search for the “ArcGIS Administrator” program which was installed alongside ArcMap, click on Desktop, select the Advanced (ArcInfo) Single Use radio button, and then the Authorize Now… button. Follow the prompts and enter the necessary info.\n\n\n\n\n\n\nIf you are on a mac os or Linux distribution, there are two ways to install ArcMap. You may use Boot camp to dual boot windows, or you can use a virtualization software to launch a windows installation. In either case you will need a windows key to fully authorize the software (which generally entails some form of credit card). For more information see this ESRI post. Here I will walk you through the VirtualBox path.\n\nInstalling VirtualBox\n\nYou should start by closing any other programs or windows. This explicit close step is important to a successful installation.\nDownload VirtualBox here.\nDouble click the VirtualBox exe and click through the guided installation.\n\n\n\nInstalling fake Windows\n\nWindows provides “evaluation” virtual environments of Windows 11 for development, available here. Download the VirtualBox version, and unzip it.\nIn Oracle VM, go to File &gt; Import Virtual Appliance and point to the Windows ova. Click next.\nMost of the defaults are fine, but I like to create separate folders of each VM I import. To do this, use the drop down underneath the table to select “other…” and create a new folder called Windows and click on the folder once to select.\nYou will also most certainly want to increase the amount of RAM available: Conversion Chart\n\n\n\nConnecting your hard drive\n\nFinally, lets set up the instance. Right click on the newly added machine and go to settings. Most of the time these are sensible defaults, although there are a few things we can tweak.\n\n\nunder the System dialog, feel free to increase the Base Memory to a reasonable amount, bearing in mind that the more you allocate to the vm, the less you have available on the host. Feel free to explore the other options but don’t change them unless you know what you’re doing or can handle a crash or two :)\nUnder Shared Folders, create a new folder (the icon with a plus)\nPoint to a shared folder on your host machine (mine is called “hold” here)\nSelect the Auto-mount option\nChange the Mount point to something sensible (like “”)\n\n\n\n\nf28ea5cdadec67c2d0c5c02bf1879da9.png\n\n\n\n\n\n\n\n\nVirtualBox performance tips\n\n\n\n\n\n\nWhen launching this environment (and really any virtualization in general), it can be painfully slow to load. You might also see a few error messages that seem to sit there for a while and do nothing (An older thread but these errors appear to be related to issues flagged here). Be patient and it should load and behave properly, take the dog for a walk.\nIf you encounter flickering screens when using the new PC, under the Display settings, make sure 3D acceleration is unchecked.\n\n\n\n\n\n\nInstalling ArcMap\n\nTo launch Windows, click the imported Windows appliance and click start. You may close the “Auto-capture” alerts tab on the right.\n\nNavigate to This PC in the windows explorer and click on the share folder button on the VirtualBox window as needed to make sure the expected folders appear. Now that you have a version of Windows installed, you can follow the instructions under the Windows tab.\n\n\n\n\n\n\n\nNote\n\n\n\nA few last notes:\n* This is a bare bones install of Windows, you might want some of the utilities I call out at the bottom of my “working” page.\n* As you may have noticed, when you downloaded the VM image, there was an expiration date ~90 days from the date you downloaded it. You will either need to activate this version of windows, or delete and reinstall as needed.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing GIS"
    ]
  },
  {
    "objectID": "install_GIS.html#qgis",
    "href": "install_GIS.html#qgis",
    "title": "Installing GIS",
    "section": "QGIS",
    "text": "QGIS\n“QGIS is a user friendly Open Source Geographic Information System (GIS) licensed under the GNU General Public License. QGIS is an official project of the Open Source Geospatial Foundation (OSGeo). It runs on Linux, Unix, Mac OSX, Windows and Android and supports numerous vector, raster, and database formats and functionalities.” When I first encountered QGIS many moons ago, it used to be a cumbersome, difficult tool to use compared to ArcMap. However, in just a few short years QGIS has made massive improvements and can even stand toe to toe with ESRI in many respects but is entirely free and open source. Additionally, it can be even easier to install than ArcMap, and if you go with the virtual installation route provided by OSGeo below you will have a platform agnostic installation of many of the GIS tools and programs that you could ever need (Python, R, PostGIS, GRASS, GDAL, ect.) As such, I hearty recommend the virtual option. Regardless of the version chosen system requirements are minimal (1 GB RAM (2 GB are better for trying Java based applications), 1GHz i386 or amd64 compatible CPU) and most modern computers will meet your needs.\n\nOS independentVirtually\n\n\nThe installation of QGIS is far less involved than it’s ESRI counterparts. The QGIS website has downloads separated by operating system here, simply select your operating system. New users are recommended to install the standalone installer of the latest release. All you need to do to get started is to download and run the executable.\n\n\nSee the OSGeo quickstart docs for a more verbose but slightly outdated set of instructions should this set of steps not do it for you.\n\nInstalling VirtualBox\n\nYou should start by closing any other programs or windows. This explicit close step is important to a successful installation.\nDownload VirtualBox here.\nDouble click the VirtualBox.exe and click through the guided installation.\n\n\n\nInstalling OSGeoLive\n\nDownload and unzip the latest version of OSGeoLive, you probably want the .amd64.vmdk.7z version. Save that unzipped folder in your VirtualBox VMs folder.\nIn Oracle VM, Click on Tools, New.\nName your new machine (I call mine FOSSGIS).\nYou can leave the Machine Folder dialog on VirtualBox VMs.\nChange your Type to Linux and your Version to Ubuntu.\nYou will most certainly want to increase your memory (ram) beyond the half gig default.\nSelect the “Use an existing virtual hard drive disk file” and point to the vmdk file. Finally, click Create.\n\n\n\n\n./_FOSSGIS.png\n\n\n\n\nInstalling OSGoeLive (alt)\n\nDownload and unzip the latest version of OSGeoLive, you probably want the osgeolive-15.0-amd64.iso version. Save that unzipped folder in your VirtualBox VMs folder.\nIn Oracle VM, Click on Tools, New.\nName your new machine (I call mine FOSSGIS).\nYou can leave the Machine Folder dialog on VirtualBox VMs.\nChange your Type to Linux and your Version to Ubuntu.\ninstall the vm\nYou will most certainly want to increase your memory (ram) beyond the half gig default.\n\n\n\n\n793313742d0e25deecd66bdc222036de.png\n\n\n\n\n\n\n\n\nVirtualBox performance tips\n\n\n\n\n\n\nWhen launching this environment (and really any virtualization in general), it can be painfully slow to load. You might also see a few error messages that seem to sit there for a while and do nothing as I show above (An older thread but these errors appear to be related to issues flagged here). Be patient and it should load and behave properly, take the dog for a walk.\nIf you encounter flickering screens when using the new PC, under the Display settings, make sure 3D acceleration is unchecked.\n\n\n\n\n\n\nSetting up\n\nFinally, lets set share a folder. Right click on the newly added machine and go to settings.\n\nUnder Shared Folders, create a new folder (the icon with a plus)\n\nPoint to a shared folder on your host machine (mine is called “FOSSFlood-master” here)\nThe shared folder is now connected!\nAt the top of the VirtualBox window click on device &gt; Insert guest additions CD, open that folder and run ‘./VBoxLinuxAdditions.run’\nsudo adduser $USER vboxsf\nIf that works it should look like this \n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re on a smaller keyboard you likely don’t have a right-ctrl key, change the host bind key to something else (I use right alt. File &gt; Preferences &gt; Input)\n\n\n\n\n\n\n\nSetting up & updating QGIS\n\nPlugins to install\n\nquickmapservive\nNormal (VHD) fixed\nVBoxSVGA\n\n\n\n\n\n```{bash}\nsudo apt install gnupg software-properties-common\npip3 install ipython ipympl jupyter jupyterlab folium rioxarray leafmap\n```\n\n\nUpdating Python\n```{bash}\npip3 install ipython ipympl jupyter jupyterlab folium rioxarray leafmap\npip3 install --upgrade ipykernel\n\njupyter-notebook --no-browser --ip 0.0.0.0 --port=8069 --allow-root\n```\n\n\nLaunch a notebook\n```{bash}\njupyter-notebook\n```\n\n\nUpdating python (old)\nWhile the above steps will serve you through the intro and intermediate classes, I also use this method almost exclusively for my python needs. Python can be a wonderful tool, but installation and update woes, along with less flexible graphical execution options means it often loses out to R when new geospatial programmers are looking for their first launch point. With this linux (ubuntu) installation, we bypass virtually all these issues (see note below). However, to get the most out of this way of accessing python, we need to set up a few things. Therefore, after setting up your virtual machine for the first time, I suggest you open up a terminal and run the following:\n```{bash}\nsudo apt-get update -y              # updates all sorts of goodies\nsudo apt install notepadqq      # Installs a better text editor (almost notepad++)\nsudo apt install python3-pip            # Installs pip3 for python\nsudo apt-get install proj-bin           # Installs and updates proj library \npip3 install pysimplegui              # Adds gui library for python scripts!\npip3 install seaborn --upgrade        # My favorite python plotting library, because matpltlib syntax is disgusting\npip3 install jupyterlab               # for notebook support\npip3 install Whitebox                 # WhiteboxTools\n\n# Only run if you need to get geopandas 5.1 to run\npip3 install setuptools==39.0.1\npip3 install pyproj==2.6.1\npip3 install pandas==0.23.3\npip3 install geopandas==0.5.1\n```\n\n\n\n\n\n\n\nNote\n\n\n\nWhen I say all, I mean all but the ArcPy library :/ ESRI provides a tiny bit of help in this direction, but it seems as though their objective is to force you into the always on licensing of the new millennium. For what it’s worth, I don’t think I’ve encountered anything written in ArcPy that wasn’t more straightforward to implement yourself, but of course your mileage will vary.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing GIS"
    ]
  },
  {
    "objectID": "install_GIS.html#installing-r",
    "href": "install_GIS.html#installing-r",
    "title": "Installing GIS",
    "section": "Installing R",
    "text": "Installing R\nR is a great tool for geospatial work, and other than the rare package that really doesn’t like working on Windows, has very few hurdles in installation if you are on either the windows or mac side thanks to the efforts of Posit (The “parent” company of R.) In order to get the most out of this ecosystem, it is recommended that you use R through it’s Integrated Development Environment (IDE) called RStudio. While you can use R without this, the synergies between the two make using it a joy and not a pain. Just make a mental note that, just like Python, R is the language, and the tool you use to run that (Notebooks, VSCode, or RStudio) are tools that make interactions and development tasks far more seamless than the single line console access patterns that you’d use if you were trying to use the bare essential environment.\nTo install this, simply navigate to the RStudio desktop website, and\n\nclick the “DOWNLOAD AND INSTALL R” button, and then follow the prompts on screen\nClick the “DOWNLOAD RSTUDIO DESKTOP FOR WINDOWS” button and follow the prompts on screen. \nThe first time you open RStudio, it will ask you where R is installed. You can leave the default radio button selected and it will automatically connect to your (just installed) version of R. \n\n\nNote: If you are feeling adventurous, you can also accomplish this the “harder” way as I outline in “how to install docker”\n\n\nA note on loading packages:\n\n\n\n\n\n\nIf you have issues with package installations\n\n\n\nI really only recommend following this section if you are on a ‘special’ use case like a GFE. If you see this error and you are on a windows computer you own/control, there is not really a reason you shouldn’t be able to run the code as was previously described. I’d remove all traces of R, RTools, and RStudio from your system and then attempt a fresh reinstall of all of them, but RTools in particular.\n\n\nIf you have issues with package installations, and in particular if you are on a GFE and get an error as follows:\n```{r}\n&gt; remotes::install_github(\"&lt;repo_here&gt;\")\nUsing GitHub PAT from the git credential store.\nDownloading GitHub repo &lt;repo_here&gt;@HEAD\nError in utils::download.file(url, path, method = method, quiet = quiet,  : \n  cannot open URL 'https://api.github.com/repos/&lt;repo_here&gt;/tarball/HEAD\n```\nyou can get around this error using the following set of lines:\n```{r}\ninstall.packages(c(\"installr\",\"pkgbuild\"))\n# Restart\ninstallr::install.Rtools(check_r_update = FALSE)\n# Restart\npkgbuild::check_build_tools(debug = TRUE)\n# Restart\ninstall.packages(\"pak\")\n# Restart\npak::pkg_install(\"NOAA-OWP/hydrofabric\")\n```",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing GIS"
    ]
  },
  {
    "objectID": "install_GIS.html#installing-python-on-windows",
    "href": "install_GIS.html#installing-python-on-windows",
    "title": "Installing GIS",
    "section": "Installing Python (on windows)",
    "text": "Installing Python (on windows)\nWindows is a special case wherein python is not easily available or as exposed as it is in linux. The easiest way to use python is to install conda via miniconda (although you might more often be pointed to Anaconda, there are too many snakes on this plane). The difference is that Anaconda comes pre-loaded with libraries that make it a little faster to recreate environments. Given that we are trying to be efficient and raise the state of the art, it’s my recommendation that you install miniconda and build an environment. Even if you end up creating a generic development environment that you reuse, it will keep you reproducible and includes less bloat (not that the overhead of the OS bloat is a larger factor here…)\n\nNote: Again, if you are feeling adventurous, you can also accomplish this the “harder” way as I outline in “how to install docker”\n\n\nQuick tips\n\nlist env: conda env list\nMake an env on the fly: conda create -n myenv myscipy",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing GIS"
    ]
  },
  {
    "objectID": "install_web.html",
    "href": "install_web.html",
    "title": "Installing Web",
    "section": "",
    "text": "The futility of documentation\n\n\n\nLike apparently everything digital these days, 2 years later and most of the material I’ve written here is too out of date to be used as a direct tutorial (like nailing a flying carpet down, the reproducibility crisis might really be a thing). The academic theme has now moved to a quasi-GUI driven deployment system which both makes the site easier to create and deploy, but also strips it of the flexibility that I loved from that older version. I also had a small crisis of content and form and so the bulk of the site (and most of my documentation) is now written in Quarto. That HUGO pattern is far cleaner and more flexible though, and I still reach for HUGO when the site doesn’t need to be data driven in the same way I was looking for with this documentation. So, I’ll keep this page up as some of these tips are still relevant, but as always make sure you defer to the official documentation when learning how to do this.\n\n\nThe creation of this website was spurred by a desire to streamline my digital footprint and more easily facilitate sharing. However, building a website of your own is not as straightforward or accessible as it otherwise could be. New efforts have vastly improved on this, to the point that if you just want a good looking web page that isn’t your GitHub profile, I recommend you start with Notion and see if that will work for you. It’s about as GUI driven a tool as I’ve ever used to set up a a page with the click and move objects, and then publish it to the world by making it public. After that you can use tinyurl to minify your URL to something more memorable like your name. If you find you need to share large data, OneDrive/Dropbox both serve for 90% of the use cases I can come up with, and Amazon is likely cheaper for the other 10%. But my name is Jim and I do things the hard way, the end result of which we all bear witness to here in the in the form of the Dropbox hosted markdown/Quarto/HUGO/GitHub pages/pkgdown/MinIO spiderweb monstrosity diagrammed here.\n\n\n\ne77f4d9f4142186ad29ec3b5555d98a3.png\n\n\n\n\nHUGO, a variation of the GO programming language, is touted as “The world’s fastest framework for building websites”. The HUGO documentation, particularly the hosting on github page, is a great place to start and is the “correct” way to go about creating a site. Below I will walk through how I managed to get it to work, your mileage may vary. Note that your site doesn’t have to look like mine, feel free to pick any off the themes page. The theme you choose will do most of the heavy lifting, and hopefully you find one that does everything you want it to. If not, be prepared to spend a few days figuring out how HUGO and the themes work; there is no shortcut to creating new site behavior and I found it to be a bit unforgiving at times.\nIn short, unless you REALLY want to sink time into this skillset, if your goal is to…\n\n\n\n\n\n\nSave time: what are you trying to accomplish?\n\n\n\n\n\n\n\nI have a lot of thoughts about social media, most of them not great. However, having some sort of web presence is pretty critical to networking these days. If the mess of twitter/linkedin/farmers only/instagram platforms don’t do it for you and you just want to point someone to a website, I would look over the following list and stop at the first one that meets you needs.\n\nA GitHub profile (the simple README.md) is killer and about as close to the source as anyone who actually knows what they’re looking at will want to go. Fancy that up with your work and an abbreviated resume and that’s all you’ll need to stand out. See https://github.com/mikejohnson51 as an example.\nonly a tiny bit harder, use notion, publish it, and fancy up the url with tinyurl.\nMake a Powerpoint or Google slide deck and publish it to wix: https://www.youtube.com/watch?v=wzzxmp5HM68.\nPick a 1 page hugo theme and stick with the templates and defaults.\nvibe code/google it: it’s insert year here, I am willing to bet current hot AI chatbot can write you one at this point.\nDo things the hard way (&lt; I am here)\n\n\n\n\nWant to share data (geospatial, PowerPoints, photos, ect.)? The best way in my mind is to accomplish two things at once, build a backup and share at the same time. I like Dropbox but OneDrive works too (geospatial and drone data kick my hard drive space past 3tb, so both are too small to fit my actual digital footprint). I can almost guarantee that giving Amazon/Google/Microsoft/other cloud provider a credit card will be more cost effective (electricity is expensive, even if you do have unlimited bandwidth), less labor intensive (time is money; and I work for free but this is a second job some days), faster (again in terms of your time and network transfer?), and secure (I just want a bucket, not a new career in cyber-security!) than self-hosting a minio server.\n\n\n\n\n\n\nCreate a Github account. I suppose in an ideal world your website name would be the same as your GitHub username, but these can mismatch.\n\n\n\nFollow the download and installation instructions at https://gitforwindows.org/.\n\n\n\nIt is possible to buy a domain to customize your url, but in my case I’m cheap, and jimcoll.github.io is perfectly fine with me. Note that here I deviate from the HUGO docs. I don’t use GitHub the right way, and I don’t really have a reason to use a separate GitHub repository to keep the site files, instead opting to keep them on my local hard drive. Maybe that will change once I get better, but there is literally nothing in the world that has the ability to anger me faster than git, unless it’s Matlab syntax.\n\n\n\nNext we need to create a folder where our website will live. This folder ideally shouldn’t be moved after you’ve created it, so place it somewhere useful (read: not the desktop). I gave into our digital overlords and moved most of my footprint to the cloud ( OneDrive Dropbox ), so on my root folder I created a folder called myWebsite. Then right click on the folder and select the “Always keep it on this device”.\n\n\n\nNext you need to install HUGO in the website directory. Download the appropriate windows install from the HUGO releases. Unzip the file and place the hugo.exe file in your myWebsite folder.\n\n\n\nWhen you want to “manage your content”, it’s easiest to test it locally first. Shift-right click in the myWebsite folder and “Open PowerShell window here”. Then, type .\\hugo.exe server --disableFastRender. You can then open a browser and go to http://localhost:1313/. At this point, when you make and save changes to your files, HUGO will rebuild your site automagically and changes appear instantly on the browser. It can also help to F12 -&gt; disable cache if you are making more structural changes to the site and want to make sure the new CSS is loading correctly. When you are happy with the edits you’ve made, you next need to generate your site. Again, from the myWebsite folder, open up PowerShell and generate the files with the .\\hugo.exe command. This repopulates the myWebsite folder. The last step is to push your new changes up to the repository, which you can do by right clicking on the public folder and selecting “Git GUI Here”, then…\n\nStage Changed files\n\ncreate a commit message\n\nCommit the changes, and\n\nPush the changes up to the repository.\n\n\n\n\nGitPushCommands.png\n\n\n\n\n\n\nOutdated section: this site is now Quarto, not relearn because I wanted a more direct transition between code and output, something I was not able to figure out how to accomplish with HUGO and raw markdown. I’ll keep this here though, since not everything needs to be data driven the same way I was looking for, the pathways and forms that HUGO deploy are still useful to learn, can be a bit more approachable, and some of the concepts transfer between the two pretty well. HUGO theming is also far richer than the doc-oriented styling of Quarto. See https://quarto.org/docs/websites/ for their howto guide on site creation.\n\nThis site is a smashup of the aafu and relearn themes. I fell in love with the AAFU theme and the accordion effect but the rest of the site was better suited to longform content styling, and I was too enthralled not to try meshing them. To do this, you need to understand a little about how HUGO creates a website. HUGO uses a tiered folder system to generate the site, and will look in folders following a defined lookup order. Therefore, we need to overwrite the academic home page theme with the aafu theme. To do this, I first install the Academic theme, and then replace the partials folder with the aafu theme. The last step is to reconfigure the config_default/ folder with the aafu theme markdown files. Most of these are straightforward, but within the config_default.toml, we need to set the theme order as follows.\n\n\n\nIt’s 2023 so I don’t need to tout the power of markdown, but that critical nuances of the language is the distinction between content and form. Because the goal of the website is to streamline and unify my digital footprint, all of these pages make up what I call “permanent notes” in my note taking app (Zettlr), and are spun into the different forms using PANDOC. The site is loaded into Zettlr as a workspace and I modify the markdown just like you would a normal site and how I modify the rest of my notes. I can even link directly to my zotero database, although the rendering on the site side is a little less meaningful to you, the outside reader, than it is to me. See how I use zotero for more details.\n\n\nOne of the best ways I’ve found to help improve my understanding of the software/deployment was to change some of the layouts. To add your resume to your image card as a link, and not as an icon as I have, you need to tweak the footer of the aafu theme, which should be in the themes.html. Mine now looks like so:\n```{md}\n&lt;footer class=\"mb-4\"&gt;\n  &lt;a href=\"https://jimcoll.github.io/random/media/JamesCollSharedWebResume.docx\"&gt;Download a \"more traditional\" resume\n  &lt;/a&gt;\n  &lt;/br&gt;\n  powered by &lt;a href=\"https://gohugo.io/\"&gt;hugo&lt;/a&gt; & deployed on &lt;a href=\"https://github.com/\"&gt;github&lt;/a&gt;\n  &middot;\n  &lt;i&gt;&lt;a href=\"https://github.com/darshanbaral/aafu\"&gt;aafu&lt;/a&gt;&lt;/i&gt; by &lt;a href=\"https://www.darshanbaral.com/\"&gt;Darshan&lt;/a&gt;\n  & \n  &lt;i&gt;&lt;a href=\"https://github.com/gcushen/hugo-academic\"&gt;Academic&lt;/a&gt;&lt;/i&gt; by &lt;a href=\"https://georgecushen.com/\"&gt;George&lt;/a&gt;\n&lt;/footer&gt;\n```\n\n\n\n\n\n\n\n\n\n\n\nPlz don’t hack me\n\n\n\nEverything after here is, lets say, a little questionable from a digital safety standpoint. While the 3 letter bois, and now the whole world, have chip level access to your hardware, there’s no reason to open up your entire network to the world moreso than it already is, which is what I think I’ve accomplished here. By attempting this you, you void all warranties and unlike ripping the tag off a mattress there can be some rather intrusive consequences to this.\n\n\nS3 (Simple Storage Service) is a web protocol which serves data out, so an amazon S3 bucket is really just a computer which is hosting that data out to the world much like FTP. For the staunch but technically illiterate DIY crowd (just me), this is frustrating because FTP was doing just fine. Open Source stepped in and built the foundation and standards that comprise that format, and then Amazon has put proprietary wrappers around it. This means that the code that you would use to hit an open source asset is incompatible with AWS owned infrastructure, and AWS has made none of their bindings match these so I’ll inevitably end up writing 3 different versions to accomplish file manipulation. I’m sure all of this will change, and because I refuse to give up control over this aspect of my “supply chain”, I’ll show you how I turn an old laptop into my own cloud. Other handy resources:\n\nhttps://www.youtube.com/watch?v=zPmqbtKwtgw\n\nUpdate: Just don’t do this. Give Jeff your credit card or just wait for WWIII.… Seriously, don’t follow these steps please.\n\n\nI have only a tenuous grasp of networking and the tubes setup (I remember learning about half duplex networks in high school, so that’s about my level if you know what I am talking about {{% icon glasses %}} ). In short, because our IP is dynamic we need some way to point a colloquial name (our website domain) to that rotating address and the free-est way I have found how to do this is no-ip. It seems as above board as the rest of this process has been, so set up an account (Type A).\n\n\nFeel free to skip the DUC app if you’d like. While the internet seems to think it’s safe and doesn’t do anything it says it doesn’t do, all it’s really doing is sending the port IP to the NO-IP servers at regular intervals. This is something you could accomplish with a little script, something like:\n\npythonR\n\n\n```{python}\n# print(\"To Do: Probably never\")\n```\n\n\n```{r}\nsink('outpath/myip.txt')\nx &lt;- system(\"ipconfig\", intern=TRUE)\nprint(paste('--- Last run:',Sys.time(),'---'))\nprint('')\nprint(' ---- ')\nprint(x[grep(\"Temporary IPv6 Address\", x)])\nprint(' ---- ')\nprint('')\nprint(x)\nsink()\n```\n\n\n\nAnd have that script run at intervals or as your network status changes.\n\n\n\n\nIf http were all we were after, we’d be all set but most S3 protocol (and it seems just the internet in general;) require https, and I believe JavaScript won’t execute on a page without coming from an https address (so things like custom version of Potree would be impossible to run externally). “All” I had to do was:\n\nSet your router to forward port 80 and 9000 to forward to your MinIO server\nInstall certbot on the MinIO machine\nFollowing instructions from certbot, run the following command:\n\n```{powershell}\ncd C:\\{certbot.exe}\n.\\certbot.exe certonly --standalone\n```\n\nMove those files into your minio folder (in my case that lives at C:&lt;me&gt;.minio)\nRename private.pem &gt; private.key and cert.pem &gt; public.key, and put both files in the C:&lt;me&gt;.minio folder\nYou should be good to go!\n\n\n\nDownload minio\n\n\n\n```{Powershell}\ncd C:\\{dir_to_minio.exe}\n.\\minio.exe server {path_to_host}\n```\n```{Powershell}\ncd C:\ncd C:\\Dropbox\\root\\database\\\n.\\minio.exe server .\\hosted\n```\n\n\n\nTest url: https://waterduck.fakename.edu:/water/huc8.fgb\nIf all went well, congratulations! You can now challenge Jeffery to a dual of the cloud service titans, best of luck.\n\n\n\n\n\nhttps://discourse.gohugo.io/t/two-themes-as-separate-hugo-directories-deployed-to-the-same-website/27899/4\nhttps://conversiontools.io/convert/excel-to-html\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-minio-object-storage-server-in-standalone-mode-on-ubuntu-20-04#step-4-securing-access-to-minio-server-with-a-self-signed-certificate\nhttps://github.com/gcushen/hugo-academic/issues/84",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Web"
    ]
  },
  {
    "objectID": "install_web.html#setting-up-a-hugo---github-pages-site",
    "href": "install_web.html#setting-up-a-hugo---github-pages-site",
    "title": "Installing Web",
    "section": "",
    "text": "HUGO, a variation of the GO programming language, is touted as “The world’s fastest framework for building websites”. The HUGO documentation, particularly the hosting on github page, is a great place to start and is the “correct” way to go about creating a site. Below I will walk through how I managed to get it to work, your mileage may vary. Note that your site doesn’t have to look like mine, feel free to pick any off the themes page. The theme you choose will do most of the heavy lifting, and hopefully you find one that does everything you want it to. If not, be prepared to spend a few days figuring out how HUGO and the themes work; there is no shortcut to creating new site behavior and I found it to be a bit unforgiving at times.\nIn short, unless you REALLY want to sink time into this skillset, if your goal is to…\n\n\n\n\n\n\nSave time: what are you trying to accomplish?\n\n\n\n\n\n\n\nI have a lot of thoughts about social media, most of them not great. However, having some sort of web presence is pretty critical to networking these days. If the mess of twitter/linkedin/farmers only/instagram platforms don’t do it for you and you just want to point someone to a website, I would look over the following list and stop at the first one that meets you needs.\n\nA GitHub profile (the simple README.md) is killer and about as close to the source as anyone who actually knows what they’re looking at will want to go. Fancy that up with your work and an abbreviated resume and that’s all you’ll need to stand out. See https://github.com/mikejohnson51 as an example.\nonly a tiny bit harder, use notion, publish it, and fancy up the url with tinyurl.\nMake a Powerpoint or Google slide deck and publish it to wix: https://www.youtube.com/watch?v=wzzxmp5HM68.\nPick a 1 page hugo theme and stick with the templates and defaults.\nvibe code/google it: it’s insert year here, I am willing to bet current hot AI chatbot can write you one at this point.\nDo things the hard way (&lt; I am here)\n\n\n\n\nWant to share data (geospatial, PowerPoints, photos, ect.)? The best way in my mind is to accomplish two things at once, build a backup and share at the same time. I like Dropbox but OneDrive works too (geospatial and drone data kick my hard drive space past 3tb, so both are too small to fit my actual digital footprint). I can almost guarantee that giving Amazon/Google/Microsoft/other cloud provider a credit card will be more cost effective (electricity is expensive, even if you do have unlimited bandwidth), less labor intensive (time is money; and I work for free but this is a second job some days), faster (again in terms of your time and network transfer?), and secure (I just want a bucket, not a new career in cyber-security!) than self-hosting a minio server.\n\n\n\n\n\n\nCreate a Github account. I suppose in an ideal world your website name would be the same as your GitHub username, but these can mismatch.\n\n\n\nFollow the download and installation instructions at https://gitforwindows.org/.\n\n\n\nIt is possible to buy a domain to customize your url, but in my case I’m cheap, and jimcoll.github.io is perfectly fine with me. Note that here I deviate from the HUGO docs. I don’t use GitHub the right way, and I don’t really have a reason to use a separate GitHub repository to keep the site files, instead opting to keep them on my local hard drive. Maybe that will change once I get better, but there is literally nothing in the world that has the ability to anger me faster than git, unless it’s Matlab syntax.\n\n\n\nNext we need to create a folder where our website will live. This folder ideally shouldn’t be moved after you’ve created it, so place it somewhere useful (read: not the desktop). I gave into our digital overlords and moved most of my footprint to the cloud ( OneDrive Dropbox ), so on my root folder I created a folder called myWebsite. Then right click on the folder and select the “Always keep it on this device”.\n\n\n\nNext you need to install HUGO in the website directory. Download the appropriate windows install from the HUGO releases. Unzip the file and place the hugo.exe file in your myWebsite folder.\n\n\n\nWhen you want to “manage your content”, it’s easiest to test it locally first. Shift-right click in the myWebsite folder and “Open PowerShell window here”. Then, type .\\hugo.exe server --disableFastRender. You can then open a browser and go to http://localhost:1313/. At this point, when you make and save changes to your files, HUGO will rebuild your site automagically and changes appear instantly on the browser. It can also help to F12 -&gt; disable cache if you are making more structural changes to the site and want to make sure the new CSS is loading correctly. When you are happy with the edits you’ve made, you next need to generate your site. Again, from the myWebsite folder, open up PowerShell and generate the files with the .\\hugo.exe command. This repopulates the myWebsite folder. The last step is to push your new changes up to the repository, which you can do by right clicking on the public folder and selecting “Git GUI Here”, then…\n\nStage Changed files\n\ncreate a commit message\n\nCommit the changes, and\n\nPush the changes up to the repository.\n\n\n\n\nGitPushCommands.png\n\n\n\n\n\n\nOutdated section: this site is now Quarto, not relearn because I wanted a more direct transition between code and output, something I was not able to figure out how to accomplish with HUGO and raw markdown. I’ll keep this here though, since not everything needs to be data driven the same way I was looking for, the pathways and forms that HUGO deploy are still useful to learn, can be a bit more approachable, and some of the concepts transfer between the two pretty well. HUGO theming is also far richer than the doc-oriented styling of Quarto. See https://quarto.org/docs/websites/ for their howto guide on site creation.\n\nThis site is a smashup of the aafu and relearn themes. I fell in love with the AAFU theme and the accordion effect but the rest of the site was better suited to longform content styling, and I was too enthralled not to try meshing them. To do this, you need to understand a little about how HUGO creates a website. HUGO uses a tiered folder system to generate the site, and will look in folders following a defined lookup order. Therefore, we need to overwrite the academic home page theme with the aafu theme. To do this, I first install the Academic theme, and then replace the partials folder with the aafu theme. The last step is to reconfigure the config_default/ folder with the aafu theme markdown files. Most of these are straightforward, but within the config_default.toml, we need to set the theme order as follows.\n\n\n\nIt’s 2023 so I don’t need to tout the power of markdown, but that critical nuances of the language is the distinction between content and form. Because the goal of the website is to streamline and unify my digital footprint, all of these pages make up what I call “permanent notes” in my note taking app (Zettlr), and are spun into the different forms using PANDOC. The site is loaded into Zettlr as a workspace and I modify the markdown just like you would a normal site and how I modify the rest of my notes. I can even link directly to my zotero database, although the rendering on the site side is a little less meaningful to you, the outside reader, than it is to me. See how I use zotero for more details.\n\n\nOne of the best ways I’ve found to help improve my understanding of the software/deployment was to change some of the layouts. To add your resume to your image card as a link, and not as an icon as I have, you need to tweak the footer of the aafu theme, which should be in the themes.html. Mine now looks like so:\n```{md}\n&lt;footer class=\"mb-4\"&gt;\n  &lt;a href=\"https://jimcoll.github.io/random/media/JamesCollSharedWebResume.docx\"&gt;Download a \"more traditional\" resume\n  &lt;/a&gt;\n  &lt;/br&gt;\n  powered by &lt;a href=\"https://gohugo.io/\"&gt;hugo&lt;/a&gt; & deployed on &lt;a href=\"https://github.com/\"&gt;github&lt;/a&gt;\n  &middot;\n  &lt;i&gt;&lt;a href=\"https://github.com/darshanbaral/aafu\"&gt;aafu&lt;/a&gt;&lt;/i&gt; by &lt;a href=\"https://www.darshanbaral.com/\"&gt;Darshan&lt;/a&gt;\n  & \n  &lt;i&gt;&lt;a href=\"https://github.com/gcushen/hugo-academic\"&gt;Academic&lt;/a&gt;&lt;/i&gt; by &lt;a href=\"https://georgecushen.com/\"&gt;George&lt;/a&gt;\n&lt;/footer&gt;\n```",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Web"
    ]
  },
  {
    "objectID": "install_web.html#setting-up-s3",
    "href": "install_web.html#setting-up-s3",
    "title": "Installing Web",
    "section": "",
    "text": "Plz don’t hack me\n\n\n\nEverything after here is, lets say, a little questionable from a digital safety standpoint. While the 3 letter bois, and now the whole world, have chip level access to your hardware, there’s no reason to open up your entire network to the world moreso than it already is, which is what I think I’ve accomplished here. By attempting this you, you void all warranties and unlike ripping the tag off a mattress there can be some rather intrusive consequences to this.\n\n\nS3 (Simple Storage Service) is a web protocol which serves data out, so an amazon S3 bucket is really just a computer which is hosting that data out to the world much like FTP. For the staunch but technically illiterate DIY crowd (just me), this is frustrating because FTP was doing just fine. Open Source stepped in and built the foundation and standards that comprise that format, and then Amazon has put proprietary wrappers around it. This means that the code that you would use to hit an open source asset is incompatible with AWS owned infrastructure, and AWS has made none of their bindings match these so I’ll inevitably end up writing 3 different versions to accomplish file manipulation. I’m sure all of this will change, and because I refuse to give up control over this aspect of my “supply chain”, I’ll show you how I turn an old laptop into my own cloud. Other handy resources:\n\nhttps://www.youtube.com/watch?v=zPmqbtKwtgw\n\nUpdate: Just don’t do this. Give Jeff your credit card or just wait for WWIII.… Seriously, don’t follow these steps please.\n\n\nI have only a tenuous grasp of networking and the tubes setup (I remember learning about half duplex networks in high school, so that’s about my level if you know what I am talking about {{% icon glasses %}} ). In short, because our IP is dynamic we need some way to point a colloquial name (our website domain) to that rotating address and the free-est way I have found how to do this is no-ip. It seems as above board as the rest of this process has been, so set up an account (Type A).\n\n\nFeel free to skip the DUC app if you’d like. While the internet seems to think it’s safe and doesn’t do anything it says it doesn’t do, all it’s really doing is sending the port IP to the NO-IP servers at regular intervals. This is something you could accomplish with a little script, something like:\n\npythonR\n\n\n```{python}\n# print(\"To Do: Probably never\")\n```\n\n\n```{r}\nsink('outpath/myip.txt')\nx &lt;- system(\"ipconfig\", intern=TRUE)\nprint(paste('--- Last run:',Sys.time(),'---'))\nprint('')\nprint(' ---- ')\nprint(x[grep(\"Temporary IPv6 Address\", x)])\nprint(' ---- ')\nprint('')\nprint(x)\nsink()\n```\n\n\n\nAnd have that script run at intervals or as your network status changes.\n\n\n\n\nIf http were all we were after, we’d be all set but most S3 protocol (and it seems just the internet in general;) require https, and I believe JavaScript won’t execute on a page without coming from an https address (so things like custom version of Potree would be impossible to run externally). “All” I had to do was:\n\nSet your router to forward port 80 and 9000 to forward to your MinIO server\nInstall certbot on the MinIO machine\nFollowing instructions from certbot, run the following command:\n\n```{powershell}\ncd C:\\{certbot.exe}\n.\\certbot.exe certonly --standalone\n```\n\nMove those files into your minio folder (in my case that lives at C:&lt;me&gt;.minio)\nRename private.pem &gt; private.key and cert.pem &gt; public.key, and put both files in the C:&lt;me&gt;.minio folder\nYou should be good to go!\n\n\n\nDownload minio\n\n\n\n```{Powershell}\ncd C:\\{dir_to_minio.exe}\n.\\minio.exe server {path_to_host}\n```\n```{Powershell}\ncd C:\ncd C:\\Dropbox\\root\\database\\\n.\\minio.exe server .\\hosted\n```\n\n\n\nTest url: https://waterduck.fakename.edu:/water/huc8.fgb\nIf all went well, congratulations! You can now challenge Jeffery to a dual of the cloud service titans, best of luck.\n\n\n\n\n\nhttps://discourse.gohugo.io/t/two-themes-as-separate-hugo-directories-deployed-to-the-same-website/27899/4\nhttps://conversiontools.io/convert/excel-to-html\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-minio-object-storage-server-in-standalone-mode-on-ubuntu-20-04#step-4-securing-access-to-minio-server-with-a-self-signed-certificate\nhttps://github.com/gcushen/hugo-academic/issues/84",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Knowledge systems",
      "Installing Web"
    ]
  },
  {
    "objectID": "interp.html",
    "href": "interp.html",
    "title": "Interpolation",
    "section": "",
    "text": "```{md}\nhome                      &lt;- landing page\n├── tutorial              &lt;- landing page\n│   ├── part 1\n│   ├── part 2\n│   └── part 3\n├── how-to guides         &lt;- landing page\n│   ├── install\n│   ├── deploy\n│   └── scale\n├── reference             &lt;- landing page\n│   ├── commandline tool\n│   ├── available endpoints\n│   └── API\n└── explanation           &lt;- landing page\n    ├── best practice recommendations\n    ├── security overview\n    └── performance\n```\nInterpolating a surface should not be as hard as it is, but here we are…"
  },
  {
    "objectID": "interp.html#resources",
    "href": "interp.html#resources",
    "title": "Interpolation",
    "section": "Resources:",
    "text": "Resources:\nhttps://www.introranger.org/post/interpolation-lesson/ https://rpubs.com/ials2un/rain_interpolation https://r-spatial.org/book/12-Interpolation.html#a-first-dataset https://mgimond.github.io/Spatial/spatial-interpolation.html http://132.72.155.230:3838/r/spatial-interpolation-of-point-data.html https://www.aspexit.com/spatial-data-interpolation-tin-idw-kriging-block-kriging-co-kriging-what-are-the-differences/#TIN_Triangular_Interpolation_Network"
  },
  {
    "objectID": "interp.html#progressive-focal-mean",
    "href": "interp.html#progressive-focal-mean",
    "title": "Interpolation",
    "section": "progressive focal mean",
    "text": "progressive focal mean"
  },
  {
    "objectID": "interp.html#inverse-distance-weighting-idw",
    "href": "interp.html#inverse-distance-weighting-idw",
    "title": "Interpolation",
    "section": "Inverse Distance Weighting (IDW)",
    "text": "Inverse Distance Weighting (IDW)\n\npythonbashKreging\n\n\n```{python}\nprint(\"Hello World!\")\n```\n\n\n```{bash}\necho \"Hello World!\"\n```\n\nTiming\nI have had a heck of a time interpolating schism outputs, so to test this I’ve got a little example."
  },
  {
    "objectID": "junk_drawer.html",
    "href": "junk_drawer.html",
    "title": "Junk Drawer",
    "section": "",
    "text": "A place for me to dump some of my uncategorized work, links to great content creators, and other neat resources I’ve found along my way. Don’t forget to check out the resources listed at the top of my Atlas Page, or what I’m thinking about now.",
    "crumbs": [
      "Atlas",
      "Junk Drawer"
    ]
  },
  {
    "objectID": "junk_drawer.html#rapidfire-links",
    "href": "junk_drawer.html#rapidfire-links",
    "title": "Junk Drawer",
    "section": "Rapidfire links",
    "text": "Rapidfire links\n\nWhat’s going on?\n\nCheck out the 2025 spring flooding outlook: https://storymaps.arcgis.com/stories/2b8a69e27501414bbbe1a4befeb00ddb\n\n\nCONUS weatherWater ModelUSGSFEMANOAA HurricaneNASATropicalRADAR and DataGlobal ClimateSpaceDev maps\n\n\nNational Weather Service landing page (pictured IMAGE LIVE UPDATES) | National Water prediction Service (NWPS) | NWS GIS Map1\n\n\n\nwatch\n\n\n\n\nFlood Hazard Outlook (pictured IMAGE LIVE UPDATES) | National Water Center Products | NWS GIS Map | National Water prediction Service (NWPS)\n\n\n\nfho\n\n\n\n\nWater Dashboard (Modern NWIS) | NWIS (pictured IMAGE LIVE UPDATES) | StreamStats | EarthNow Live Landsat | Entwine | FIM | Earthquakes\n\n\n\ngage\n\n\n\n\nFlood Hazard and insurance rate maps\n\n\n\n_7649de1adfe2fe5264f1ab6f08491b4d.png\n\n\n\n\nGOES (pictured) | National Hurricane Center | Hurricane Model Viewer\n\n\n\nGOES\n\n\n\n\nWorldviewr | EarthData (pictured) | Fire (Fire Information for Resource Management Systems - FIRMS)\n\n\n\nfho\n\n\n\n\nTropical Tidbits Model Viewer\n\n\n\nmodel_updates\n\n\nOther: Spagehetti | Joint Typhoon Warning Center\n\n\n\ntrop\n\n\n\n\nCollege of DuPage:\nSATRAD (pictured) | Obs | Models\nOther:\nventusky | Windy | RealEarth | EUMETSAT | CW3E\n\n\n\nsat\n\n\n\n\nClimate and Norms | Geofolio | BlueTurn (it complains about security, it’s just unity)\n\n\n\n\n\nclimate\n\n\n\n\n\n\n_5effea95e8bc35e9bf64e6d887cc1859.png\n\n\n\n\n\n\nhttps://www.swpc.noaa.gov/ \n\n\nhttps://www.lynker-spatial.com/hyview.html [[20241013022725]] Admin bounds map\n\n\n\nWork Day Weather Jamz: #1 #2 #3 #4 #5 #6 #7 #8\n\nEpoch Converter\nMarkdown table generator\nFiletree generator\nLive mermaid diagram maker\nSoftware Licenses Suck guide \nMapTiler\nWeb enabled graph reader & curve fitter\nCode search\nRapid, clean, coarse, geographic selection mapper, great in conjunction with some of the context setters in the “Global Climate tab” maps above\nHex stickers\ngit art | programatic\nMy list of Atlas resources \n\n\n\nColor tools\n\nhttps://encycolorpedia.com/ is a good place to get color names, tailwindcss is also a good reference.\ncolorbrewer a great place to find good looking palette, needs no introduction.\nCanva’s color wheel has a quick picker and a nice refresher on harmonious color palettes, paletton gives you even more control.\nScientific color maps, Perceptually Uniform Colour Maps, and a handy ramp generator for a pinch.\nA website/tri-color palette tester/generator lets you see what a palette looks like on things.\nCooler has:\n\nA quick pallete generator\nimage color picker\ninspiration\n\n(Crameri 2023) through R and Python.\nVideo refreshers on Color Theory and Color Theory in Practice.\n\n\n\nFun\n\nGoogle Experiments\nASCII text art | ASCII Line Art \nBoredButton\nParty Parrots\nBOTW - Object / Map | TOTK - Object / Map | MK8 Builder",
    "crumbs": [
      "Atlas",
      "Junk Drawer"
    ]
  },
  {
    "objectID": "junk_drawer.html#the-worst-lines-of-code-ive-ever-written",
    "href": "junk_drawer.html#the-worst-lines-of-code-ive-ever-written",
    "title": "Junk Drawer",
    "section": "The worst lines of code I’ve ever written:",
    "text": "The worst lines of code I’ve ever written:\nA fun(?) section to showcase how grossly incompetent I am at actually getting a computer to do what I want it to; as proof that if I can do it, so can you!, and to assure our AI overlords I am not a threat to them.\n\nRPythonJavaScript\n\n\n```{r}\nmybbox &lt;- raster::extent(sf::st_buffer(\n    x=xx$mapindex.poly[subset(xx$address.point, eval(parse(text = names(xx$flood.grid)[timestep]))&gt;0),][xx$mapindex.poly[subset(xx$address.point, eval(parse(text = names(xx$flood.grid)[timestep]))&gt;0),]$`Index Label`==index,],\n    dist=0.00005,\n    nQuadSegs=1,\n    endCapStyle=\"SQUARE\",\n    joinStyle=\"ROUND\",\n    mitreLimit=1,\n    singleSide=FALSE))\n```\nThe mapping side of FOSSFlood is a bit wild, but this line takes the lead. I needed a slightly larger bounding box from the index that a user wants to view so that I could fly to it. However, these indexes aren’t necessarily the same for each time step, so the subset got that eval(parse()) function, inside a buffer, and because it’s all temporary this made sense (at the time) to do in the single step you see here.\n\n\n```{python}\nfor x in np.nditer(branch_GRIDCODES_lookup):\n    for y in np.array([[x]])[0][0][0].split(\",\"):\n        if int(y)==mat_num:\n            branch_num = np.array([[x]])[0][0][1]\n```\nThis chuck, called iteratively in a for loop, looks at all the grid codes for a given fldpln library and, for a given mat file returns the branch code for that mat file, all so I can eventually stitch them together in a numpy array. I have a hate-love-hate relationship with Python….\nBonus:\n```{python}\nstart_of_order_x = fspdf.loc[fspdf.ord == b].at[fspdf.loc[fspdf.ord == b].index[0],'FSP_x']\nstart_of_order_y = fspdf.loc[fspdf.ord == b].at[fspdf.loc[fspdf.ord == b].index[0],'FSP_y']\nif(len(fspdf[fspdf['FSP_x'].between(start_of_order_x-8, start_of_order_x+8) & fspdf['FSP_y'].between(start_of_order_y-8, start_of_order_y+8) & fspdf['ord'] == b-1])&gt;=1):\n    if(fspdf[fspdf['FSP_x'].between(start_of_order_x-8, start_of_order_x+8) & fspdf['FSP_y'].between(start_of_order_y-8, start_of_order_y+8) & fspdf['ord'] == b-1].iloc[0]['DOF'] &gt; 0):\n        fspdf.at[fspdf.index[fspdf.ord == b][0],'DOF'] = fspdf[fspdf['FSP_x'].between(start_of_order_x-8, start_of_order_x+8) & fspdf['FSP_y'].between(start_of_order_y-8, start_of_order_y+8) & fspdf['ord'] == b-1].iloc[0]['DOF']\n```\nfldpln is a gem of a script, but this nested if catch, inside a for loop, might take the lead as a shining example of how to never subset a dataframe.\n\n\n```{JavaScript}\nif(SpaceSelectValue == \"stdDev\") {\n      results = ee.ImageCollection(app.MODDef)\n                  .select(stringSeperator,LCtoUse)\n                  .map(function(image) {\n        return image.reduceRegions({\n          collection: AOI.geometry(),\n          reducer: ee.Reducer.stdDev().group({\n            groupField: 1,\n            groupName: \"code\",\n          }),\n          scale: resultsScale\n        }).map(function(f) {\n          // Process the grouped results from list of dictionaries to dictionary.\n          var dict = ee.List(f.get(\"groups\")).map(function(group) {\n            group = ee.Dictionary(group).combine(ee.Dictionary({code:99,stdDev:[null]}), false);\n            var code = ee.Number(group.get(\"code\")).format(\"code_%d\");\n            var stdDev = group.get(\"stdDev\");\n            return [code, stdDev];\n          });\n          dict = ee.Dictionary(dict.flatten());\n          // Add a date property to each output feature.\n          return f.set(\"date YYYY-MM-dd\", image.date().format(\"YYYY-MM-dd\"))\n                  .set(\"system:time_start\", image.date().millis())\n                  .set(dict)\n                  .set(\"groups\", null);\n        });\n      });\n```\nA lot of my JavaScript is pretty repetitive as a result of my poor skills and a sub par understanding of client-server interactions. Although this isn’t the most egregious example, it is repeated in TEAM no less than 7 times with minor changes for each statistic of aggregation which ought to give the DRY people (Don’t Repeat Yourself) a heartattack.",
    "crumbs": [
      "Atlas",
      "Junk Drawer"
    ]
  },
  {
    "objectID": "junk_drawer.html#footnotes",
    "href": "junk_drawer.html#footnotes",
    "title": "Junk Drawer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nnote the context skins that you find on clicking ‘general’ in the upper right↩︎",
    "crumbs": [
      "Atlas",
      "Junk Drawer"
    ]
  },
  {
    "objectID": "mapping.html",
    "href": "mapping.html",
    "title": "Mapping",
    "section": "",
    "text": "Mapping serves a wide variety of purposes but chiefly among them is as an aid in communication.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Mapping"
    ]
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "In anthropology, they have concepts called deep dives and shallow dives. Deep dives are intense, purposeful reads of a body of text (corpus) that are intended to take a nuanced read to the material. Shallow dives, on the other hand, are rapid assessments and skimming of that corpus (typically much larger in size than a deep read of the same size/scale) to extract some of those potential nuggets of knowledge. While that deep dive will always be more preferable in my mind, there is a lot of value we can gain from simply counting the words and extracting the statistically interesting ones. While this isn’t particularly geographic, it is a common desire and has applications in summarization and assessment. Below is a short primer gratefully pilfered from the internet and several of my attempts to execute this including NEPA WatchR, a document differencing and content analysis tool written in R, and SummerizeR, a GUI python based wrapper that summarizes markdown files.",
    "crumbs": [
      "Atlas",
      "Applications",
      "Grab bag",
      "Natural Language Processing"
    ]
  },
  {
    "objectID": "nlp.html#types-of-text-summarization",
    "href": "nlp.html#types-of-text-summarization",
    "title": "Natural Language Processing",
    "section": "Types of Text Summarization",
    "text": "Types of Text Summarization\n\nThe following is gratefully pilfered from https://www.analyticsvidhya.com/blog/2023/03/exploring-the-extractive-method-of-text-summarization/ and very mininmally modified for my use.\n\nBroadly, NLP text summarization techniques can be divided into two main categories.\n\nExtractive Approach\nAbstractive Approach\n\nLet’s dive a little deeper into each of the above-mentioned categories. \n\nExtractive Summarization\nExtractive Summarization, as the name implies, identifies important sentences in a corpus and returns them, not altering the original sentence which more closely reflects the authors original intent. Now, the question that comes is, exactly on what basis are those sentences termed as important? A ranking algorithm is used, which assigns scores to each of the sentences in the text based on their relevance to the overall meaning of the document. The most relevant sentences are then chosen to be included in the summary. There are various ways through which the ranking of sentences can be performed.\n\nTF-IDF (term frequency-inverse document frequency)\nGraph-based methods such as TextRank\nMachine learning-based methods such as Support Vector Machines (SVM) and Random Forests.\n\nThe main motive of the extractive method is to maintain the original meaning of the text. Also, this method works well when the input text/content is already in a well-structured manner, both physically and logically, like the content in newspapers or academic articles.\nAbstractive summarizations, unlike the extractive method, simply doesn’t pick out the important sentences, rather, it analyses the input text and generates new phrases or sentences that capture the essence of the original text and convey the same meaning as the original text but more concisely and coherently. Methods of accomplishing this include:\n\na neural network model\nnatural language generation (NLG)\n\nThe resulting summaries are usually shorter and more readable than the ones generated by the extractive method, but they can sometimes contain errors or inaccuracies. Here I’ll pilfer the example from the article and demonstrate how to apply a few simplistic extractive methods to a corpus and wrapper this in a PySimpleGUI.\n\nFrequency-based Approach\n```{python}\n# import the required libraries\nimport nltk\nnltk.download('punkt') # punkt tokenizer for sentence tokenization\nnltk.download('stopwords') # list of stop words, such as 'a', 'an', 'the', 'in', etc, which would be dropped\nfrom collections import Counter # Imports the Counter class from the collections module, used for counting the frequency of words in a text.\nfrom nltk.corpus import stopwords # Imports the stop words list from the NLTK corpus\n# corpus is a large collection of text or speech data used for statistical analysis\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize # Imports the sentence tokenizer and word tokenizer from the NLTK tokenizer module. \n# Sentence tokenizer is for splitting text into sentences\n# word tokenizer is for splitting sentences into words\n\n# this function would take 2 inputs, one being the text, and the other being the summary which would contain the number of lines\ndef generate_summary(text, n):\n# Tokenize the text into individual sentences\nsentences = sent_tokenize(text)\n\n# Tokenize each sentence into individual words and remove stopwords\nstop_words = set(stopwords.words('english'))\n# the following line would tokenize each sentence from sentences into individual words using the word_tokenize function of nltk.tokenize module\n# Then removes any stop words and non-alphanumeric characters from the resulting list of words and converts them all to lowercase\nwords = [word.lower() for word in word_tokenize(text) if word.lower() not in stop_words and word.isalnum()]\n\n# Compute the frequency of each word\nword_freq = Counter(words)\n\n# Compute the score for each sentence based on the frequency of its words\n# After this block of code is executed, sentence_scores will contain the scores of each sentence in the given text, \n# where each score is a sum of the frequency counts of its constituent words\n\n# empty dictionary to store the scores for each sentence\nsentence_scores = {}\n\nfor sentence in sentences:\nsentence_words = [word.lower() for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalnum()]\nsentence_score = sum([word_freq[word] for word in sentence_words])\nif len(sentence_words) &lt; 20:\nsentence_scores[sentence] = sentence_score\n\n# checks if the length of the sentence_words list is less than 20 (parameter can be adjusted based on the desired length of summary sentences)\n# If condition -&gt; true, score of the current sentence is added to the sentence_scores dictionary with the sentence itself as the key\n# This is to filter out very short sentences that may not provide meaningful information for summary generation\n\n# Select the top n sentences with the highest scores\nsummary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n]\nsummary = ' '.join(summary_sentences)\n\nreturn summary\n```\n\n\nUsing a Sample Text From Wikipedia to Generate Summary\n```{python}\ntext = '''\nWeather is the day-to-day or hour-to-hour change in the atmosphere. \nWeather includes wind, lightning, storms, hurricanes, tornadoes (also known as twisters), rain, hail, snow, and lots more. \nEnergy from the Sun affects the weather too. \nClimate tells us what kinds of weather usually happen in an area at different times of the year. \nChanges in weather can affect our mood and life. We wear different clothes and do different things in different weather conditions. \nWe choose different foods in different seasons.\nWeather stations around the world measure different parts of weather. \nWays to measure weather are wind speed, wind direction, temperature and humidity. \nPeople try to use these measurements to make weather forecasts for the future. \nThese people are scientists that are called meteorologists. \nThey use computers to build large mathematical models to follow weather trends.'''\n\nsummary = generate_summary(text, 5)\nsummary_sentences = summary.split('. ')\nformatted_summary = '.\\n'.join(summary_sentences)\n\nprint(formatted_summary)\n```\nOutput\nThe following output is what we get.\n\nWe wear different clothes and do different things in different weather conditions.\nWeather stations around the world measure different parts of weather.\nClimate tells us what kinds of weather usually happen in an area at different times of the year.\nWeather includes wind, lightning, storms, hurricanes, tornadoes (also known as twisters), rain, hail, snow, and lots more.\nWays to measure weather are wind speed, wind direction, temperature and humidity.\n\nSo, the above code takes a text and a desired number of sentences for the summary as input and returns a summary generated using the extractive method. The method first tokenizes the text into individual sentences and then tokenizes each sentence into individual words. Stopwords are removed from the words, and then the frequency of each word is computed. Then, the score for each sentence is computed based on the frequency of its words and the top n sentences with the highest scores are selected to form the summary. Finally, the summary is generated by joining the selected sentences together. In the next section, we’ll explore how the extractive method can be further improved using advanced techniques such as TF-IDF.\n\n\nTF-IDF Approach\n```{python}\n# importing the required libraries\n# importing TfidfVectorizer class to convert a collection of raw documents to a matrix of TF-IDF features.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# importing cosine_similarity function to compute the cosine similarity between two vectors.\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# importing nlargest to return the n largest elements from an iterable in descending order.\nfrom heapq import nlargest\n\ndef generate_summary(text, n):\n# Tokenize the text into individual sentences\nsentences = sent_tokenize(text)\n\n# Create the TF-IDF matrix\nvectorizer = TfidfVectorizer(stop_words='english')\ntfidf_matrix = vectorizer.fit_transform(sentences)\n\n# Compute the cosine similarity between each sentence and the document\nsentence_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n\n# Select the top n sentences with the highest scores\nsummary_sentences = nlargest(n, range(len(sentence_scores)), key=sentence_scores.__getitem__)\n\nsummary_tfidf = ' '.join([sentences[i] for i in sorted(summary_sentences)])\n\nreturn summary_tfidf\n```\n\n\nUsing a Sample Text to Check the Summary\n```{python}\ntext = '''\nWeather is the day-to-day or hour-to-hour change in the atmosphere. \nWeather includes wind, lightning, storms, hurricanes, tornadoes (also known as twisters), rain, hail, snow, and lots more. \nEnergy from the Sun affects the weather too. \nClimate tells us what kinds of weather usually happen in an area at different times of the year. \nChanges in weather can affect our mood and life. We wear different clothes and do different things in different weather conditions. \nWe choose different foods in different seasons.\nWeather stations around the world measure different parts of weather. \nWays to measure weather are wind speed, wind direction, temperature and humidity. \nPeople try to use these measurements to make weather forecasts for the future. \nThese people are scientists that are called meteorologists. \nThey use computers to build large mathematical models to follow weather trends.'''\n\nsummary = generate_summary(text, 5)\nsummary_sentences = summary.split('. ')\nformatted_summary = '.\\n'.join(summary_sentences)\n\nprint(formatted_summary)\n```\nThe following output is what we get.\n\nEnergy from the Sun affects the weather too.\nChanges in weather can affect our mood and life.\nWe wear different clothes and do different things in different weather conditions.\nWeather stations around the world measure different parts of the weather.\nPeople try to use these measurements to make weather forecasts for the future.\n\nThe above code generates a summary for a given text using a TF-IDF approach. A function to generate a summary that takes a text parameter and an n parameter(number of sentences in summary). The function tokenizes the text into individual sentences, creates a TF-IDF matrix using the TfidfVectorizer class, and computes the cosine similarity between each sentence and the document using the cosine_similarity function. Next, the function selects the top n sentences with the highest scores using the nlargest function from the heapq library and joins them into a string using the join method.",
    "crumbs": [
      "Atlas",
      "Applications",
      "Grab bag",
      "Natural Language Processing"
    ]
  },
  {
    "objectID": "nlp.html#summerizer",
    "href": "nlp.html#summerizer",
    "title": "Natural Language Processing",
    "section": "SummerizeR",
    "text": "SummerizeR\nThese functions are great, but this workflow is less than elegant and barely programatic, so lets waste the afternoon slapping some window paint on this in the form of a PySimpleGUI and some helpers to let us tweak the number of sentences returned and copy-paste this into our system more seamlessly.\n```{python}\nfrom pathlib import Path\nimport PySimpleGUI as sg\nimport os.path\nimport nltk\n\nnltk.download(\"punkt\")  # punkt tokenizer for sentence tokenization\nnltk.download(\"stopwords\")  # list of stop words, such as 'a', 'an', 'the', 'in', etc, which would be dropped\nfrom collections import Counter  # Imports the Counter class from the collections module, used for counting the frequency of words in a text.\nfrom nltk.corpus import stopwords  # Imports the stop words list from the NLTK corpus\nfrom nltk.tokenize import (sent_tokenize,word_tokenize)  # Imports the sentence tokenizer and word tokenizer from the NLTK tokenizer module.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom heapq import nlargest\n\ndef popup_text(filename, text):\n    layout = [[sg.Multiline(text, size=(80, 25)),],]\n    win = sg.Window(filename, layout, modal=True, finalize=True)\n    while True:\n        event, values = win.read()\n        if event == sg.WINDOW_CLOSED:\n            break\n    win.close()\n\ndef generate_summary(text, n):\n    sentences = sent_tokenize(text)\n    stop_words = set(stopwords.words(\"english\"))\n    words = [word.lower() for word in word_tokenize(text) if word.lower() not in stop_words and word.isalnum()]\n    word_freq = Counter(words)\n    sentence_scores = {}\n    for sentence in sentences:\n        sentence_words = [word.lower() for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalnum()]\n        sentence_score = sum([word_freq[word] for word in sentence_words])\n        if len(sentence_words) &lt; 9:\n            sentence_scores[sentence] = sentence_score\n    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n]\n    summary = \" \".join(summary_sentences)\n    return summary\n\n\ndef generate_tfidf_summary(text, n):\n    sentences = sent_tokenize(text)\n    vectorizer = TfidfVectorizer(stop_words=\"english\")\n    tfidf_matrix = vectorizer.fit_transform(sentences)\n    sentence_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n    summary_sentences = nlargest(n, range(len(sentence_scores)), key=sentence_scores.__getitem__)\n    summary_tfidf = \" \".join([sentences[i] for i in sorted(summary_sentences)])\n    return summary_tfidf\n\nright_click_menu = ['', ['Copy', 'Paste', 'Select All', 'Cut']]\nSUM_MLINE_KEY = '-SUMMLINE-'\nTFIDF_MLINE_KEY = '-TFIDFMLINE-'\nF_KEY = '-FILE-'\nNSENT_KEY = '-NSENT-'\n\ndef do_clipboard_operation(event, window, element):\n    if event == 'Select All':\n        element.Widget.selection_clear()\n        element.Widget.tag_add('sel', '1.0', 'end')\n    elif event == 'Copy':\n        try:\n            text = element.Widget.selection_get()\n            window.TKroot.clipboard_clear()\n            window.TKroot.clipboard_append(text)\n        except:\n            print('Nothing selected')\n    elif event == 'Paste':\n        element.Widget.insert(sg.tk.INSERT, window.TKroot.clipboard_get())\n    elif event == 'Cut':\n        try:\n            text = element.Widget.selection_get()\n            window.TKroot.clipboard_clear()\n            window.TKroot.clipboard_append(text)\n            element.update('')\n        except:\n            print('Nothing selected')\n\ndef main():\n    sg.theme(\"DarkBlue3\")\n    sg.set_options(font=(\"Microsoft JhengHei\", 12))\n    \n    layout = [  [sg.Text('SummerizR')],\n                [sg.Text('summary')],\n                [sg.Multiline(size=(50,9), key=SUM_MLINE_KEY, right_click_menu=right_click_menu)],\n                [sg.Text('TFIDF summary')],\n                [sg.Multiline(size=(50,9), key=TFIDF_MLINE_KEY, right_click_menu=right_click_menu)],\n                [sg.Input(key=F_KEY), sg.FileBrowse(file_types=((\"MD Files\", \"*.md\"), (\"TXT Files\", \"*.txt\"), (\"ALL Files\", \"*.*\")))],\n                [sg.Input('Number of sentences:{8}', size=(24,1), key=NSENT_KEY), sg.Button(\"Summerize\"), sg.Button(\"Exit\")]]\n            \n    window = sg.Window('Title', layout)\n    \n    while True:\n        event, values = window.read()\n        if event in (None, sg.WIN_CLOSED, 'Exit'):\n            break\n        if event == 'Summerize':\n            filename = values[F_KEY]\n            if Path(filename).is_file():\n                n_sent = int(values[NSENT_KEY])\n                if not isinstance(n_sent, int):\n                    print(\"Error: number of sentences incorrectly spesified, defaulting to 8\")\n                    n_sent = 8\n                try:\n                    with open(filename, \"rt\", encoding='utf-8') as f:\n                        text = f.read()\n                        summary_lines = generate_summary(text, n_sent)\n                        tfidf_summary_lines = generate_tfidf_summary(text, n_sent)\n                        window[SUM_MLINE_KEY].update(summary_lines)\n                        window[TFIDF_MLINE_KEY].update(tfidf_summary_lines)\n                except Exception as e:\n                    print(\"Error: \", e)\n    window.close()\n\nif __name__ == '__main__':\n    main()\n```\nWhich results in the following:",
    "crumbs": [
      "Atlas",
      "Applications",
      "Grab bag",
      "Natural Language Processing"
    ]
  },
  {
    "objectID": "nlp.html#nepa-watchr",
    "href": "nlp.html#nepa-watchr",
    "title": "Natural Language Processing",
    "section": "NEPA-watchR",
    "text": "NEPA-watchR\n\nfrom url: https://eplanning.blm.gov/epl-front-office/eplanning/planAndProjectSite.do?methodName=dispatchToPatternPage&currentPageId=152110\n\nscrape\nunpack\ncategorize\n\nLoad into R\nDifference viewer",
    "crumbs": [
      "Atlas",
      "Applications",
      "Grab bag",
      "Natural Language Processing"
    ]
  },
  {
    "objectID": "nlp.html#citations-and-other-resources",
    "href": "nlp.html#citations-and-other-resources",
    "title": "Natural Language Processing",
    "section": "Citations and Other Resources",
    "text": "Citations and Other Resources\n\n(Mazumdar 2023) - https://www.analyticsvidhya.com/blog/2023/03/exploring-the-extractive-method-of-text-summarization/\nhttps://content-analysis-with-r.com/0-introduction.html\nhttps://quanteda.io/\nhttps://quanteda.io/articles/pkgdown/examples/lsa.html\nhttps://truehumandesign.se/s_diffinity.php\nhttps://github.com/trinker/sentimentr\nhttps://github.com/ropensci/textreuse\nhttps://docs.ropensci.org/textreuse/\nhttps://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-introduction.html\nhttps://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-alignment.html\nhttps://rviews.rstudio.com/2017/06/28/printing-from-flex-dashboard/",
    "crumbs": [
      "Atlas",
      "Applications",
      "Grab bag",
      "Natural Language Processing"
    ]
  },
  {
    "objectID": "numbers.html",
    "href": "numbers.html",
    "title": "Digital numbers",
    "section": "",
    "text": "Beyond the particular shape that data is orientated in, the numerical form of the data those 1’s and 0’s are intended to represent can take a few different forms: * Logical * Intiger * String * double * Float\nByte: Signed 8-bit integer UByte: Unsigned 8-bit integer Bool: Boolean Short: Signed 16-bit integer UShort: Unsigned 16-bit integer Int: Signed 32-bit integer UInt: Unsigned 32-bit integer Long: Signed 64-bit integer ULong: Unsigned 64-bit integer Float: Single precision floating point number Double: Double precision floating point number String: UTF8 string Json: General JSON type intended to be application specific DateTime: ISO 8601 date time Binary: General binary type intended to be application specific\n\n\nint float32 flaot64 https://www.youtube.com/watch?v=wGhBjMcY2YQ&ab_channel=StanislavMarchevsky"
  },
  {
    "objectID": "numbers.html#numerical-representations-of-data",
    "href": "numbers.html#numerical-representations-of-data",
    "title": "Digital numbers",
    "section": "",
    "text": "int float32 flaot64 https://www.youtube.com/watch?v=wGhBjMcY2YQ&ab_channel=StanislavMarchevsky"
  },
  {
    "objectID": "pointcloud.html",
    "href": "pointcloud.html",
    "title": "Point Clouds",
    "section": "",
    "text": "Point cloud data is one of my favorite and most frustrating forms of data to interact with. It represents a significant milestone in most workflows since the collection of observations from the natural world is most commonly a point representation, and there are very few large scale applications whose inputs don’t touch on or hinge on the point cloud data and the phenomenon they aim to represent.\nOne of the most accessible and immediatly relevent data an average user will encounter is point clouds from the USGS entwine dataset.\n“The Point Data Abstraction Library (PDAL) is a C++ library for translating and manipulating point cloud data” . Much like the other giants of geospatial processing, and due perhaps to its orientation in relation to ”big data”, PDAL has really light-posted the pathways towards cloud native and optimized data manipulations. By that, I refer to the ability to perform partial read operations via http protocol, as opposed to more traditional computing pathways which require the entire dataset to be read into memory or otherwise serially streamed. This transformation requires the target point cloud to have been saved as an entwine point cloud, something the USGS and Hobu have been working diligently on and to date there are 2,125 point clouds staged and available for you to hit here. It’s worth noting however, that just because there are cloud native paths to stream that data into the compute environment, does not mean that it’s the fastest or even cheapest way to accomplish your end goal. If you plan on hitting the data more than a few times or if the dataset is static, you may be able to read more data in faster by performing the one-time download of the massive dataset than you would by taking tiny subsets over and over each time you’d like to run your analysis. Because this is so hardware and geographic dataset specific, timings have been hard to track down but using a docker’d image on a Intel Core i5-9600K CPU @ 3.7 GHz with 32 gigs of RAM available and light background use, the following timings were found:\nhttps://pdal.io/en/2.7.1/project/docs.html https://pdal.io/en/2.6.0/workshop/manipulation/ground/ground.html#workshop-ground https://pdal.io/en/2.4.3/workshop/exercises/analysis/dtm/dtm.html https://pdal.io/en/2.4.3/workshop/exercises/analysis/rasterize/rasterize.html There are two sorts of workflows",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Point Clouds"
    ]
  },
  {
    "objectID": "pointcloud.html#common-pipeline-examples",
    "href": "pointcloud.html#common-pipeline-examples",
    "title": "Point Clouds",
    "section": "Common pipeline examples:",
    "text": "Common pipeline examples:\nPull class to tif: {\"pipeline\":[{\"type\":\"readers.ept\",\"filename\":\"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/TX_Coastal_B1_2018/ept.json\",\"bounds\":\"([-10605693, -10601522], [3505846, 3508523])\"},{\"type\":\"filters.range\",\"limits\":\"Classification[17:17]\"},{\"type\":\"writers.gdal\",\"dimension\":\"Z\",\"resolution\":1,\"filename\":\"bridge_1.tif\"}]} pull to csv: {\"type\":\"writers.text\",\"format\":\"csv\",\"order\":\"X,Y,Z\",\"keep_unspecified\":\"false\",\"filename\":\"outputfile.csv\"}\n{\"pipeline\":[{\"type\":\"readers.ept\",\"filename\":\"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/TX_Coastal_B1_2018/ept.json\",\"bounds\":\"([-10605693, -10601522], [3505846, 3508523])\"},{\"type\":\"filters.returns\",\"groups\":\"first,only\"},{\"type\":\"writers.gdal\",\"dimension\":\"Z\",\"resolution\":1,\"filename\":\"FR_1.tif\"}]}",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Point Clouds"
    ]
  },
  {
    "objectID": "pointcloud.html#data",
    "href": "pointcloud.html#data",
    "title": "Point Clouds",
    "section": "Data",
    "text": "Data\nhttps://registry.opendata.aws/usgs-lidar/ https://usgs.entwine.io/ https://portal.opentopography.org/usgsDataset?dsid=USGS_LPC_TX_Central_B1_2017_LAS_2019 https://www.sciencebase.gov/catalog/item/4f70ab64e4b058caae3f8def",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Point Clouds"
    ]
  },
  {
    "objectID": "pointcloud.html#other-tools",
    "href": "pointcloud.html#other-tools",
    "title": "Point Clouds",
    "section": "Other tools",
    "text": "Other tools\nhttps://github.com/oilshit/las_converter https://maps.equatorstudios.com/ https://www.danielgm.net/cc/",
    "crumbs": [
      "Atlas",
      "Geoprocessing",
      "Data Forms and Flows",
      "Point Clouds"
    ]
  },
  {
    "objectID": "probHAND.html",
    "href": "probHAND.html",
    "title": "probHAND",
    "section": "",
    "text": "https://github.com/sclaw/probHAND"
  },
  {
    "objectID": "ras2fim_install.html",
    "href": "ras2fim_install.html",
    "title": "ras2fim_install",
    "section": "",
    "text": "RAS2FIM uses conda to maintain a python environment, but requires the HECRASController, a utility which at present requires a graphical windows environment. There are robust documents available in the official readme that can walk you through the necessary steps to get RAS2FIM installed, so here I’ll outline the tweaks I make to that process to make my life harder than it needs to be, and executing and developing RAS2FIM a bit cleaner and more reproducible. The biggest difference between how I walk you through installation here and how the “official” or “recommended” documentation is that I like to keep things off my primary PC by making that starting point a fresh installation of a windows VirtualBox installed as I outline in how I install GIS.\nOur larger workflow will involve:\n\nInstalling VirtualBox\nInstalling and setting up Windows\nAnd finally, setting up conda and the environment\n\n\n\n\nYou should start by closing any other programs or windows. This explicit close step is important to a successful installation.\nDownload VirtualBox here.\nDouble click the VirtualBox exe and click through the guided installation.\nreboot your PC (you should really do this after every install)\n\n\n\n\n\n\n\nThis is the same rough procedure as outlined in spinning up a windows environment under the “Mac & Linux (via VirtualBox)” tab, with a few extra RAS2FIM utilities.\n\n\nWindows provides “evaluation” virtual environments of Windows 11 for development, available here. Download the VirtualBox version, and unzip it.\nIn Oracle VM, go to File &gt; Import Virtual Appliance and point to the Windows ova. Click next.\nMost of the defaults are fine, but I like to create separate folders of each VM I import. To do this, use the drop down underneath the table to select “other…” and create a new folder called Windows and click on the folder once to select.\nYou will also most certainly want to increase the amount of RAM available: Conversion Chart. Because RAS2FIM is not a light program, it’s best to throw a decent chunk of RAM at it so that the RAS engine has enough room to open, close, and manipulate the files and calculations it needs to do.\n\n\n\n\n\nFinally, lets set up the instance. Right click on the newly added machine and go to settings. Most of the time these are sensible defaults, although there are a few things we can tweak.\n\n\nunder the System dialog, feel free to increase the Base Memory to a reasonable amount, bearing in mind that the more you allocate to the vm, the less you have available on the host. Feel free to explore the other options but don’t change them unless you know what you’re doing or can handle a crash or two :)\nUnder Shared Folders, create a new folder (the icon with a plus)\nPoint to a shared folder on your host machine (mine is called “hold” here)\nSelect the Auto-mount option\nChange the Mount point to something sensible (like “G:\")\n\n\n\n\n\n\n\n\nVirtualBox performance tips\n\n\n\n\n\n\nWhen launching this environment (and really any virtualization in general), it can be painfully slow to load. You might also see a few error messages that seem to sit there for a while and do nothing. An older thread points fingers at errors across a few sources to no solution but be patient and it should load and behave properly, take the dog for a walk.\nIf you encounter flickering screens when using the new PC, under the Display settings, make sure 3D acceleration is unchecked.\n\n\n\n\n\n\n\nNow that we have a fresh version of Windows, we can start following the standard RAS2FIM setup instructions. Here are pointers to some handy utilities and relevant docs that make working in this raw environment easier. Items marked with a checkbox are explicit installations to go though.\n\nInstall git via either\n\npowershell: winget install Git.Git\nor the git utility.\n\nGoogle Chrome\nnotepad++\n7zip\nAnaconda (miniconda is missing the requisite prompt installs)\nHEC-RAS 6.3\n\ndocs\n\nRAS2FIM\n\nwiki\nvideo\n\n\n\n\n\n\n\n\nNote\n\n\n\nA last note: As you may have noticed, when you downloaded the VM image, there was an expiration date ~60 days from the date you downloaded it. You will either need to activate this version of windows, or delete and reinstall as needed. More recent versions I’ve tested seem to force reboot an hour after you start them, which is just enough time to start being productive and a real PITA.\n\n\n\n\n\n\nAt this point we are set up to follow the standard RAS2FIM installation procedures, recreated below for ease and completeness.\n\n\nIf you are following along with these instructions this step is not needed, but you might be recreating an environment and looking to this page for guidance so to be overly comprehensive, if you are trying to update or overwrite an environment, you should first remove and then reinstall the environment with conda deactivate and then conda remove --name {ENV_NAME} --all\n\n\n\n\nClone the repo to your directory of choice: git clone https://github.com/NOAA-OWP/ras2fim.git\nFrom the Anaconda powershell prompt navigate to the top of the ras2fim repo folder and build the environment with : conda env create -f environment.yml\nActivate the environment with: conda activate ras2fim\nIf your window looks like this, you have successfully installed RAS2FIM on your system!\n\n\n\n\n\nThe last step we need to accomplish is to gather the key national datasets necessary to run the utility. While many of these can in th\nCongratulations! RAS2FIm is now locked and loaded to run! Your next steps depend on what you want to accomplish, but if you’re at a loss, the two sample sets of data are staged and ready to test.\nIf you are trying to run RAS2FIM over a particular AOI,"
  },
  {
    "objectID": "ras2fim_install.html#installation-from-0",
    "href": "ras2fim_install.html#installation-from-0",
    "title": "ras2fim_install",
    "section": "",
    "text": "RAS2FIM uses conda to maintain a python environment, but requires the HECRASController, a utility which at present requires a graphical windows environment. There are robust documents available in the official readme that can walk you through the necessary steps to get RAS2FIM installed, so here I’ll outline the tweaks I make to that process to make my life harder than it needs to be, and executing and developing RAS2FIM a bit cleaner and more reproducible. The biggest difference between how I walk you through installation here and how the “official” or “recommended” documentation is that I like to keep things off my primary PC by making that starting point a fresh installation of a windows VirtualBox installed as I outline in how I install GIS.\nOur larger workflow will involve:\n\nInstalling VirtualBox\nInstalling and setting up Windows\nAnd finally, setting up conda and the environment\n\n\n\n\nYou should start by closing any other programs or windows. This explicit close step is important to a successful installation.\nDownload VirtualBox here.\nDouble click the VirtualBox exe and click through the guided installation.\nreboot your PC (you should really do this after every install)\n\n\n\n\n\n\n\nThis is the same rough procedure as outlined in spinning up a windows environment under the “Mac & Linux (via VirtualBox)” tab, with a few extra RAS2FIM utilities.\n\n\nWindows provides “evaluation” virtual environments of Windows 11 for development, available here. Download the VirtualBox version, and unzip it.\nIn Oracle VM, go to File &gt; Import Virtual Appliance and point to the Windows ova. Click next.\nMost of the defaults are fine, but I like to create separate folders of each VM I import. To do this, use the drop down underneath the table to select “other…” and create a new folder called Windows and click on the folder once to select.\nYou will also most certainly want to increase the amount of RAM available: Conversion Chart. Because RAS2FIM is not a light program, it’s best to throw a decent chunk of RAM at it so that the RAS engine has enough room to open, close, and manipulate the files and calculations it needs to do.\n\n\n\n\n\nFinally, lets set up the instance. Right click on the newly added machine and go to settings. Most of the time these are sensible defaults, although there are a few things we can tweak.\n\n\nunder the System dialog, feel free to increase the Base Memory to a reasonable amount, bearing in mind that the more you allocate to the vm, the less you have available on the host. Feel free to explore the other options but don’t change them unless you know what you’re doing or can handle a crash or two :)\nUnder Shared Folders, create a new folder (the icon with a plus)\nPoint to a shared folder on your host machine (mine is called “hold” here)\nSelect the Auto-mount option\nChange the Mount point to something sensible (like “G:\")\n\n\n\n\n\n\n\n\nVirtualBox performance tips\n\n\n\n\n\n\nWhen launching this environment (and really any virtualization in general), it can be painfully slow to load. You might also see a few error messages that seem to sit there for a while and do nothing. An older thread points fingers at errors across a few sources to no solution but be patient and it should load and behave properly, take the dog for a walk.\nIf you encounter flickering screens when using the new PC, under the Display settings, make sure 3D acceleration is unchecked.\n\n\n\n\n\n\n\nNow that we have a fresh version of Windows, we can start following the standard RAS2FIM setup instructions. Here are pointers to some handy utilities and relevant docs that make working in this raw environment easier. Items marked with a checkbox are explicit installations to go though.\n\nInstall git via either\n\npowershell: winget install Git.Git\nor the git utility.\n\nGoogle Chrome\nnotepad++\n7zip\nAnaconda (miniconda is missing the requisite prompt installs)\nHEC-RAS 6.3\n\ndocs\n\nRAS2FIM\n\nwiki\nvideo\n\n\n\n\n\n\n\n\nNote\n\n\n\nA last note: As you may have noticed, when you downloaded the VM image, there was an expiration date ~60 days from the date you downloaded it. You will either need to activate this version of windows, or delete and reinstall as needed. More recent versions I’ve tested seem to force reboot an hour after you start them, which is just enough time to start being productive and a real PITA.\n\n\n\n\n\n\nAt this point we are set up to follow the standard RAS2FIM installation procedures, recreated below for ease and completeness.\n\n\nIf you are following along with these instructions this step is not needed, but you might be recreating an environment and looking to this page for guidance so to be overly comprehensive, if you are trying to update or overwrite an environment, you should first remove and then reinstall the environment with conda deactivate and then conda remove --name {ENV_NAME} --all\n\n\n\n\nClone the repo to your directory of choice: git clone https://github.com/NOAA-OWP/ras2fim.git\nFrom the Anaconda powershell prompt navigate to the top of the ras2fim repo folder and build the environment with : conda env create -f environment.yml\nActivate the environment with: conda activate ras2fim\nIf your window looks like this, you have successfully installed RAS2FIM on your system!\n\n\n\n\n\nThe last step we need to accomplish is to gather the key national datasets necessary to run the utility. While many of these can in th\nCongratulations! RAS2FIm is now locked and loaded to run! Your next steps depend on what you want to accomplish, but if you’re at a loss, the two sample sets of data are staged and ready to test.\nIf you are trying to run RAS2FIM over a particular AOI,"
  },
  {
    "objectID": "ras2fim_install.html#howto-run-ras2fim-over-sample-data",
    "href": "ras2fim_install.html#howto-run-ras2fim-over-sample-data",
    "title": "ras2fim_install",
    "section": "HowTo: Run RAS2FIM over sample data",
    "text": "HowTo: Run RAS2FIM over sample data\npython ras2fim.py -w 10170204 -i C:\\HEC\\input_folder -o C:\\HEC\\output_folder -p EPSG:26915 -v False -n E:\\X-NWS\\X-National_Datasets -r \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.3\"\nWhile there are “smart defaults” in place for both versions of the program used for OWP’s use case in scaling the system for a nation, you are free (and probably should) manually populate all of them if you are running this locally. You also control runtime options using the ras2fim\\config\\r2f_config.env file."
  },
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "Reproducibility",
    "section": "",
    "text": "You may have heard of the reproducibility crisis in Science. This oft-wielded word (“reproducibility”) is usually thrown around in two contexts. The first, and perhaps more prominent in recent years, is the failures of reproducibility with malicious intent. This is where researchers alter figures, p-hack, arbitrarily create data and or otherwise lie or inflate results in the furtherance of career and “prestige”. That will unfortunately always be a part of society. The other and more usually intended context is that of the Nature 2016 article “Estimating the reproducibility of psychological science” (Open Science Collaboration 2015), a portion of which I reproduce below. As always, the geographer is missing, but my two closest analogs, “Engineering” and “Earth and Environment” pose two very different shapes of bell curves.\n\nThose who know me might say I’m a pretty spatial person and one of the worst geographers on the planet. I don’t know street names, place names, and most of my direction giving is based on gesturing and coffee shop landmarks. That is to say, place is pretty arbitrary to me. Space however is not, and so when I functionalize or apply an algorithm in one spot on the earth, I refuse to be satisfied until that works elsewhere. This seems counter-productive until I am asked a week and a half later to do it with a slight tweak and have to overcome all that inertia and friction again. And so, to help me get past this, I’ve become a hard core note taker, and make code and documentation available directly in line (“data driven”) where it makes sense. When that’s not feasible or the appropriate use case, I default to dockerized compute environments.\n\n\n\n\nReferences\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Reproducibility"
    ]
  },
  {
    "objectID": "revealjs_test_slidedeck.html#format-testing",
    "href": "revealjs_test_slidedeck.html#format-testing",
    "title": "So long as you get the GIS&T of it",
    "section": "format testing",
    "text": "format testing\n\n\n\n\n\n\nGoals:\n\nTest the dark maze that is my broken understanding of:\n\nHTML/CSS/Markdown/Quarto?/R/python/pandoc?/revealjs\n\nMake sure slides render\n\nOutcomes and Takeaways:\n\nMaybe my formatting will actually work\nCool guy stuff over here\n\n\n\n\n\n\n\n\n\n\nSilly tests\n\n\nHow’d you get here? Background image from: Water PNGs by Vecteezy, https://www.vecteezy.com/members/ahasanaraakter"
  },
  {
    "objectID": "revealjs_test_slidedeck.html#inline-emogis-and-images",
    "href": "revealjs_test_slidedeck.html#inline-emogis-and-images",
    "title": "So long as you get the GIS&T of it",
    "section": "Inline emogis and images",
    "text": "Inline emogis and images\nWorks:\n\nSpatial conflation ; Network conflation \n\nDoes not:\nSpatial conflation ; Network conflation \n\nI thought I had this?"
  },
  {
    "objectID": "revealjs_test_slidedeck.html#tabs-break-things",
    "href": "revealjs_test_slidedeck.html#tabs-break-things",
    "title": "So long as you get the GIS&T of it",
    "section": "Tabs break things?",
    "text": "Tabs break things?"
  },
  {
    "objectID": "revealjs_test_slidedeck.html#slope-tests-domain-context",
    "href": "revealjs_test_slidedeck.html#slope-tests-domain-context",
    "title": "So long as you get the GIS&T of it",
    "section": "Slope tests: domain context",
    "text": "Slope tests: domain context\n\n\n\n\n(aside: want to see your examples here? send me data!)\n\nDomain ContextSelected river (live!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution? Stop being fancy…"
  },
  {
    "objectID": "revealjs_test_slidedeck.html#just-leaflet",
    "href": "revealjs_test_slidedeck.html#just-leaflet",
    "title": "So long as you get the GIS&T of it",
    "section": "Just leaflet",
    "text": "Just leaflet"
  },
  {
    "objectID": "revealjs_test_slidedeck.html#closing-bookened",
    "href": "revealjs_test_slidedeck.html#closing-bookened",
    "title": "So long as you get the GIS&T of it",
    "section": "Closing bookened",
    "text": "Closing bookened\n\n\n\nOutcomes and Takeaways:\n\nHopefully those rendered?\n\ngreat?\nSucess?\n\nEverything after here should work…\nHow did you end up here?\n\nNext Steps:\n\nFinally finish somethi\nGet back to work\nSolve Hydrology\n\n\n\n\n\n\nHow did you end up here?"
  },
  {
    "objectID": "revealjs_test_slidedeck.html#prefer-div-to-md-for-normal-images",
    "href": "revealjs_test_slidedeck.html#prefer-div-to-md-for-normal-images",
    "title": "So long as you get the GIS&T of it",
    "section": "Prefer div to md for normal images",
    "text": "Prefer div to md for normal images\n\n\n\n\n\n\nit passes but it looks better as a div"
  },
  {
    "objectID": "revealjs_test_slidedeck.html#position-styles-can-go-wherever-in-the-css-string",
    "href": "revealjs_test_slidedeck.html#position-styles-can-go-wherever-in-the-css-string",
    "title": "So long as you get the GIS&T of it",
    "section": "Position styles can go wherever in the css string",
    "text": "Position styles can go wherever in the css string\n\n\n\nBorders (0 padding, ect) don’t matter"
  },
  {
    "objectID": "revealjs_test_slidedeck.html#design-styles",
    "href": "revealjs_test_slidedeck.html#design-styles",
    "title": "So long as you get the GIS&T of it",
    "section": "Design styles",
    "text": "Design styles\n\n\n\n\n\n\n\n\nWhy do I do this to myself?"
  },
  {
    "objectID": "sciencemethod.html",
    "href": "sciencemethod.html",
    "title": "The Scientific Method",
    "section": "",
    "text": "Inspiration from https://www.researchgate.net/figure/The-Scientific-Method_fig2_228980756",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "The Scientific Method"
    ]
  },
  {
    "objectID": "silly_standards.html",
    "href": "silly_standards.html",
    "title": "Silly standards",
    "section": "",
    "text": "Standards\nI find a lot of issues with “standards” as they are implemented because they are: * Gatekept by ISO and paid for at exorbitant prices: * e.g. it’s $190 to find “specifies methods for determining the velocity and cross-sectional area of water flowing in open channels and for calculating the discharge employing point velocity measurement devices. Although some general procedures are discussed, this document does not describe in detail how to use or deploy these systems.” - https://www.iso.org/standard/72754.html * It’s $243 to learn how to semantically talk about Observations, measurements and samples in the context of GIS: “Observations commonly involve sampling of an ultimate feature-of-interest. This document defines a common set of sample types according to their spatial, material (for ex situ observations) or statistical nature. The schema includes relationships between sample features (sub-sampling, derived samples).” - https://www.iso.org/standard/82463.html * &gt; Note: What you can see on a site is filtered for public consumption (missing entire chapters and sections): https://www.iso.org/obp/ui/en/#iso:std:iso:19156:ed-2:v1:en * not followed internally * Typically as a result of “building the plane as we fly” * Poorly communicated * Built off legacy, sometimes very long running and although building the plane as we fly is too fast, building at the speed of the movement of continents is also undesirable\nHowever. standards also make life so, so much easier.\n\nStandards make backward compatibility easier\nThey make interoperability easier\nThey accelerate research to operations: [[20240823150118]] Reseach to Oporations\n\nIt’s critical to have group consensus and buy in if you hope to have standards work\nSemantics makes standards possible, If you don’t have the same language for concepts, it’s hard to make standards work.\nSyntax makes communication more reliable"
  },
  {
    "objectID": "soc.html",
    "href": "soc.html",
    "title": "Separating Concerns",
    "section": "",
    "text": "Part of sharing a context means sharing the same goal, but it’s also critical to work under the same constraints and share the same values. Doing that takes time and willpower from all parties involved and I’m not in the game to influence people. One of the better ways I’ve found of conveying those is to make explicit the separate concerns and expectations you have and the iron triangle is as natural a tool to do that as any. The iron triangle is a concept that you see mirrored across a couple different domains, but the service variation is probably the most familiar. This tries to get at the idea that there are tradeoffs that need to be made in how we expect to receive services. If something is good and cheap, you are not going to get it fast. If something is good and fast, it’s not going to be cheap, and if it’s fast and cheap it’s probably not going to be very good. While some might try to argue that this is a false choice, the general sentiment still applies. \nThis is a good start but I like to extend this by applying (Garvin 1987)’s Eight Dimensions of Quality to the “good” aspect because that helps more explicitly force users to choose aspects that are important to them. I do that by equating good to quality, and using those dimensions to compose that sphere.  Finally, I like to imagine that you have to put this in front of someone and force them to pick the particular spheres/attributes that they value (and perhaps consequently, what can be de-emphasized). That could be a simple checkbox, but it’s probably better to force them to rank the attributes from highest to lowest since the effort expended is allocated as such when we actually start building the damn thing.\n\n\n\n\nReferences\n\nGarvin, David A. 1987. “Competing on the Eight Dimensions of Quality.” Harvard Business Review, November.",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Separating Concerns"
    ]
  },
  {
    "objectID": "templates.html#cross-formatting",
    "href": "templates.html#cross-formatting",
    "title": "Templates",
    "section": "Cross formatting",
    "text": "Cross formatting\n\nTabsBoxesSecretsColumnsDocs & utilities\n\n\n```{md}\n::: {.panel-tabset}\n\n## tab 1\n\n## tab 2\n\n:::\n```\n\n\n\nAlso worth peaking at https://www.andreashandel.com/posts/2023-06-13-custom-callouts-quarto/ & https://quarto.org/docs/authoring/callouts.html\n\n\nmdhtml collapse\n\n\n```{md}\n::: {.callout-note  collapse=\"false\"}\n## This would be a callout title\n\nNote that there are five types of callouts, including:\n`note`, `warning`, `important`, `tip`, and `caution`.\n:::\n```\n\n\n```{html}\n&lt;details&gt;\n&lt;summary&gt; What points were used?&lt;/summary&gt;\nI am collapsed\n&lt;/details&gt;\n```\n\n\n\n\n\n```{md}\n::: {.content-hidden}\n\nWill not appear in HTML.\n\n:::\n```\n\n\n\nSee: Some placed in &lt;https://docs.zettlr.com/en/core/snippets/&gt;\n\n```{md}\n:::: {#fig-hersh41 layout-ncol=\"2\"}\n\nWhere A is placesd\n\nWhere b is placed\n\n(a left): A screen and (b right) a derivation \n::::\n```\n-or-\n```{md}\n:::: columns\n::: {.column}\nContent of column 1\n:::\n\n::: {.column}\nContent of column 2\n:::\n::::\n```\n\n\n\nQuarto main page\nCode fence options\n\n```{md}\n#, fig.height=9}\n#| label: fig-meancheck\n#| warning: false\n#| echo: false\n#| eval: false\n```\n\nPage layout\nKatex formatting\nLive mermaid diagram maker\n\n\n\n\n\nNotes\n\nStructuring docsStructuring notesZenkettle headersDocs and utilities\n\n\n\nSee also: a (WIP) Data Driven Workflow\n\n```{md}\nhome                      &lt;- landing page\n├── tutorial              &lt;- landing page\n│   └── part 1            &lt;- landing page\n├── how-to guides         &lt;- landing page\n│   ├── install\n│   └── deploy\n├── reference             &lt;- landing page\n│   ├── commandline tool\n│   └── available endpoints\n└── explanation           &lt;- landing page\n    ├── best practice recommendations\n    └── performance\nPaper\n├── Introduction\n├── Background       &lt;- literature_notes,reference,explanation\n├── Methods          &lt;- how-to guides,reference,explanation \n├── Results\n├── Discussion       &lt;- reference,explanation \n└── Conclutions    \n```\n\n\n\nsee also: installing Zotero\n\nA literature note header and some form of a content prompt like so:\n```{md}\n---\ntitle: \"&lt;paper title&gt;\"\nid: \nkeywords:\n  - Drone\n  - lo_q-med_q-hi_q\n...\n[citation]\n# Summary\n## Mine\n## MaxF\n## TFIDF\n# Annotations\n&lt;From top sticky note&gt;\n## What was the question, problem or purpose of the study?\n## What methods are deployed?\n### What data did they use?\n### What noteable codebase/tools were developed/deployed?\n## What are the authors’ major findings/conclusions?\n## What evidence supports their interpretations?\n## Why are the findings of this research important?\n## Explicit next steps they suggest\n## Did I find anything surprising?\n## What am I confused about?\n## Interesting figures?\n## Promising References\n```\n\n\n```{md}\n---\ntitle: \"Title\"\nid: YYYYMMDDmmss\nkeywords:\n  - fill\n  - these\n  - in\n...\n```\n\n\n\nZettlr docs\nKatex formatting\nMarkdown table generator\nFiletree generator\nLive mermaid diagram maker",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Templates"
    ]
  },
  {
    "objectID": "templates.html#functions",
    "href": "templates.html#functions",
    "title": "Templates",
    "section": "Functions",
    "text": "Functions\n\nSee also: Packaging and Production\n\n\nRPython\n\n\n```{r}\nfunfunc &lt;- function(path_to_inputs,path_to_outputs,overwrite=FALSE,is_verbose=FALSE) {\n  # sinew::moga(file.path(getwd(),\"R/funfunc.R\"),overwrite = TRUE)\n  # devtools::document()\n  # pkgdown::build_site(new_process=TRUE)\n  #\n  # devtools::load_all()\n  \n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  # ----- User inputs --------------------------------------------------------------------------------------------\n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  fn_time_start &lt;- Sys.time()\n  \n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  # ----- Step function ------------------------------------------------------------------------------------------\n  # More verbose description as needed\n  # Can be multiple lines\n  #///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  if(is_verbose) {\n    runtime &lt;- Sys.time() - fn_time_start\n    units(runtime) &lt;- \"hours\"\n    print(paste(\"Total Compute Time: \",round(runtime, digits = 3),\"hours\"))\n  }\n  \n  return(TRUE)\n}\n```\n\n\n```{python}\nimport argparse\nimport geopandas\nimport contextily\nimport os\nimport sys\nimport time\nimport datetime\n\ndef main(is_verbose):\n    \"\"\"\n    Subset dam lines from hydrofabric\n    Args:\n        data_path (str): The path to hydrofabric data.\n        hf_id (str): A hydrofabric flowpath id to use as the headwater.\n        is_verbose (bool): Whether to print verbose output.\n        \n    Example usage:\n    python /hydrofabric3D/Python/subset_dam_lines.py \\\n        -data_path \"/media/sf_G_DRIVE/data/\" \\\n        -hf_id 'wb-2414833' \\\n        -v\n\n    flowpaths, transects, xyz = subset\\_dam\\_lines(data\\_path = \"/media/sf\\_G\\_DRIVE/data/\", hf\\_id = 'wb-2414833', is\\_verbose = True)\n\n    \"\"\"\n    start_runtime = time.time()\n\n    if is_verbose:\n        end_runtime = time.time()\n        time_passed = (end_runtime - start_runtime) // 1\n        time_passed = datetime.timedelta(seconds=time_passed)\n        print('Total Compute Time: ' + str(time_passed))\n\n    return True\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Subset dam break lines.\")\n    parser.add_argument(\"data_path\", help=\"The path to hydrofabric data.\")\n    parser.add_argument(\"hf_id\", type=str, help=\"A hydrofabric flowpath id to use as the headwater.\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Enable verbose output.\")\n\n    args = parser.parse_args()\n\n    processed_result = subset_dam_lines(args.data_path, args.hf_id, args.verbose)\n```",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Templates"
    ]
  },
  {
    "objectID": "templates.html#qr-code-helpers",
    "href": "templates.html#qr-code-helpers",
    "title": "Templates",
    "section": "QR code helpers",
    "text": "QR code helpers\n\nChromeRQuarto\n\n\nThe share page on the more tools ellipse.\n\n\nVia the qrcode package (tips):\n```{r}\nqrcode::add_logo(\n  qrcode::qr_code(\"https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fdvsax6nc9mmc1.jpeg\", ecl = \"H\"),\n  file.path(\"_temp\",fsep = .Platform$file.sep),\n  ecl = \"L\",\n  hjust = \"c\",\n  vjust = \"c\"\n)\n```\n\n\nIn quarto’s terminal: quarto install extension jmbuhr/quarto-qrcode\nShortcode:",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Templates"
    ]
  },
  {
    "objectID": "templates.html#pages",
    "href": "templates.html#pages",
    "title": "Templates",
    "section": "Pages",
    "text": "Pages\n\nPowerpoint\n\nStandard Landscape\n\nFull page: 13.333 in x 7.5 in | 338.7 mm x 190.5 mm (16:9)\n960 x 540 mm\n0.25 in &gt; 6.35\n6.35\n\nH 533.65\nV 953.65\n\n\nreveal\n\nStock: 1050 x 700\nMine: 1920 x 1080\n\n\n\n\nPrinted page\n\nPortrait:\n\nFull page: 8.5 in x 11 in | 215.9 mm x 279.4 mm (0.7:1)\nStandard publication boarder of 0.5 in: 7.5 in x 10 in | 177.8 mm x 254 mm\n\nFull page guides at:\n\nHorizontal: 0.5 in & 10.5 in | 12.7 mm & 266.7 mm\nVertical: 0.5 in & 8 in | 12.7 mm & 203.2 mm\n\n\nStandard print boarder of 0.25 in: 8 in x 10.5 in | 203.2 mm x 266.7 mm\n\nFull page guides at:\n\nHorizontal: 0.25 in & 8.25 in | 6.35 mm & 209.55 mm\nVertical: 0.25 in & 10.75 in | 6.35 mm & 273.05 mm\n\n\n\nLandscape:\n\nFull page: 11 in x 8.5 in | 279.4 mm x 215.9 mm (1:0.7)\nStandard publication boarder of 0.5 in: 10 in x 7.5 in | 254 mm x 177.8 mm\n\nFull page guides at:\n\nHorizontal: 0.5 in & 8 in | 12.7 mm & 196.85 mm\nVertical: 0.5 in & 10.5 in | 12.7 mm & 273.05 mm\n\n\nStandard print boarder of 0.25 in: 10.5 in x 8 in | 266.7 mm x 203.2 mm\n\nFull page guides at:\n\nHorizontal: 0.25 in & 8.25 in | 6.35 mm & 209.55 mm\nVertical: 0.25 in & 10.75 in | 6.35 mm & 273.05 mm",
    "crumbs": [
      "Atlas",
      "Sensemaking",
      "Templates"
    ]
  },
  {
    "objectID": "wicked_problems.html",
    "href": "wicked_problems.html",
    "title": "Wicked problems",
    "section": "",
    "text": "Water is a wicked problem\nLike trying to sculpt fog in the dark with a chainsaw, water is a wicked problem. This term, wicked problem, is intentionally phrased as such because these challenges have unique attributes that make it worth pausing for reflection before we dive headfirst into tackling it. Wicked problems, as defined in (Freeman 2000), reduced in (“Wicked-Problems-Flower.png (51005100)” n.d.), and modified for my own wicked problem below, include 8 characteristics that make them a wicked hard problem to tackle.\n\n\n\n\n\nmindmap\n  Wicked Problem\n    Every problem is unique\n      E.G.: Water is not the same as a biological outbreak\n    Every problem is connected to others\n      E.G.: FEWS\n    There is no clear problem definition\n    Are multi-causal, multi-scler, and interconnected.\n    Include multiple, invested stakeholders with different values, goals, and objectives.\n    Straddel oranizational and dicsiplinary boundaries\n    Solutions to an aspect have implications/ramifications across the system\n    Solutions are not right or wrong, but good and bad\n    Can be difficult to measure or evaluate effects of implemented solutions\n    Problems are never completely solved\n      They are wicked problems, not wicked puzzles\n\n\n\n\n\n\nWhile many of these are unclear and vague, a lot of this work can not start without first creating an accounting of it. Most of this comes from the ideas of mass balance as laid out in hydrology, and in my attempts at measuring flow via PIV.\n\n\n\n\n\nReferences\n\nFreeman, David M. 2000. “Wicked Water Problems: Sociology and Local Water Organizations in Addressing Water Resources Policy.” Journal of the American Water Resources Association 36 (3): 483–91. https://doi.org/10.1111/j.1752-1688.2000.tb04280.x.\n\n\n“Wicked-Problems-Flower.png (51005100).” n.d. https://transitiondesignseminarcmu.net/wp-content/uploads/2016/12/Wicked-problems-flower.png. Accessed February 12, 2024."
  }
]